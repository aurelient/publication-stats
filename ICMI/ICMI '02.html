


<!doctype html>


<head><script type="text/javascript">_cf_loadingtexthtml="<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";
_cf_jsonprefix='//';
_cf_clientid='1DF8256057A41F1BB21306FB582060A8';</script><script type="text/javascript" src="/CFIDE/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfform.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/masks.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/FCKeditor/fckeditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/adapter/yui/ext-yui-adapter.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/ext-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/resizable.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dragdrop/dragdrop.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/util.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/build/state/State-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/widget-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dialog/dialogs.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/cf/cf.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />



<title>Proceedings of the 4th IEEE International Conference on Multimodal Interfaces</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
	
  --></style>
 


<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>


<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  function setDiv() {
	var m = document.getElementById('divmain');
	var mh = m.offsetHeight;
	var t = document.getElementById('divtools');
	var th = t.offsetHeight;
	var tg = document.getElementById('divtags');
	var tgh = tg.offsetHeight;
	var calcheight = mh - th;
	if (tgh > calcheight  ){
	  var x = (th + tgh) - mh;
	  if ( (th + tgh) - mh < 65) {
	  }
	  else {
		 document.getElementById('divtags').innerHTML = ""; 
		 var tg = document.getElementById('divtags');
		 var tgh = tg.offsetHeight;
		 tg.style.height = tgh  + 'px';
	  }
	}
	else {
		tg.style.height = calcheight + 'px';
		document.getElementById('divtags').innerHTML = "";
	}

//  do I need to check after I resize to be sure I didn't go too big?
//	var tg2 = document.getElementById('divtags');
//	var tgh2 = tg.offsetHeight;	
//	if (tgh2 > mh + 65) {
//	  var y = mh + 65;
//	  alert('expanded too much ' + tgh2 + ' should be at most ' + y);
//	  tg.style.height = y + 'px';
//	  document.getElementById('divtags').innerHTML = "";
//	}

  }
</script>

<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>



<meta name="citation_publisher" content="IEEE Computer Society"> <meta name="citation_title" content="Proceedings of the 4th IEEE International Conference on Multimodal Interfaces"> <meta name="citation_date" content="10/14/2002"> <meta name="citation_isbn" content="0-7695-1834-6"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=846222"> 



<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFFORM');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFDIV');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFTEXTAREA');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFWINDOW');
</script>

<script type="text/javascript">
	var _cf_window_init_1338241011134=function()
	{
		_cf_bind_init_1338241011135=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide_body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338241011135);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window1338241011133', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241011134);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241011137=function()
	{
		_cf_bind_init_1338241011138=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags_body','bindExpr':['showthetags.cfm?id=846222']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338241011138);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=846222',{ modal:false, closable:true, divid:'cf_window1338241011136', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241011137);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241011140=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window1338241011139', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241011140);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241011142=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window1338241011141', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241011142);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241011144=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window1338241011143', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241011144);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241011146=function()
	{
		_cf_bind_init_1338241011147=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder_body','bindExpr':['savetobinder.cfm?id=846222']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338241011147);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=846222',{ modal:false, closable:true, divid:'cf_window1338241011145', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241011146);
</script>
</head>

<body style="text-align:center" onLoad="window.focus();">

<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'http://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
                        
<script src='AC_RunActiveContent.js' type="text/javascript"></script>




<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>



<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
	
    <tr style="vertical-align:top">
		
		<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text"  ><img src="http://dl.acm.org/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
		</td>
        
        <td style="padding-left: 5px; padding-right:10px; padding-bottom:0px;" class="small-link-text">
        	<table style="width:100%; border-collapse:collapse; padding:0px">
			<tr><td style="text-align:center">
				
                            <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
                    
					</td>		
			</tr>
			</table> 
        </td>
		<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text">
			 <p style="margin-top:0px; margin-bottom:10px;">
					
                            <a href="https://dl.acm.org/signin.cfm?cfid=105750519&amp;cftoken=91765132" class="small-link-text" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
                            &nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm?cfid=105750519&amp;cftoken=91765132"  class="small-link-text" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
						
			 </p>
            
			<table style="padding: 5px; border-collapse:collapse; float:right">
				
				
                            
                            <tr>
                            <td class="small-link-text" style="text-align:right">
                            <form name="qiksearch" action="results.cfm?h=1&amp;cfid=105750519&amp;cftoken=91765132" method="post">
                           
                           
                            
                                     
                                    <span style="margin-left:0px"><label><input type="text" name="query" size="34" value=" " /></label>&nbsp;
                                    <input style="vertical-align:top;" type="image" alt="Search" name="Go" src="http://dl.acm.org/images/search_small.jpg" />
                                    
                                    </span>
							  </form>
                                </td>
                            </tr>
                          
				  
			</table>

			
			
		</td>

	</tr>
    
    
    <tr><td colspan="3" class="small-link-text" style="padding-bottom:5px; padding-top:0px; text-align:center">
		<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
         
         </td>
    </tr>
    </table>
	
<map name="port" id="port" > 
  <area shape="rect" coords="1,1,60,50" href="http://www.acm.org/" alt="ACM Home Page" />
  <area shape="rect" coords="65,1,275,68" href="http://dl.acm.org/dl.cfm?CFID=105750519&CFTOKEN=91765132" alt="ACM Digital Library Home Page" />
</map>

<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
	<tr style="vertical-align:top">
		<td style="padding-right:10px; text-align:left" class="small-link-text">
        	<div id="divmain" style="border:1px solid #356b20;">
				 
				<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
					
                    	<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;"><strong>Proceedings of the 4th IEEE International Conference on Multimodal Interfaces</strong></h1>
                        
                </div>
                
                  

<table class="medium-text" style="border-collapse:collapse; padding:0px;">

<col style="width:540px" />

<tr style="vertical-align:top">
  <td>
    <table style="border-collapse:collapse; padding:2px;" class="medium-text">
      <col style="width:80px;" />
      <col style="width:auto" />
      <tr style="vertical-align:top">
        
      </tr>
    </table>

	
        <table style="margin-top: 10px" border="0" class="medium-text" cellpadding="2" cellspacing="0">
            <tr><td><table border="0" class="medium-text" cellpadding="1" cellspacing="0">

<tr valign="top">
    <td nowrap="nowrap" style="padding-top:10px;">Publication:</td>
</tr>

	<tr valign="top">
    	<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Proceeding</td>
	    <td colspan="2"></td>
	</tr>
    
    <tr valign="top">
	    <td colspan="3" style="padding-left:10px">ICMI '02 Proceedings of the 4th IEEE International Conference on Multimodal Interfaces 

        </td>
	</tr>
    <tr valign="top">
	    <td colspan="3" style="padding-left:10px"> <span class="small-link-text">IEEE Computer Society</span> <span class="small-link-text">Washington, DC</span><span class="small-link-text">, USA</span> <span class="small-link-text">  &copy;2002</span> 
          <br />           
                    
		           <a href="citation.cfm?id=846222&amp;picked=prox&amp;cfid=105750519&amp;cftoken=91765132"  target="_self" class="small-link-text">table&nbsp;of&nbsp;contents</a>
			  ISBN:0-7695-1834-6 
       </td>
	</tr>

</table></td></tr>
        </table>
    

  </td>

  <td rowspan="20" nowrap="nowrap">
	<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
		<tr>
        	<td align="center" style="padding-bottom: 5px;">
			  
               
              </td>
              <td valign="top" align="left" nowrap="nowrap">
	             2002 Proceeding<br />
                 
        	 </td>
        </tr>
        
        <tr>
        	<td colspan="2" valign="baseline" style="padding-bottom:5px;">
            <img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
            <a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
            </td>
         </tr>
         <tr>
            <td  class="small-text" colspan="2" valign="top" style="padding-left:30px;">
				
	                    	&middot;&nbsp;Downloads (6 Weeks): 344<br />
    	                    &middot;&nbsp;Downloads (12 Months): 1,654<br />
                          
                        &middot;&nbsp;Citation Count: 463 
			</td>
         </tr>

	</table>
  </td>
</tr>
</table>

<br clear="all" />

                  
                 <br clear="all" />
			</div>
			
		</td>
		<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
	
            <div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
              <div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;"><strong>Tools and Resources</strong></h1></div>


<ul title="Tools and Resources" style="list-style: none; list-style-position:inside;
margin-left: 0px;
padding-left: 0em;
text-indent: 5px;
margin-bottom: 0px;">


<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:6px;">
   <span class="small-link-text">TOC Service:</span>
   	  
	  <img src="http://dl.acm.org/images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
      <ul style="margin-left: 0; padding-left: 0; display:inline;">
      	
        <li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
        <li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');"  class="small-link-text">RSS</a></li>
		        
      </ul>
    </span>
</li>

        <li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:6px;">
        <a href="citation.cfm?id=846222&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
         </span></li>
    




<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Export Formats:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=846222&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=846222&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=846222&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
      </ul>
    </span>
</li>



 
   <li style="list-style-image:url(img/calbullet.jpg);margin-top:15px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Upcoming Conference:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px; margin-left:25px;"><a href="http://www.acm.org/icmi/2012/ " title="INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION" class="small-link-text">ICMI '12</a></li>
      </ul>
    </span>
	</li>
    


</ul>           

  <!-- ADDTHIS BUTTON BEGIN -->
  
  <!-- ADDTHIS BUTTON END -->

<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>
  <!-- AddThis Button BEGIN -->



<!-- AddThis Button BEGIN -->
<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_slashdot"></a>
<a class="addthis_button_reddit"></a>


<span class="addthis_separator">|</span>
<a href="http://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>
<!-- AddThis Button END -->

  
 

  
  
            </div>
            
		</td>
	</tr>
    
</table>



</div>


<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">




<div id="fback" style="text-align:left; padding-top:10px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="feedback" border="0" /></a>
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design"><strong>Feedback</strong></a>

<span style="padding:10px;">|</span>




<span>Switch to <a href="citation.cfm?id=846222&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>


</span>

 
<div class="small-text" style="margin-top:10px; margin-bottom:5px;"> 
<br />

    <a href="#abstract"  title="Abstract" style="padding:5px"><span style='color:#999999'>Abstract</span></a> |
    
    <a href="#authors"  title="Authors" style="padding:5px"><span style='color:#999999'>Authors</span></a> |
    <a href="#references"  title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
    <a href="#citedby"  title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
    <a href="#indexterms"  title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
    <a href="#source"  title="Publication" style="padding:5px"><span>Publication</span></a> |
    <a href="#revs"  title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |               
	<a href="#comments"  title="Comments" style="padding:5px"><span>Comments</span></a>
	
     |               
	<a href="#prox"  title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
    
</div>
    
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;"/>



</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
       	
		  	<div class="flatbody">
              <div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
                  An abstract is not available.
              </div>            
                      
			
			</div>
            
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
		
             <div class="small-text" style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
                  Author information is not available
              </div>            

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	References are not available

</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	Citings are not available
		
 </div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

Index Terms are not available


</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">



<table border="0" class="medium-text" cellpadding="0" cellspacing="5">




    <tr valign="top">
    	<td>Title</td>
		<td>ICMI '02 Proceedings of the 4th IEEE International Conference on Multimodal Interfaces           
    	           <a href="citation.cfm?id=846222&picked=prox&CFID=105750519&CFTOKEN=91765132"  target="_self" class="small-link-text">table&nbsp;of&nbsp;contents</a>
			    
		 </td>
    </tr>
	
        <tr>
         
             <td>Sponsors</td>
            
          <td>
			  <a href="sig.cfm?id=SP923&CFID=105750519&CFTOKEN=91765132"> SIGCHI</a> ACM Special Interest Group on Computer-Human Interaction
          </td>
         </tr>
     
        <tr>
         
              <td></td>
          
          <td>
			  <a href="http://www.acm.org/"> ACM</a> Association for Computing Machinery
          </td>
         </tr>
     
            <tr><td>Publisher</td><td>IEEE Computer Society Washington, DC, USA  &copy;2002
            </td>
            </tr>
     <tr><td>ISBN</td><td>0-7695-1834-6 </td></tr> 
      
       <tr>
       <td></td>
       <td>
      
       
       </td>
       </tr>


	   
                    <tr valign="top">
                    <td>Conference</td>
                    <td valign="top" align="left"  style="padding-bottom: 25px;">
                        <strong style="padding-right:10px">ICMI-MLMI</strong><a href="event.cfm?id=RE354&CFID=105750519&CFTOKEN=91765132" title="Multimodal Interfaces and Machine Learning for Multimodal Interaction">Multimodal Interfaces and Machine Learning for Multimodal Interaction</a>
                        
                               
                                <a href="event.cfm?id=RE354&CFID=105750519&CFTOKEN=91765132" title="Multimodal Interfaces and Machine Learning for Multimodal Interaction"><img border="0" src="http://portalparts.acm.org/event_logos/RE354.jpg" title="ICMI-MLMI logo" height="14"  width="100" ALT="ICMI-MLMI logo" style="vertical-align:top"></a>
                                 
        
                     </td>
                    </tr>
                    <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 272 of 669 submissions, 41%</td></tr>
                       <tr valign="top">
                        <td colspan="2" style="padding-left:25px;">
                        	<table>
                            	<tr><td>
                                        <!-- WebCharts3D v5.1(2077) -->
<IMG SRC="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=Images/2641357700525582.JPG" id="Images_2641357700525582_JPG" name="Images_2641357700525582_JPG" usemap="#Images_2641357700525582_JPG_map" border="0"/>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAB' id='GP1338241011326AAAB'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '03</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>130</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAC' id='GP1338241011326AAAC'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '03</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>45</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAD' id='GP1338241011326AAAD'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>102</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAE' id='GP1338241011326AAAE'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>40</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAF' id='GP1338241011326AAAF'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>99</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAG' id='GP1338241011326AAAG'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>55</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAH' id='GP1338241011326AAAH'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>IMCI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>100</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAI' id='GP1338241011326AAAI'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>IMCI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>44</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAJ' id='GP1338241011326AAAJ'><tr><td width='8'>&nbsp;</td><td width='84'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>ICMI-MLMI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>118</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAK' id='GP1338241011326AAAK'><tr><td width='8'>&nbsp;</td><td width='84'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>ICMI-MLMI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>41</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAL' id='GP1338241011326AAAL'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>120</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241011326AAAM' id='GP1338241011326AAAM'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>47</td></tr></table>
<MAP name='Images_2641357700525582_JPG_map'>
<AREA shape='rect' coords='0,0,1,1'/>
<AREA shape="rect" coords="290,159,308,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAM",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAM",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAM",event)'/>
<AREA shape="rect" coords="272,74,290,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAL",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAL",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAL",event)'/>
<AREA shape="rect" coords="243,166,261,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAK",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAK",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAK",event)'/>
<AREA shape="rect" coords="225,76,243,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAJ",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAJ",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAJ",event)'/>
<AREA shape="rect" coords="196,162,214,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAI",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAI",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAI",event)'/>
<AREA shape="rect" coords="178,97,196,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAH",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAH",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAH",event)'/>
<AREA shape="rect" coords="149,149,167,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAG",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAG",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAG",event)'/>
<AREA shape="rect" coords="131,98,149,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAF",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAF",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAF",event)'/>
<AREA shape="rect" coords="102,167,120,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAE",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAE",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAE",event)'/>
<AREA shape="rect" coords="84,95,102,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAD",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAD",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAD",event)'/>
<AREA shape="rect" coords="56,161,74,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAC",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAC",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAC",event)'/>
<AREA shape="rect" coords="38,62,56,213" onMouseover='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAB",event,true)' onMouseout='xx_set_visible("Images_2641357700525582_JPG","GP1338241011326AAAB",event,false)' onMousemove='xx_move_tag("Images_2641357700525582_JPG","GP1338241011326AAAB",event)'/>
<AREA shape="rect" coords="160,13,227,27"/>
<AREA shape="rect" coords="89,13,160,27"/>
</MAP>

<script language="javascript" src="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=script.js"></script>

                            			</td>
                                        <td style="padding-left:20px;">
                                           <table style="border-width: 1px; border-style: solid; width:100%; border-spacing: 6px;" class="text12">
                                              <tr bgcolor="#ffffff">
                                                <th style="width:50%">Year</th>
                                                <th  align="right" style="width:15%;">Submitted</th>
                                                <th  align="right" style="width:15%">Accepted</th>
                                                <th  align="center">Rate</th>
                                              </tr>
                                              
                                                      <tr bgcolor="#f0f0f0">
                                                          <td>ICMI '03</td>
                                                          <td align="right">130</td>
                                                          <td align="right">45</td>
                                                          <td align="center">35%</td>
                                                       </tr>
                                              
                                                      <tr bgcolor="#ffffff">
                                                          <td>ICMI '06</td>
                                                          <td align="right">102</td>
                                                          <td align="right">40</td>
                                                          <td align="center">39%</td>
                                                       </tr>
                                              
                                                      <tr bgcolor="#f0f0f0">
                                                          <td>ICMI '07</td>
                                                          <td align="right">99</td>
                                                          <td align="right">55</td>
                                                          <td align="center">56%</td>
                                                       </tr>
                                              
                                                      <tr bgcolor="#ffffff">
                                                          <td>IMCI '08</td>
                                                          <td align="right">100</td>
                                                          <td align="right">44</td>
                                                          <td align="center">44%</td>
                                                       </tr>
                                              
                                                      <tr bgcolor="#f0f0f0">
                                                          <td>ICMI-MLMI '09</td>
                                                          <td align="right">118</td>
                                                          <td align="right">41</td>
                                                          <td align="center">35%</td>
                                                       </tr>
                                              
                                                      <tr bgcolor="#ffffff">
                                                          <td>ICMI '11</td>
                                                          <td align="right">120</td>
                                                          <td align="right">47</td>
                                                          <td align="center">39%</td>
                                                       </tr>
                                              
                                               <tr bgcolor="#f0f0f0">
                                                  <td><strong>Overall</strong></td>
                                                  <td align="right">669</td>
                                                  <td align="right">272</td>
                                                  <td align="center">41%</td>
                                       			</tr>
                                              </table>
                         				</td>
                                    </tr>
                            </table>
                        </td>
                        
                        
                    </tr>
                     
                     
                     
</table>




</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	<br />Reviews are not available for this item
        
        <div align="left" style="margin-top:30px">
					<a title="Computing Reviews" href="ocr_review_main.cfm?CFID=105750519&CFTOKEN=91765132">
                 <img src="http://dl.acm.org/images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
        
        
       		<ul style="list-style:disc; display:inline-block">
	            <li>Access <a href="ocr_review_main.cfm?CFID=105750519&CFTOKEN=91765132" target="_blank">critical reviews</a> of computing literature.</li>
            	<li><a href="http://www.computingreviews.com/Reviewer/"  target="_blank">Become a reviewer</a> for Computing Reviews</li>
            </ul>
        </div>
        
</div>



<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
         
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<div>
<div>
	
	<p style="margin-left:5px;">
    <strong>Be the first to comment</strong>
    	
          	To Post a comment please <a href="signin.cfm?CFID=105750519&CFTOKEN=91765132">sign in or create</a> a free Web account</a>
        
    
    
	 </p>
	   	
 
</div>


</div>

	
		<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
        
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" >

<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 4th IEEE International Conference on Multimodal Interfaces</h5>


<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>

<div style="clear:both">
    
        <div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><span class="link-text">no previous proceeding</span> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=958432&picked=prox&CFID=105750519&CFTOKEN=91765132" title="Next: ICMI '03">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
        
</div>


 
<table class="text12" border="0">

  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847780&CFID=105750519&CFTOKEN=91765132">Preface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: .11</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847781&CFID=105750519&CFTOKEN=91765132">Conference Organizers</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: .12</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1529921&CFID=105750519&CFTOKEN=91765132">Preface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: xi</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.10001" title="DOI">10.1109/ICMI.2002.10001</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1529921&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1529922&CFID=105750519&CFTOKEN=91765132">Conference Organizers</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: xii</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.10000" title="DOI">10.1109/ICMI.2002.10000</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1529922&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847727&CFID=105750519&CFTOKEN=91765132">Layered Representations for Human Activity Recognition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100082450&CFID=105750519&CFTOKEN=91765132">Nuria Oliver</a>, 
                        <a href="author_page.cfm?id=81100323543&CFID=105750519&CFTOKEN=91765132">Eric Horvitz</a>, 
                        <a href="author_page.cfm?id=81100630127&CFID=105750519&CFTOKEN=91765132">Ashutosh Garg</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 3</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166960" title="DOI">10.1109/ICMI.2002.1166960</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847727&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow5" style="display:inline;"><br /><div style="display:inline">We present the use of layered probabilistic representations using Hidden Markov Models for performing sensing, learning, and inference at multiple levels of temporal granularity. We describe the use of the representation in a system that diagnoses states ...</div></span>
          <span id="toHide5" style="display:none;"><br /><div style="display:inline">We present the use of layered probabilistic representations using Hidden Markov Models for performing sensing, learning, and inference at multiple levels of temporal granularity. We describe the use of the representation in a system that diagnoses states of a user's activity based on real-time streams of evidence from video, acoustic, and computer interactions. We review the representation, present an implementation, and report on experiments with the layered representation in an office-awareness application.</div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847721&CFID=105750519&CFTOKEN=91765132">Evaluating Integrated Speech- and Image Understanding</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100148661&CFID=105750519&CFTOKEN=91765132">C. Bauckhage</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 9</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166961" title="DOI">10.1109/ICMI.2002.1166961</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847721&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow6" style="display:inline;"><br /><div style="display:inline">The capability to coordinate and interrelate speech and vision is a virtual prerequisite for adaptive, cooperative, and flexible interaction among people. It is therefore to assume that human-machine interaction, too, would benefit from intelligent interfaces ...</div></span>
          <span id="toHide6" style="display:none;"><br /><div style="display:inline">The capability to coordinate and interrelate speech and vision is a virtual prerequisite for adaptive, cooperative, and flexible interaction among people. It is therefore to assume that human-machine interaction, too, would benefit from intelligent interfaces for integrated speech and image processing. In this paper, we first sketch an interactive system that integrates automatic speech processing with image understanding. Then, we concentrate on performance assessment which we believe is an emerging key issue in multimodal interaction. We explain the benefit of time scale analysis and usability studies and evaluate our system accordingly.</div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847725&CFID=105750519&CFTOKEN=91765132">Techniques for Interactive Audience Participation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100068218&CFID=105750519&CFTOKEN=91765132">Dan Maynes-Aminzade</a>, 
                        <a href="author_page.cfm?id=81100493478&CFID=105750519&CFTOKEN=91765132">Randy Pausch</a>, 
                        <a href="author_page.cfm?id=81407593498&CFID=105750519&CFTOKEN=91765132">Steve Seitz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 15</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166962" title="DOI">10.1109/ICMI.2002.1166962</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847725&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow7" style="display:inline;"><br /><div style="display:inline">At SIGGRAPH in 1991, Loren and Rachel Carpenter unveiled an interactive entertainment system that allowed members of a large audience to control an onscreen game using red and green reflective paddles. In the spirit of this approach, we present a new ...</div></span>
          <span id="toHide7" style="display:none;"><br /><div style="display:inline">At SIGGRAPH in 1991, Loren and Rachel Carpenter unveiled an interactive entertainment system that allowed members of a large audience to control an onscreen game using red and green reflective paddles. In the spirit of this approach, we present a new set of techniques that enable members of an audience to participate, either cooperatively or competitively, in shared entertainment experiences. Our techniques allow audiences with hundreds of people to control onscreen activity by (1) leaning left and right in their seats, (2) batting a beach ball while its shadow is used as a pointing device, and (3) pointing laser pointers at the screen. All of these techniques can be implemented with inexpensive, off the shelf hardware. We have tested these techniques with a variety of audiences; in this paper we describe both the computer vision based implementation and the lessons we learned about designing effective content for interactive audience participation.</div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847724&CFID=105750519&CFTOKEN=91765132">Perceptual Collaboration in Neem</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100242831&CFID=105750519&CFTOKEN=91765132">P. Barthelmess</a>, 
                        <a href="author_page.cfm?id=81406595676&CFID=105750519&CFTOKEN=91765132">C. A. Ellis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 21</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166963" title="DOI">10.1109/ICMI.2002.1166963</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847724&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow8" style="display:inline;"><br /><div style="display:inline">The Neem Platform is a research test bed for Project Neem, concerned with the development of socially and culturally aware collaborative systems in a wide range of domains.In this paper we discuss a novel use of Perceptual Interfaces, applied to group ...</div></span>
          <span id="toHide8" style="display:none;"><br /><div style="display:inline">The Neem Platform is a research test bed for Project Neem, concerned with the development of socially and culturally aware collaborative systems in a wide range of domains.In this paper we discuss a novel use of Perceptual Interfaces, applied to group collaboration support. In Neem, the multimodal content of human to human interaction is analyzed and reasoned upon. Applications react to this implicit communication by dynamically adapting their behavior according to the perceived group context. In contrast, PerceptualInterfaces have been traditionally employed to handle explicit (multimodal) commands from users, and are as a rule not concerned with the communication that takes place among humans.The Neem Platform is a generic (application neutral) component-based evolvable framework that provides functionality that facilitates building such perceptual collaborative applications.</div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847722&CFID=105750519&CFTOKEN=91765132">A Tracking Framework for Collaborative Human Computer Interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100183174&CFID=105750519&CFTOKEN=91765132">E. Polat</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 27</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166964" title="DOI">10.1109/ICMI.2002.1166964</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847722&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow9" style="display:inline;"><br /><div style="display:inline">The ability to track multiple people and their body parts (i.e., face and hands) in a complex environment is crucial for designing a collaborative natural human computer interaction (HCI). One of the challenging issues in tracking body parts of people ...</div></span>
          <span id="toHide9" style="display:none;"><br /><div style="display:inline">The ability to track multiple people and their body parts (i.e., face and hands) in a complex environment is crucial for designing a collaborative natural human computer interaction (HCI). One of the challenging issues in tracking body parts of people is the data association uncertainty while assigning measurements to the proper tracks in the case of occlusion and close interaction of body parts of different people. This paper describes a framework for tracking body parts of people in 2D/3D using multiple hypothesis tracking (MHT) algorithm. A path coherence function has been incorporated along with MHT to reduce the negative effects of closely spaced measurements that produce unconvincing tracks and needless computations. The performance of the framework has been validated using experiments on real sequence of images.</div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847723&CFID=105750519&CFTOKEN=91765132">A Structural Approach to Distance Rendering in Personal Auditory Displays</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100083982&CFID=105750519&CFTOKEN=91765132">Federico Fontana</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 33</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166965" title="DOI">10.1109/ICMI.2002.1166965</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847723&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow10" style="display:inline;"><br /><div style="display:inline">A virtual resonating environment aiming at enhancing our perception of distance is proposed. This environment reproduces the acoustics inside a tube, thus conveying peculiar distance cues to the listener. The corresponding resonator has been prototyped ...</div></span>
          <span id="toHide10" style="display:none;"><br /><div style="display:inline">A virtual resonating environment aiming at enhancing our perception of distance is proposed. This environment reproduces the acoustics inside a tube, thus conveying peculiar distance cues to the listener. The corresponding resonator has been prototyped using a wave-based numerical scheme called Waveguide Mesh, that gave the necessary versatility to the model during the design and parameterization of the listening environment. Psychophysical tests show that this virtual environment conveys robust distance cues.</div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847726&CFID=105750519&CFTOKEN=91765132">A Multimodal Electronic Travel Aid Device</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100124753&CFID=105750519&CFTOKEN=91765132">A. Panuccio</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 39</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166966" title="DOI">10.1109/ICMI.2002.1166966</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847726&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow11" style="display:inline;"><br /><div style="display:inline">This paper describes an Electronic Travel Aid device, that may enable blind individuals to "see the world with their ears". A wearable prototype will be assembled using low-cost hardware: earphones, sunglasses fitted with two micro cameras, and a palmtop ...</div></span>
          <span id="toHide11" style="display:none;"><br /><div style="display:inline">This paper describes an Electronic Travel Aid device, that may enable blind individuals to "see the world with their ears". A wearable prototype will be assembled using low-cost hardware: earphones, sunglasses fitted with two micro cameras, and a palmtop computer. The system, which currently runs on a desktop computer, is able to detect the light spot produced by a laser pointer, compute its angular position and depth, and generate a corresponding sound providing the auditory cues for the perception of the position and distance of the pointed surface patch. It permits different sonification modes that can be chosen by drawing,with the laser pointer, a predefined stroke which will be recognized by a Hidden Markov Model. In this way the blind person can use a common pointer as a replacement of the cane and will interact with the device by using a flexible and natural sketch based interface.</div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847758&CFID=105750519&CFTOKEN=91765132">Lecture and Presentation Tracking in an Intelligent Meeting Room</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100113771&CFID=105750519&CFTOKEN=91765132">Ivica Rogina</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 47</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166967" title="DOI">10.1109/ICMI.2002.1166967</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847758&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow12" style="display:inline;"><br /><div style="display:inline">Archiving, indexing, and later browsing through stored presentations and lectures is a task that can be observed with a growing frequency. We have investigated the special problems and advantages of lectures and propose the design and adaptation of a ...</div></span>
          <span id="toHide12" style="display:none;"><br /><div style="display:inline">Archiving, indexing, and later browsing through stored presentations and lectures is a task that can be observed with a growing frequency. We have investigated the special problems and advantages of lectures and propose the design and adaptation of a speech recognizer towards a lecture such that the recognition accurucy can be significantly improved by prior analysis of the presented documents using a special class-based language model. We defined a tracking accuracy measure which measures how well a system can automatically align recognized words with parts of a presentation and show that by prior exploitation of the presented documents, the trucking accuracy can be improved. The system described in this paper is part of an intelligent meeting room developed in the European- Union-sponsored project FAME (Facilitating Agent for Multicultural Exchange).</div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847762&CFID=105750519&CFTOKEN=91765132">Parallel Computing-Based Architecture for Mixed-Initiative Spoken Dialogue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418600407&CFID=105750519&CFTOKEN=91765132">Ryuta Taguma</a>, 
                        <a href="author_page.cfm?id=81418593841&CFID=105750519&CFTOKEN=91765132">Tatsuhiro Moriyama</a>, 
                        <a href="author_page.cfm?id=81337490187&CFID=105750519&CFTOKEN=91765132">Koji Iwano</a>, 
                        <a href="author_page.cfm?id=81100289983&CFID=105750519&CFTOKEN=91765132">Sadaoki Furui</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 53</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166968" title="DOI">10.1109/ICMI.2002.1166968</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847762&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow13" style="display:inline;"><br /><div style="display:inline">This paper describes a new method of implementing mixed-initiative spoken dialogue systems based on parallel computing architecture. In a mixed-initiative dialogue, the user as well as the system needs to be capable of controlling the dialogue sequence. ...</div></span>
          <span id="toHide13" style="display:none;"><br /><div style="display:inline">This paper describes a new method of implementing mixed-initiative spoken dialogue systems based on parallel computing architecture. In a mixed-initiative dialogue, the user as well as the system needs to be capable of controlling the dialogue sequence. In our implementation, various language models corresponding to different dialogue contents, such as requests for information or replies to the system, are built and multiple recognizers using these language models are driven under a parallel computing architecture. The dialogue content of the user is automatically detected based on likelihood scores given by the recognizers, and the content is used to build the dialogue. A transitional probability from one dialogue state uttering a kind of content to another state uttering a different content is incorporated into the likelihood score. A flexible dialogue structured that gives users the initiative to control the dialogue is implemented by this architecture. Real-time dialogue systems for retrieving information about restraunts and food stores are built and evaluated in terms of dialogue content identification rate and keyword accuracy. the proposed architecture has the advantage that the dialogue system can be easily modified without remaking the whole language model.</div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847761&CFID=105750519&CFTOKEN=91765132">3-D N-Best Search for Simultaneous Recognition of Distant-Talking Speech of Multiple Talkers</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100584410&CFID=105750519&CFTOKEN=91765132">Satoshi Nakamura</a>, 
                        <a href="author_page.cfm?id=81309492953&CFID=105750519&CFTOKEN=91765132">Panikos Heracleous</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 59</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166969" title="DOI">10.1109/ICMI.2002.1166969</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847761&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow14" style="display:inline;"><br /><div style="display:inline">A microphone array is the promising solution for realizing hands-free speech recognition in real environments. Accurate talker localization is very important for speech recognition using the microphone array. However localization of a moving talker is ...</div></span>
          <span id="toHide14" style="display:none;"><br /><div style="display:inline">A microphone array is the promising solution for realizing hands-free speech recognition in real environments. Accurate talker localization is very important for speech recognition using the microphone array. However localization of a moving talker is difficult in noisy reverberantenvironments. The talker localization errors degrade the performance of speech recognition. To solve the problem, we proposed a new speech recognition algorithm which considers multiple talker direction hypotheses simultaneously [2]. The proposed algorithm performs Viterbi search in 3-dimensional trellis space composed of talker directions, input frames, and HMM states. In this paper we describe a new simultaneous recognition algorithm of distant-talking speech of multiple talkers using the extended 3-D N-best search algorithm. The algorithm exploits a path distance-based clustering and a likelihood normalization technique appeared to be necessary in order to build an efficient system for our purpose. We evaluated the proposed method using reverberated data, which are those simulated by the image method and recorded in a real room. The image method was used to know the accuracy-reverberation time relationship, and the real data was used to evaluate the real performance of our algorithm. The obtained Top 3 results of the Simultaneous Word Accuracy was 73.02% under 162ms reverberation time and using the image method.</div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847759&CFID=105750519&CFTOKEN=91765132">Integration of Tone Related Feature for Chinese Speech Recognition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418597616&CFID=105750519&CFTOKEN=91765132">Pui-Fung Wong</a>, 
                        <a href="author_page.cfm?id=81418593552&CFID=105750519&CFTOKEN=91765132">Man-Hung Siu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 64</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166970" title="DOI">10.1109/ICMI.2002.1166970</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847759&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow15" style="display:inline;"><br /><div style="display:inline">Chinese is a tonal language that uses fundamental frequency, in addition to phones for word differentiation. Commonly used front-end features, such as Mel-Frequency Cepstral Coefficients (MFCC), however, are optimized for non-tonal languages such as ...</div></span>
          <span id="toHide15" style="display:none;"><br /><div style="display:inline">Chinese is a tonal language that uses fundamental frequency, in addition to phones for word differentiation. Commonly used front-end features, such as Mel-Frequency Cepstral Coefficients (MFCC), however, are optimized for non-tonal languages such as English and are not mainly focused on pitch information that is important for tone identification. In this paper, we examine the integration of tone-related acoustic features for Chinese recognition. We propose the use of Cepstrum Method (CEP), which uses the same configurations as in MFCC extraction, for the extraction of pitch-related features. The pitch periods extracted from the CEP algorithm can be used directly for speech recognition and do not require any special treatment for unvoiced frames. In addition, we explore a number of feature transformations and find that the addition of a properly normalized and transformed set of pitch related-features can reduce the recognition error rate from 34.61% to 29.45% on the Chinese 1998 National Performance Assessment (Project 863) corpus.</div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847760&CFID=105750519&CFTOKEN=91765132">Talking Heads: Which Matching between Faces and Synthetic Voices?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100149384&CFID=105750519&CFTOKEN=91765132">Marc Mersiol</a>, 
                        <a href="author_page.cfm?id=81381602537&CFID=105750519&CFTOKEN=91765132">Noel Chateau</a>, 
                        <a href="author_page.cfm?id=81384592835&CFID=105750519&CFTOKEN=91765132">Valerie Maffiolo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 69</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166971" title="DOI">10.1109/ICMI.2002.1166971</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847760&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow16" style="display:inline;"><br /><div style="display:inline">The integration of synthetic faces and text-to-speech voice synthesis (what we call "talking heads") allows new applications in the area of man-machine interfaces. In a close future, talking heads might be useful communicative interface agents. But before ...</div></span>
          <span id="toHide16" style="display:none;"><br /><div style="display:inline">The integration of synthetic faces and text-to-speech voice synthesis (what we call "talking heads") allows new applications in the area of man-machine interfaces. In a close future, talking heads might be useful communicative interface agents. But before making an extensive use of talking heads, several issues have to be checked according to their acceptability by users. An important issue is to make sure that the used synthetic voices match to their faces. The scope of this paper is to study the coherence that might exist between synthetic voices and faces. Twenty-four subjects rated the coherence of all the combinations between ten faces and six voices. The main results of this paper show that not all associations between faces and voices are relevant and that some associations are better rated than others according to qualitative criteria.</div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847756&CFID=105750519&CFTOKEN=91765132">Robust Noisy Speech Recognition with Adaptive Frequency Bank Selection</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100052491&CFID=105750519&CFTOKEN=91765132">Dajin Lu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 75</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166972" title="DOI">10.1109/ICMI.2002.1166972</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847756&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow17" style="display:inline;"><br /><div style="display:inline">With the development of automatic speech recognition technology, the robustness problem of speech recognition system is becoming more and more important. This paper addresses the problem of speech recognition in additive background noise environment. ...</div></span>
          <span id="toHide17" style="display:none;"><br /><div style="display:inline">With the development of automatic speech recognition technology, the robustness problem of speech recognition system is becoming more and more important. This paper addresses the problem of speech recognition in additive background noise environment. Since the frequency energy of different types of noise focuses on different frequency banks, the effect of additive noise on each frequency bank are different. The seriously obscured frequency banks have little word signal information left, and are harmful for subsequence speech processing. Wu et al.[1] applied the frequency bank selection theory to robust word boundary detection in noise environment, and obtained good detection results. In this paper, this theory is extended to noisy speech recognition. Unlike the standard MFCC which uses all frequency banks for cepstral coefficients, we only use the frequency banks that are slightest corruptedand discard the seriously obscured ones. Cepstral coefficients are calculated only on the selected frequency banks. Moreover, acoustic model is also adapted to match the modification of acoustic feature. Experiments on continuous digital speech recognition show that the proposed algorithm leads to better performance than spectral subtraction and cepstral mean normalization at low SNRs</div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847757&CFID=105750519&CFTOKEN=91765132">Covariance-Tied Clustering Method In Speaker Identification</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418596136&CFID=105750519&CFTOKEN=91765132">ZhiQiang Wang</a>, 
                        <a href="author_page.cfm?id=81418597815&CFID=105750519&CFTOKEN=91765132">Yang Liu</a>, 
                        <a href="author_page.cfm?id=81418593264&CFID=105750519&CFTOKEN=91765132">Peng Ding</a>, 
                        <a href="author_page.cfm?id=81418598036&CFID=105750519&CFTOKEN=91765132">Xu Bo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 81</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166973" title="DOI">10.1109/ICMI.2002.1166973</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847757&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow18" style="display:inline;"><br /><div style="display:inline">Gaussian mixture models (GMMs) have been successfully applied to the classier for speaker modeling in speaker identification. However, there are still problems to solve, such as the clustering methods. Conditional K-Means Algorithm utilizes Euclidean ...</div></span>
          <span id="toHide18" style="display:none;"><br /><div style="display:inline">Gaussian mixture models (GMMs) have been successfully applied to the classier for speaker modeling in speaker identification. However, there are still problems to solve, such as the clustering methods. Conditional K-Means Algorithm utilizes Euclidean distance taking all datadistribution as sphericity, which is not the distribution of the actual data. In this paper we present a new method to make use of covariance information to direct the clustering of GMMs, namely covariance-tied clustering. This method is consisted of two parts: obtaining thecovariance matrices using data sharing technique based on binary tree and making use of the covariance matrices to direct clustering. The experiments results prove that this method leads to worthwhile reductions of error rates in speaker identification. Much remains to be done toexplore fully the covariance information.</div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847747&CFID=105750519&CFTOKEN=91765132">Context-Based Multimodal Input Understanding in Conversational Systems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100205070&CFID=105750519&CFTOKEN=91765132">Joyce Chai</a>, 
                        <a href="author_page.cfm?id=81100484491&CFID=105750519&CFTOKEN=91765132">Shimei Pan</a>, 
                        <a href="author_page.cfm?id=81100264735&CFID=105750519&CFTOKEN=91765132">Michelle X. Zhou</a>, 
                        <a href="author_page.cfm?id=81309508814&CFID=105750519&CFTOKEN=91765132">Keith Houck</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 87</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166974" title="DOI">10.1109/ICMI.2002.1166974</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847747&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow19" style="display:inline;"><br /><div style="display:inline">In a multimodal human-machine conversation, user inputs are often abbreviated or imprecise. Sometimes, only fusing multimodal inputs together cannot derive a complete understanding. To address these inadequacies, we are building a semantics-based multimodal ...</div></span>
          <span id="toHide19" style="display:none;"><br /><div style="display:inline">In a multimodal human-machine conversation, user inputs are often abbreviated or imprecise. Sometimes, only fusing multimodal inputs together cannot derive a complete understanding. To address these inadequacies, we are building a semantics-based multimodal interpretationframework called MIND (Multimodal Interpretation for Natural Dialog). The unique feature of MIND is the use of a variety of contexts (e.g., domain context and conversation context) to enhance multimodal fusion. In this paper, we present a semantic rich modeling scheme and a context-based approach that enable MIND to gain a full understanding of user inputs, including those ambiguous and incomplete ones.</div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847746&CFID=105750519&CFTOKEN=91765132">Context-Sensitive Help for Multimodal Dialogue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100277911&CFID=105750519&CFTOKEN=91765132">Helen Wright Hastie</a>, 
                        <a href="author_page.cfm?id=81329489623&CFID=105750519&CFTOKEN=91765132">Michael Johnston</a>, 
                        <a href="author_page.cfm?id=81351604966&CFID=105750519&CFTOKEN=91765132">Patrick Ehlen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 93</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166975" title="DOI">10.1109/ICMI.2002.1166975</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847746&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow20" style="display:inline;"><br /><div style="display:inline">Multimodal interfaces offer users unprecedented flexibility in choosing a style of interaction. However, users are frequently unaware of or forget shorter or more effective multimodal or pen-based commands. This paper describes a working help system ...</div></span>
          <span id="toHide20" style="display:none;"><br /><div style="display:inline">Multimodal interfaces offer users unprecedented flexibility in choosing a style of interaction. However, users are frequently unaware of or forget shorter or more effective multimodal or pen-based commands. This paper describes a working help system that leverages the capabilities of a multimodal interface in order to provide targeted, unobtrusive, context-sensitive help. This Multimodal Help System guides the user to the most effective way to specify a request, providing transferable knowledge that can be used in future requests without repeatedly invoking the help system.</div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847744&CFID=105750519&CFTOKEN=91765132">Referring to Objects with Spoken and Haptic Modalities</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100517448&CFID=105750519&CFTOKEN=91765132">Fr&#233;d&#233;ric Landragin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 99</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166976" title="DOI">10.1109/ICMI.2002.1166976</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847744&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow21" style="display:inline;"><br /><div style="display:inline">The gesture input modality considered in multimodal dialogue systems is mainly reduced to pointing or manipulating actions. With an approach based on the spontaneous character of the communication, the treatment of such actions involves many processes. ...</div></span>
          <span id="toHide21" style="display:none;"><br /><div style="display:inline">The gesture input modality considered in multimodal dialogue systems is mainly reduced to pointing or manipulating actions. With an approach based on the spontaneous character of the communication, the treatment of such actions involves many processes. Without any constraints, the user may use gesture in association with speech and may exploit the visual context peculiarities, guiding his articulation of gesture trajectories and his choices of words. The semantic interpretation of multimodal utterances also becomes a complex problem, taking into account varieties of referring expressions, varieties of gestural trajectories, structural parameters from the visual context, and also directives from a specific task.Following the spontaneous approach, we propose to give the maximal understanding capabilities to dialogue systems, to ensure that various interaction modes must be taken into account. Considering the development of haptic devices (as PHANToM) which increase the capabilities of sensations, particularly tactile and kinesthetic ones, we propose to explore a new domain of research concerning the integration of haptic gesture into multimodal dialogue systems, in terms of its possible associations with speech for objects reference and manipulation. We focus in this paper on the compatibility between haptic gesture and multimodal reference models, and on the consequences of processing this new modality on intelligent system architectures, which is not yet enough studied from a semantic point of view.</div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847738&CFID=105750519&CFTOKEN=91765132">Towards Visually-Grounded Spoken Language Acquisition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100653347&CFID=105750519&CFTOKEN=91765132">Deb Roy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 105</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166977" title="DOI">10.1109/ICMI.2002.1166977</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847738&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow22" style="display:inline;"><br /><div style="display:inline">A characteristic shared by most approaches to natural language understanding and generation is the use of symbolic representations of word and sentence meanings. Frames and semantic nets are examples of symbolic representations. Symbolic methods are ...</div></span>
          <span id="toHide22" style="display:none;"><br /><div style="display:inline">A characteristic shared by most approaches to natural language understanding and generation is the use of symbolic representations of word and sentence meanings. Frames and semantic nets are examples of symbolic representations. Symbolic methods are inappropriate for applications which require natural language semantics to be linked to perception, as is the case in tasks such as scene description or human-robot interaction. This paper presents two implemented systems, one that learns to generate, and one that learns to understand visually-grounded spoken language. These implementations are part of our ongoing effort to develop a comprehensive model of perceptually-grounded semantics.</div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847741&CFID=105750519&CFTOKEN=91765132">Modeling Output in the EMBASSI Multimodal Dialog System</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100305062&CFID=105750519&CFTOKEN=91765132">Gregor M&#246;hler</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 111</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166978" title="DOI">10.1109/ICMI.2002.1166978</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847741&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow23" style="display:inline;"><br /><div style="display:inline">In this paper we present the concept for an abstract modeling of output render components. We illustrate how this categorization serves to seamlessly integrate previously unknown output multimodalities coherently into the multimodal presentations of ...</div></span>
          <span id="toHide23" style="display:none;"><br /><div style="display:inline">In this paper we present the concept for an abstract modeling of output render components. We illustrate how this categorization serves to seamlessly integrate previously unknown output multimodalities coherently into the multimodal presentations of the EMBASSI dialog system. We present a case study and conclude with an overview of related work.</div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847743&CFID=105750519&CFTOKEN=91765132">Multimodal Dialogue Systems for Interactive TVApplications</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100633757&CFID=105750519&CFTOKEN=91765132">Aseel Ibrahim</a>, 
                        <a href="author_page.cfm?id=81100568572&CFID=105750519&CFTOKEN=91765132">Pontus Johansson</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 117</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166979" title="DOI">10.1109/ICMI.2002.1166979</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847743&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow24" style="display:inline;"><br /><div style="display:inline">Many studies have shown the advantages of building multimodal systems, however not in the interactive TV application context. This paper reports on a qualitative study of a multimodal program guide for interactive TV. The system was designed by adding ...</div></span>
          <span id="toHide24" style="display:none;"><br /><div style="display:inline">Many studies have shown the advantages of building multimodal systems, however not in the interactive TV application context. This paper reports on a qualitative study of a multimodal program guide for interactive TV. The system was designed by adding speech interaction to an already existing TV program guide. Study results indicate that spoken natural language input combined with visual output is preferable for TV applications. Furthermore, the user feedback requires a clear distinction between the dialogue system's domain result and system status in the visual output. Consequently, we propose an interaction model that consists of three entities: user, domain results, and system feedback.</div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847740&CFID=105750519&CFTOKEN=91765132">Human - Robot Interaction: Engagement between Humans and Robots for Hosting Activities</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100223737&CFID=105750519&CFTOKEN=91765132">Candace L. Sidner</a>, 
                        <a href="author_page.cfm?id=81339497984&CFID=105750519&CFTOKEN=91765132">Myroslava Dzikovska</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 123</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166980" title="DOI">10.1109/ICMI.2002.1166980</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847740&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow25" style="display:inline;"><br /><div style="display:inline">To participate in conversations with people, robots must not only see and talk with people but make use of the conventions of conversation and of how to be connected to their human counterparts. This paper reports on research on engagement in human-human ...</div></span>
          <span id="toHide25" style="display:none;"><br /><div style="display:inline">To participate in conversations with people, robots must not only see and talk with people but make use of the conventions of conversation and of how to be connected to their human counterparts. This paper reports on research on engagement in human-human interaction and applications to (non-autonomous) robots interacting with humans in hosting activities.</div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847739&CFID=105750519&CFTOKEN=91765132">Viewing and Analyzing Multimodal Human-computer Tutorial Dialogue: A Database Approach</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100200615&CFID=105750519&CFTOKEN=91765132">Jack Mostow</a>, 
                        <a href="author_page.cfm?id=81100558536&CFID=105750519&CFTOKEN=91765132">Joseph Beck</a>, 
                        <a href="author_page.cfm?id=81418593840&CFID=105750519&CFTOKEN=91765132">Raghu Chalasani</a>, 
                        <a href="author_page.cfm?id=81418596925&CFID=105750519&CFTOKEN=91765132">Andrew Cuneo</a>, 
                        <a href="author_page.cfm?id=81100499393&CFID=105750519&CFTOKEN=91765132">Peng Jia</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 129</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166981" title="DOI">10.1109/ICMI.2002.1166981</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847739&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow26" style="display:inline;"><br /><div style="display:inline">It is easier to record logs of multimodal human-computer tutorial dialogue than to make sense of them. In the 2000-2001 school year, we logged the interactions of approximately 400 students who used Project LISTEN's Reading Tutor and who read aloud over ...</div></span>
          <span id="toHide26" style="display:none;"><br /><div style="display:inline">It is easier to record logs of multimodal human-computer tutorial dialogue than to make sense of them. In the 2000-2001 school year, we logged the interactions of approximately 400 students who used Project LISTEN's Reading Tutor and who read aloud over 2.4 million words. This paper discusses some difficulties we encountered converting the logs into a more easily understandable database. It is faster to write SQL queries to answer research questions than to analyze complex log files each time. The database also permits us to construct a viewer to examine individual Reading Tutor-student interactions. This combination of queriesand viewable data has turned out to be very powerful, and we discuss how we have combined them to answer research questions.</div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847745&CFID=105750519&CFTOKEN=91765132">Adaptive Dialog Based upon Multimodal Language Acquisition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100538735&CFID=105750519&CFTOKEN=91765132">James Flanagan</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 135</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166982" title="DOI">10.1109/ICMI.2002.1166982</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847745&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow27" style="display:inline;"><br /><div style="display:inline">Communicating by voice with speech-enabled computer applications based on preprogrammed rule grammers suffers from constrained vocabulary and sentence structures. Deviations from the allowed language result in an unrecognized utterance that will not ...</div></span>
          <span id="toHide27" style="display:none;"><br /><div style="display:inline">Communicating by voice with speech-enabled computer applications based on preprogrammed rule grammers suffers from constrained vocabulary and sentence structures. Deviations from the allowed language result in an unrecognized utterance that will not be understood and processed by the system. One way to alleviate this restriction consists in allowing the user to expand the computer's recognized and understood language by teaching the computer system new language knowledge. We present an adaptive dialog system capable of learning from users new words, phrases and sentences, and their corresponding meanings. User input incorporates multiple modalities, including speaking, typing, pointing, drawing, and image capturing. The allowed language can thus be expanded in real time by users according to their preferences. By acquiring new language knowledge the system becomes more capable in specific tasks, although its language is still constrained.</div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847742&CFID=105750519&CFTOKEN=91765132">Integrating Emotional Cues into a Framework for Dialogue Management</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100312995&CFID=105750519&CFTOKEN=91765132">Hartwig Holzapfel</a>, 
                        <a href="author_page.cfm?id=81100154196&CFID=105750519&CFTOKEN=91765132">Christian Fuegen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 141</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166983" title="DOI">10.1109/ICMI.2002.1166983</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847742&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow28" style="display:inline;"><br /><div style="display:inline">Emotions are very important in human-human communication but are usually ignored in human-computer interaction. Recent work focuses on recognition and generation of emotions as well as emotion driven behavior. Our work focuses on the use of emotions ...</div></span>
          <span id="toHide28" style="display:none;"><br /><div style="display:inline">Emotions are very important in human-human communication but are usually ignored in human-computer interaction. Recent work focuses on recognition and generation of emotions as well as emotion driven behavior. Our work focuses on the use of emotions in dialogue systems that can be used with speech input or as well in multi-modal environments.This paper describes a framework for using emotional cues in a dialogue system and their informational characterization. We describe emotion models that can be integrated into the dialogue system and can be used in different domains and tasks. Our application of the dialogue system is planned to model multi-modal human-computer-interaction with a humanoid robotic system.</div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847734&CFID=105750519&CFTOKEN=91765132">Data Driven Design of an ANN/HMM System for On-line Unconstrained Handwritten Character Recognition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100479976&CFID=105750519&CFTOKEN=91765132">Haifeng Li</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 149</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166984" title="DOI">10.1109/ICMI.2002.1166984</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847734&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow29" style="display:inline;"><br /><div style="display:inline">This paper is dedicated to data driven design method for a hybrid ANN / HMM based handwriting recognition system. On one hand, a data driven designed neural modelling of handwriting primitives is proposed. ANNs are firstly used as state models in a HMM ...</div></span>
          <span id="toHide29" style="display:none;"><br /><div style="display:inline">This paper is dedicated to data driven design method for a hybrid ANN / HMM based handwriting recognition system. On one hand, a data driven designed neural modelling of handwriting primitives is proposed. ANNs are firstly used as state models in a HMM primitivedivider that associates each signal frame with an ANN by minimizing the accumulated prediction error. Then, the neural modelling is realized by training each network on its own frame set. Organizing these two steps in an EM algorithm, precise primitive models are obtained. On the other hand, a data driven systematic method is proposed for HMM topology inference task. All possible prototypes of a pattern class are firstly merged into several clustersby a Tabu search aided clustering algorithm. Then a multiple parallel-path HMM is constructed for the pattern class. Experiments prove an 8% recognition improvement with a saving of 50% of system resources, compared to an intuitively designed referential ANN / HMM system.</div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847733&CFID=105750519&CFTOKEN=91765132">Gesture Patterns during Speech Repairs</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81408598841&CFID=105750519&CFTOKEN=91765132">Lei Chen</a>, 
                        <a href="author_page.cfm?id=81100139151&CFID=105750519&CFTOKEN=91765132">Mary Harper</a>, 
                        <a href="author_page.cfm?id=81100361835&CFID=105750519&CFTOKEN=91765132">Francis Quek</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 155</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166985" title="DOI">10.1109/ICMI.2002.1166985</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847733&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow30" style="display:inline;"><br /><div style="display:inline">Speech and gesture are two primary modes used in natural human communication; hence, they are important inputs for a multimodal interface to process. One of the challenges for multimodal interfaces is to accurately recognize the words in spontaneous ...</div></span>
          <span id="toHide30" style="display:none;"><br /><div style="display:inline">Speech and gesture are two primary modes used in natural human communication; hence, they are important inputs for a multimodal interface to process. One of the challenges for multimodal interfaces is to accurately recognize the words in spontaneous speech. This is partly due to the presence of speech repairs, which seriously degrade the accuracy of current speech recognition systems. Based on the assumption that speech and gesture arise from same thought process, we would expect to find patterns of gesture that co-occur with speech repairs that can be exploited by a multimodal processing system to more effectively process spontaneous speech.To evaluate this hypothesis, we have conducted a measurement study of gesture and speech repair data extracted from videotapes of natural dialogs. Although we have found that gestures do not always co-occur with speech repairs, we observed that modification gesture patterns have a high correlation with content replacement speech repairs, but rarely occur with content repetitions. These results suggest that gesture patterns can help us to classify different types of speech repairs in order to correct them more accurately [6].</div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847737&CFID=105750519&CFTOKEN=91765132">Prosody Based Co-analysis for Continuous Recognition of Coverbal Gestures</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100493220&CFID=105750519&CFTOKEN=91765132">Sanshzar Kettebekov</a>, 
                        <a href="author_page.cfm?id=81408601901&CFID=105750519&CFTOKEN=91765132">Mohammed Yeasin</a>, 
                        <a href="author_page.cfm?id=81100236445&CFID=105750519&CFTOKEN=91765132">Rajeev Sharma</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 161</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166986" title="DOI">10.1109/ICMI.2002.1166986</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847737&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow31" style="display:inline;"><br /><div style="display:inline">Although recognition of natural speech and gestures have been studied extensively, previous attempts of combining them in a unified framework to boost classification were mostly semantically motivated, e.g., keyword-gesture co-occurrence. Such formulations ...</div></span>
          <span id="toHide31" style="display:none;"><br /><div style="display:inline">Although recognition of natural speech and gestures have been studied extensively, previous attempts of combining them in a unified framework to boost classification were mostly semantically motivated, e.g., keyword-gesture co-occurrence. Such formulations inherit the complexity of natural language processing. This paper presents a Bayesian formulation that uses a phenomenon of gesture and speech articulation for improving accuracy of automatic recognition of continuous coverbal gestures. The prosodic features from the speech signal were co-analyzed with the visual signal to learn the prior probability of co-occurrence of the prominent spoken segments with the particular kinematical phases of gestures. It was found that the above co-analysis helps in detecting and disambiguating small hand movements, which subsequently improves the rate of continuous gesture recognition. The efficacy of the proposed approach was demonstrated on a large database collected from the weather channel broadcast. This formulation opens new avenues for bottom-up frameworks of multimodal integration.</div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847730&CFID=105750519&CFTOKEN=91765132">Purdue RVL-SLLL ASL Database for Automatic Recognition of American Sign Language</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100568845&CFID=105750519&CFTOKEN=91765132">Avi C. Kak</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 167</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166987" title="DOI">10.1109/ICMI.2002.1166987</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847730&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow32" style="display:inline;"><br /><div style="display:inline">This article reports on an extensive database of American Sign Language (ASL) motions, handshapes, words and sentences. Research on automatic recognition of ASL requires a suitable database for the training and the testing of algorithms. The databases ...</div></span>
          <span id="toHide32" style="display:none;"><br /><div style="display:inline">This article reports on an extensive database of American Sign Language (ASL) motions, handshapes, words and sentences. Research on automatic recognition of ASL requires a suitable database for the training and the testing of algorithms. The databases that are currently available do not allow, for algorithmic development that requires a step-by-step approach to ASL recognition -from the recognition of individual hand-shapes, to the recognition of motion primitives, and, finally, to the recognition of full sentences. We have sought to remove these deficiencies in a new database  Purdue RVL-SLLL ASL database.</div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847736&CFID=105750519&CFTOKEN=91765132">The Role of Gesture in Multimodal Referring Actions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100517448&CFID=105750519&CFTOKEN=91765132">Fr&#233;d&#233;ric Landragin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 173</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166988" title="DOI">10.1109/ICMI.2002.1166988</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847736&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow33" style="display:inline;"><br /><div style="display:inline">When deictic gestures are produced on a touch screen, they can take forms which can lead to several sorts of ambiguities. Considering that the resolution of a multimodal reference requires the identification of the referents and of the context ("reference ...</div></span>
          <span id="toHide33" style="display:none;"><br /><div style="display:inline">When deictic gestures are produced on a touch screen, they can take forms which can lead to several sorts of ambiguities. Considering that the resolution of a multimodal reference requires the identification of the referents and of the context ("reference domain") from which these referents are extracted, we focus on the linguistic, gestural, and visual clues that a dialogue system may exploit to comprehend the referring intention. We explore the links between words, gestures and perceptual groups, doing so in terms of the clues that delimit the reference domain. We also show the importance of taking the domain into account for dialogue management, particularly for the comprehension of further utterances, when they seem to implicitly use a pre-existing restriction to a subset of objects. We propose a strategy of multimodal reference resolution based on this notion of reference domain, and we illustrate its efficiency with prototypic examples built from a study of significant referring situations extracted from a corpus. We give at last the future directions of our works concerning some linguistic and task aspects that are not integrated here.</div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847735&CFID=105750519&CFTOKEN=91765132">Hand Gesture Symmetric Behavior Detection and Analysis in Natural Conversation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100313933&CFID=105750519&CFTOKEN=91765132">Yingen Xiong</a>, 
                        <a href="author_page.cfm?id=81100361835&CFID=105750519&CFTOKEN=91765132">Francis Quek</a>, 
                        <a href="author_page.cfm?id=81100130227&CFID=105750519&CFTOKEN=91765132">David McNeill</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 179</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166989" title="DOI">10.1109/ICMI.2002.1166989</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847735&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow34" style="display:inline;"><br /><div style="display:inline">We present an experimental investigation into the phenomenon of gestural symmetry for two-handed gestures accompanying speech. We describe an approach to compute hand motion symmetries based on the correlation computations. Local symmetries are detected ...</div></span>
          <span id="toHide34" style="display:none;"><br /><div style="display:inline">We present an experimental investigation into the phenomenon of gestural symmetry for two-handed gestures accompanying speech. We describe an approach to compute hand motion symmetries based on the correlation computations. Local symmetries are detected using a windowing operation. We demonstrate that the selection of a smaller window size results in better sensitivity to local symmetries at the expense of noise in the form of spurious symmetries and symmetry dropoffs'. Our algorithmapplies a hole filling' post process to address these detection problems. We examine the role of the detected motion symmetries of two-handed gestures in the structuring of speech. We compared discourse segments corresponding to extracted symmetries in two natural conversations against a discourse analysis by expert psycholinguistic coders. These comparisons illustrate the effectiveness of the symmetry feature for the under-standing of underlying discourse structure. We believe thatthis basic characteristic of two-handed gestures accompanying speech must be incorporated in any multimodal interaction system involving two-handed gestures and speech.</div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847732&CFID=105750519&CFTOKEN=91765132">A Multi-Class Pattern Recognition System for Practical Finger Spelling Translation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100298281&CFID=105750519&CFTOKEN=91765132">Jose L. Hernandez-Rebollar</a>, 
                        <a href="author_page.cfm?id=81100438849&CFID=105750519&CFTOKEN=91765132">Robert W. Lindeman</a>, 
                        <a href="author_page.cfm?id=81100146010&CFID=105750519&CFTOKEN=91765132">Nicholas Kyriakopoulos</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 185</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166990" title="DOI">10.1109/ICMI.2002.1166990</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847732&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow35" style="display:inline;"><br /><div style="display:inline">This paper presents a portable system and method for recognizing the 26 hand shapes of the American Sign Language alphabet, using a novel glove-like device. Two additional signs, 'space', and 'enter' are added to the alphabet to allow the user to form ...</div></span>
          <span id="toHide35" style="display:none;"><br /><div style="display:inline">This paper presents a portable system and method for recognizing the 26 hand shapes of the American Sign Language alphabet, using a novel glove-like device. Two additional signs, 'space', and 'enter' are added to the alphabet to allow the user to form words or phrases andsend them to a speech synthesizer. Since the hand shape for a letter varies from one signer to another, this is a 28-class pattern recognition system. A three-level hierarchical classifier divides the problem into "dispatchers" and "recognizers." After reducing pattern dimension from ten to three, the projection of class distributions onto horizontal planes makes it possible to apply simple linear discrimination in 2D, and Bayes' Rule in those cases where classes had features with overlapped distributions. Twenty-one out of 26 letters were recognized with 100% accuracy; the worst case, letter U, achieved 78%.</div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847731&CFID=105750519&CFTOKEN=91765132">A Map-Based System Using Speech and 3D Gestures for Pervasive Computing</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100504716&CFID=105750519&CFTOKEN=91765132">Andrea Corradini</a>, 
                        <a href="author_page.cfm?id=81418595068&CFID=105750519&CFTOKEN=91765132">Richard M. Wesson</a>, 
                        <a href="author_page.cfm?id=81100149852&CFID=105750519&CFTOKEN=91765132">Philip R. Cohen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 191</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166991" title="DOI">10.1109/ICMI.2002.1166991</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847731&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow36" style="display:inline;"><br /><div style="display:inline">We describe an augmentation of Quickset, a multimodal voice/pen system that allows users to create and control map-based, collaborative, interactive simulations. In this paper, we report on our extension of the graphical pen input mode from stylus/mouse ...</div></span>
          <span id="toHide36" style="display:none;"><br /><div style="display:inline">We describe an augmentation of Quickset, a multimodal voice/pen system that allows users to create and control map-based, collaborative, interactive simulations. In this paper, we report on our extension of the graphical pen input mode from stylus/mouse to 3D hand movements. To do this, the map is projected onto a virtual plane in space, specified by the operator beforethe start of the interactive session. We then use our geometric model to compute the intersection of hand movements with the virtual plane, translating these into map coordinates on the appropriate system. The goal of this research is the creation of a body-centered, multimodal architecture employing both speech and 3D hand gestures, which seamlessly and unobtrusively supports distributed interaction. The augmented system, built on top of an existing architecture, also provides an improved visualization, management and awareness of a shared understanding. Potential applications of this work include tele-medicine, battlefield management and any kind of collaborative decision-making during which users may wish to be mobile.</div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847728&CFID=105750519&CFTOKEN=91765132">Hand Tracking Using Spatial Gesture Modeling and Visual Feedback for a Virtual DJ System</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100080550&CFID=105750519&CFTOKEN=91765132">Andy Cassidy</a>, 
                        <a href="author_page.cfm?id=81100379244&CFID=105750519&CFTOKEN=91765132">Dan Hook</a>, 
                        <a href="author_page.cfm?id=81100219731&CFID=105750519&CFTOKEN=91765132">Avinash Baliga</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 197</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166992" title="DOI">10.1109/ICMI.2002.1166992</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847728&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow37" style="display:inline;"><br /><div style="display:inline">The ability to accurately track hand movement provides new opportunities for human computer interaction (HCI). Many of today's commercial hand tracking devices based on gloves can be cumbersome and expensive. An approach that avoids these problems is ...</div></span>
          <span id="toHide37" style="display:none;"><br /><div style="display:inline">The ability to accurately track hand movement provides new opportunities for human computer interaction (HCI). Many of today's commercial hand tracking devices based on gloves can be cumbersome and expensive. An approach that avoids these problems is to use computer vision to capture hand motion.In this paper, we present a complete real-time hand tracking and 3-D modeling system based on a single camera. In our system, we extract feature points from a video stream of a hand to control a virtual hand model with 2-D global motion and 3-D local motion. The on screen model gives the user instant feedback on the estimated position of the hand. This visual feedback allows a user to compensate for the errors in tracking.The system is used for three example applications. The first application uses hand tracking and gestures to take on the role of the mouse. The second interacts with a 3D virtual environment using the 3D hand model. The last application is a virtual DJ system that is controlled by hand motion tracking and gestures.</div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847729&CFID=105750519&CFTOKEN=91765132">State Sharing in a Hybrid Neuro-Markovian On-Line Handwriting Recognition System through a Simple Hierarchical Clustering Algorithm</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100480049&CFID=105750519&CFTOKEN=91765132">Haifeng Li</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 203</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166993" title="DOI">10.1109/ICMI.2002.1166993</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847729&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow38" style="display:inline;"><br /><div style="display:inline">HMM has been largely applied in many fields with great successes. To achieve a better performance, an easy way is using more states or more free parameters for a better signal modelling. Thus, state sharing and state clipping methods have been proposed ...</div></span>
          <span id="toHide38" style="display:none;"><br /><div style="display:inline">HMM has been largely applied in many fields with great successes. To achieve a better performance, an easy way is using more states or more free parameters for a better signal modelling. Thus, state sharing and state clipping methods have been proposed to reduceparameter redundancy and to limit the explosive consummation of system resources. In this paper, we focus here on a simple state sharing method for a hybrid neuro-markovian on-line handwriting recognition system. At first, a likelihood-based distance is proposed for measuring the similarity between two HMM state models. After wards, a minimum quantification error aimed hierarchical clustering algorithm is also proposed to select the most representative models. Here, models are shared to the most under the constraint of the minimum system performance loss. As the result, we maintain about 98% of the system performance while about 60% of the parameters are reduced.</div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847755&CFID=105750519&CFTOKEN=91765132">An Automatic Speech Translation System on PDAs for Travel Conversation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100361183&CFID=105750519&CFTOKEN=91765132">Hiroaki Hattori</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 211</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166995" title="DOI">10.1109/ICMI.2002.1166995</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847755&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow39" style="display:inline;"><br /><div style="display:inline">We present an automatic speech-to-speech translation system for Personal Digital Assistants (PDAs) that helps oral communication between Japanese and English speakers in various situations while traveling. Our original compact large vocabulary continuous ...</div></span>
          <span id="toHide39" style="display:none;"><br /><div style="display:inline">We present an automatic speech-to-speech translation system for Personal Digital Assistants (PDAs) that helps oral communication between Japanese and English speakers in various situations while traveling. Our original compact large vocabulary continuous speech recognition engine, compact translation engine based on a lexicalized grammar, and compact Japanese speech synthesis engine lead to the development of a Japanese/English bi-directional speech translation system that works with limited computational resources.</div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847754&CFID=105750519&CFTOKEN=91765132">A PDA-Based Sign Translator</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418598307&CFID=105750519&CFTOKEN=91765132">Jing Zhang</a>, 
                        <a href="author_page.cfm?id=81418592425&CFID=105750519&CFTOKEN=91765132">Xilin Chen</a>, 
                        <a href="author_page.cfm?id=81350589970&CFID=105750519&CFTOKEN=91765132">Jie Yang</a>, 
                        <a href="author_page.cfm?id=81100599954&CFID=105750519&CFTOKEN=91765132">Alex Waibel</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 217</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166996" title="DOI">10.1109/ICMI.2002.1166996</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847754&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow40" style="display:inline;"><br /><div style="display:inline">In this paper, we propose an effective approach for a PDA-based sign system, and it presents user the sign translator. Its main functions include 3 parts: detection, recognition and translation. Automatic detection and recognition of text in natural ...</div></span>
          <span id="toHide40" style="display:none;"><br /><div style="display:inline">In this paper, we propose an effective approach for a PDA-based sign system, and it presents user the sign translator. Its main functions include 3 parts: detection, recognition and translation. Automatic detection and recognition of text in natural scenes is a prerequisite forautomatic sign translator. In order to make the system robust for text detection in various natural scenes, the detection approach efficiently embeds multi-resolution, adaptive search in a hierarchical framework with different emphases at each layer. We also introduce an intensity-based OCR method to recognize character in various fonts and lighting condition, where we employ Gabor transform to obtain local features, and LDA for selection and classification of features. The recognition rate is 92.4% for the testing set got from the natural sign. Sign is different from the normal used sentence. It is brief, with a lot of abbreviations and place nouns. We here only briefly introduce a rule-based place name translation. We have integrated all these functions in a PDA, which can capture sign image, auto segment andrecognize the Chinese sign, and translate it into English.</div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847752&CFID=105750519&CFTOKEN=91765132">The NESPOLE! Multimodal Interface for Cross-lingual Communication - Experience and Lessons Learned</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100438179&CFID=105750519&CFTOKEN=91765132">Erica Costantini</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 223</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166997" title="DOI">10.1109/ICMI.2002.1166997</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847752&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow41" style="display:inline;"><br /><div style="display:inline">In this paper we describe the design, evolution, and development of the user interface components of the NESPOLE! speech-to-speech translation system. The NESPOLE! system was designed for users with medium-to-low levels of computer literacy and web expertise. ...</div></span>
          <span id="toHide41" style="display:none;"><br /><div style="display:inline">In this paper we describe the design, evolution, and development of the user interface components of the NESPOLE! speech-to-speech translation system. The NESPOLE! system was designed for users with medium-to-low levels of computer literacy and web expertise. The user interface was designed to effectively combine web browsing, real-time sharing of graphical information and multi-modal annotations using a shared whiteboard, and real-time multilingual speech communication, all within an e-commerce scenario. Data collected in sessions with na&#239;ve users in several stages in the process of system development formed the basis for improving the effectiveness and usability of the system. We describe this development process, the resulting interface components and the lessons learned.</div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847750&CFID=105750519&CFTOKEN=91765132">Research of Machine Learning Method for Specific Information Recognition on the Internet</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81387591974&CFID=105750519&CFTOKEN=91765132">Dequan Zheng</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 229</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166998" title="DOI">10.1109/ICMI.2002.1166998</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847750&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow42" style="display:inline;"><br /><div style="display:inline">With the available resources on the Internet becoming plentiful, a large amount of harmfulinformation is permeating in and has been influencing people's normal work and living seriously. Therefore, some harmful data stream must be recognized and filtered ...</div></span>
          <span id="toHide42" style="display:none;"><br /><div style="display:inline">With the available resources on the Internet becoming plentiful, a large amount of harmfulinformation is permeating in and has been influencing people's normal work and living seriously. Therefore, some harmful data stream must be recognized and filtered out effectively.After analyzing some harmful contents in Internet information stream, we present a new method, which recognizes specific information by Machine Learning (ML). We extracted key information from a number of corpuses through ML method to obtain the part of speech (POS) Transfer-Form for key information by learning from corpuses, which is based on the same pronunciation matching of key information. Further more, the testing value of key information will be obtained in real corpus to examine the likelihood between matching rules from information streams and those learnt from corpuses through the average value of POS transfer probability of key information. Therefore, the testing value for the whole real data stream will be obtained. The experiment proved that the method was efficient for recognizing certainInternet harmful information.</div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847749&CFID=105750519&CFTOKEN=91765132">The Added Value of Multimodality in the NESPOLE! Speech-to-Speech Translation System: an Experimental Study</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100438179&CFID=105750519&CFTOKEN=91765132">Erica Costantini</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 235</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1166999" title="DOI">10.1109/ICMI.2002.1166999</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847749&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow43" style="display:inline;"><br /><div style="display:inline">Multimodal interfaces, which combine two or more input modes (speech, pen, touch), are expected to be more efficient, natural and usable than single-input interfaces. However, the advantage of multimodal input has only been ascertained in highly controlled ...</div></span>
          <span id="toHide43" style="display:none;"><br /><div style="display:inline">Multimodal interfaces, which combine two or more input modes (speech, pen, touch), are expected to be more efficient, natural and usable than single-input interfaces. However, the advantage of multimodal input has only been ascertained in highly controlled experimentalconditions [4, 5, 6]; in particular, we lack data about what happens with real' human-human, multilingual communication systems. In this work we discuss the results of an experiment aiming to evaluate the added value of multimodality in a "true" speech-to-speech translation system, the NESPOLE! system, which provides for multilingual and multimodal communication in the tourism domain, allowing users to interact through the internet sharing maps, web-pages and pen-based gestures. We compared two experimental conditions differing as to whether multimodal resources were available: a speech-only condition (SO), and a multimodal condition (MM). Most of the data show tendencies for MM to be better than SO.</div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847748&CFID=105750519&CFTOKEN=91765132">Multi-Modal Translation System and Its Evaluation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100066959&CFID=105750519&CFTOKEN=91765132">Shigeo Morishima</a>, 
                        <a href="author_page.cfm?id=81100584410&CFID=105750519&CFTOKEN=91765132">Satoshi Nakamura</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 241</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167000" title="DOI">10.1109/ICMI.2002.1167000</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847748&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow44" style="display:inline;"><br /><div style="display:inline">Speech-to-speech translation has been studied to realize natural human communication beyond language barriers. Toward further multi-modal natural communication, visual information such as face and lip movements will be necessary. In this paper, we introduce ...</div></span>
          <span id="toHide44" style="display:none;"><br /><div style="display:inline">Speech-to-speech translation has been studied to realize natural human communication beyond language barriers. Toward further multi-modal natural communication, visual information such as face and lip movements will be necessary. In this paper, we introduce a multi-modal English-to-Japanese and Japanese-to-English translation system that also translates the speaker's speech motion while synchronizing it to the translated speech. To retain the speaker's facial expression, we substitute only the speech organ's image with the synthesized one, which is made by a three-dimensional wire-frame model that is adaptable to any speaker. Our approach enables image synthesis and translation with an extremely small database. We conduct subjective evaluation tests using the connected digit discrimination test using data with and without audio-visual lip-synchronization. The results confirm the significant quality of the proposed audio-visual translation system and the importance of lip-synchronization.</div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847751&CFID=105750519&CFTOKEN=91765132">Towards Universal Speech Recognition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418596135&CFID=105750519&CFTOKEN=91765132">Zhirong Wang</a>, 
                        <a href="author_page.cfm?id=81418593429&CFID=105750519&CFTOKEN=91765132">Umut Topkara</a>, 
                        <a href="author_page.cfm?id=81100622802&CFID=105750519&CFTOKEN=91765132">Tanja Schultz</a>, 
                        <a href="author_page.cfm?id=81100599954&CFID=105750519&CFTOKEN=91765132">Alex Waibel</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 247</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167001" title="DOI">10.1109/ICMI.2002.1167001</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847751&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow45" style="display:inline;"><br /><div style="display:inline">The increasing interest in multilingual applications like speech-to-speech translation systems is accompanied by the need for speech recognition front-ends in many languages that can also handle multiple input languages at the same time. In this paper ...</div></span>
          <span id="toHide45" style="display:none;"><br /><div style="display:inline">The increasing interest in multilingual applications like speech-to-speech translation systems is accompanied by the need for speech recognition front-ends in many languages that can also handle multiple input languages at the same time. In this paper we describe a universal speech recognition system that fulfills such needs. It is trained by sharing speech and text data across languages and thus reduces the number of parameters and overhead significantly at the cost of only slight accuracy loss. The final recognizer eases the burden of maintaining several monolingual engines, makes dedicated language identification obsolete and allows for code-switching within an utterance. To achieve these goals we developed new methods forconstructing multilingual acoustic models and multilingual n-gram language models.</div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847753&CFID=105750519&CFTOKEN=91765132">Improved Named Entity Translation and Bilingual Named Entity Extraction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81451592930&CFID=105750519&CFTOKEN=91765132">Fei Huang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 253</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167002" title="DOI">10.1109/ICMI.2002.1167002</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847753&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow46" style="display:inline;"><br /><div style="display:inline">Translation of named entities (NE), including proper names, temporal and numerical expressions, is very important in multilingual natural language processing, like crosslingual information retrieval and statistical machine translation. In this paper ...</div></span>
          <span id="toHide46" style="display:none;"><br /><div style="display:inline">Translation of named entities (NE), including proper names, temporal and numerical expressions, is very important in multilingual natural language processing, like crosslingual information retrieval and statistical machine translation. In this paper we present an integrated approach to extract a named entity translation dictionary from a bilingual corpus while at the same time improving the named entity annotation quality.Starting from a bilingual corpus where the named entities are extracted independently for each language, a statistical alignment model is used to align the named entities. An iterative process is applied to extract named entity pairs with higher alignment probability. This leads to a smaller but cleaner named entity translation dictionary and also to a significant improvement of the monolingual named entity annotation quality for both languages. Experimental result shows that the dictionary size is reduced by 51.8% and the annotation quality is improved from70.03 to 78.15 for Chinese and 73.38 to 81.46 in terms of F-score.</div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847698&CFID=105750519&CFTOKEN=91765132">Active Gaze Tracking for Human-Robot Interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100090341&CFID=105750519&CFTOKEN=91765132">Rowel Atienza</a>, 
                        <a href="author_page.cfm?id=81100303406&CFID=105750519&CFTOKEN=91765132">Alexander Zelinsky</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 261</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167004" title="DOI">10.1109/ICMI.2002.1167004</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847698&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow47" style="display:inline;"><br /><div style="display:inline">In our effort to make human-robot interfaces more user-friendly, we built an active gaze tracking system that can measure a person's gaze direction in real-time. Gaze normally tells which object in his/her surrounding a person is interested in. Therefore, ...</div></span>
          <span id="toHide47" style="display:none;"><br /><div style="display:inline">In our effort to make human-robot interfaces more user-friendly, we built an active gaze tracking system that can measure a person's gaze direction in real-time. Gaze normally tells which object in his/her surrounding a person is interested in. Therefore, it can be used as a medium for human-robot interaction like instructing a robot arm to pick a certain object a user is looking at. In this paper, we discuss how we developed and put together algorithms for zoom camera calibration, low-level control of active head, face and gaze tracking to create an active gaze tracking system.</div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847695&CFID=105750519&CFTOKEN=91765132">3-D Articulated Pose Tracking for Untethered Diectic Reference</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100236659&CFID=105750519&CFTOKEN=91765132">David Demirdjian</a>, 
                        <a href="author_page.cfm?id=81100537374&CFID=105750519&CFTOKEN=91765132">Trevor Darrell</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 267</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167005" title="DOI">10.1109/ICMI.2002.1167005</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847695&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow48" style="display:inline;"><br /><div style="display:inline">Arm and body pose are useful cues for diectic reference-users naturally extend their arms to objects of interest in a dialog. We present recent progress on untethered sensing of articulated arm and body configuration using robust stereo vision techniques. ...</div></span>
          <span id="toHide48" style="display:none;"><br /><div style="display:inline">Arm and body pose are useful cues for diectic reference-users naturally extend their arms to objects of interest in a dialog. We present recent progress on untethered sensing of articulated arm and body configuration using robust stereo vision techniques. These techniques allow robust, accurate, real-time tracking of 3-D position and orientation. We demonstrate users' performance with our system on object selection tasks and describe our initial efforts to integrate this system into a multimodal conversational dialog framework.</div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847693&CFID=105750519&CFTOKEN=91765132">Tracking Focus of Attention in Meetings</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100650593&CFID=105750519&CFTOKEN=91765132">Rainer Stiefelhagen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 273</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167006" title="DOI">10.1109/ICMI.2002.1167006</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847693&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow49" style="display:inline;"><br /><div style="display:inline">This paper presents an overview of our work on tracking focus of attention in meeting situations. We have developed a system capable of estimating participants' focus of attention from multiple cues. In our system we employ an omni-directional camera ...</div></span>
          <span id="toHide49" style="display:none;"><br /><div style="display:inline">This paper presents an overview of our work on tracking focus of attention in meeting situations. We have developed a system capable of estimating participants' focus of attention from multiple cues. In our system we employ an omni-directional camera to simultaneously track the faces of participants sitting around a meeting table and use neural networks to estimate their head poses. In addition, we use microphones to detect who is speaking. The system predicts participants' focus of attention from acoustic and visual information separately, and then combines the output of the audio- and video-based focus of attention predictors.In addition this work reports recent experimental results: In order to determine how well we can predict a subject's focus of attention solely on the basis of his or her head orientation, we have conducted an experiment in which we recorded head and eye orientations of participants in a meeting using special tracking equipment. Our results demonstrate that head orientation was a sufficient indicator of the subjects' focus target in 89% of the time. Furthermore we discuss how the neural networks used to estimate head orientation can be adapted to work in new locations and under new illumination conditions.</div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847699&CFID=105750519&CFTOKEN=91765132">A Probabilistic Dynamic Contour Model for Accurate and Robust Lip Tracking</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81409593093&CFID=105750519&CFTOKEN=91765132">Qiang Wang</a>, 
                        <a href="author_page.cfm?id=81100305817&CFID=105750519&CFTOKEN=91765132">Haizhou Ai</a>, 
                        <a href="author_page.cfm?id=81451595382&CFID=105750519&CFTOKEN=91765132">Guangyou Xu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 281</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167007" title="DOI">10.1109/ICMI.2002.1167007</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847699&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow50" style="display:inline;"><br /><div style="display:inline">In this paper a new Condensation style contour tracking method called Probabilistic Dynamic Contour (PDC) is proposed for lip tracking: a novel mixture dynamic model is designed to represent shape more compactly and to tolerate larger motions between ...</div></span>
          <span id="toHide50" style="display:none;"><br /><div style="display:inline">In this paper a new Condensation style contour tracking method called Probabilistic Dynamic Contour (PDC) is proposed for lip tracking: a novel mixture dynamic model is designed to represent shape more compactly and to tolerate larger motions between frames, a measurement model is designed to include multiple visual cues. The proposed PDC tracker has the advantage that it is conceptually general but effectively suitable for lip tracking with the designed dynamic and measurement model. The new tracker improves the traditional Condensation style tracker in three aspects: Firstly, the dynamic model is partially derived from the image sequence, so the tracker does not need to learn the dynamics in advance. Secondly, the measurement model is easy to be updated during tracking, which avoids modeling the foreground object in prior. Thirdly, to improve the tracker's speed, a compact representation of shape and a noise model are proposed to reduce the samples required to represent the posterior distribution. Experiment on lip contour tracking shows that the proposed method tracks contour robustly as well as accurately compare to the existing tracking method.</div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847694&CFID=105750519&CFTOKEN=91765132">Attentional Object Spotting by Integrating Multimodal Input</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100474961&CFID=105750519&CFTOKEN=91765132">Chen Yu</a>, 
                        <a href="author_page.cfm?id=81100635541&CFID=105750519&CFTOKEN=91765132">Dana H. Ballard</a>, 
                        <a href="author_page.cfm?id=81100413890&CFID=105750519&CFTOKEN=91765132">Shenghuo Zhu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 287</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167008" title="DOI">10.1109/ICMI.2002.1167008</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847694&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow51" style="display:inline;"><br /><div style="display:inline">An intelligent human-computer interface is expected to allow computers to work with users in a cooperative manner. To achieve this goal, computers need to be aware of user attention and provide assistances without explicit user request. Cognitive studies ...</div></span>
          <span id="toHide51" style="display:none;"><br /><div style="display:inline">An intelligent human-computer interface is expected to allow computers to work with users in a cooperative manner. To achieve this goal, computers need to be aware of user attention and provide assistances without explicit user request. Cognitive studies of eye movements suggest that in accomplishing well-learned tasks, the performer's focus of attention is locked with the ongoing work and more than 90% of eye movements are closely related to the objects beingmanipulated in the tasks. In light of this, we have developed an attentional object spotting system that integrates multimodal data consisting of eye positions, head positions and video from the "first-person" perspective. To detect the user's focus of attention, we modeled eye gaze and head movements using a hidden Markov model (HMM) representation. For each attentional point in time, the object of user interest is automatically extracted and recognized. We report the results of experiments on finding attentional objects in the natural task of "making a peanut-butter sandwich".</div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847696&CFID=105750519&CFTOKEN=91765132">Lip Tracking for MPEG-4 Facial Animation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418592672&CFID=105750519&CFTOKEN=91765132">Zhilin Wu</a>, 
                        <a href="author_page.cfm?id=81336487587&CFID=105750519&CFTOKEN=91765132">Petar S. Aleksic</a>, 
                        <a href="author_page.cfm?id=81350591848&CFID=105750519&CFTOKEN=91765132">Aggelos K. Katsaggelos</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 293</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167009" title="DOI">10.1109/ICMI.2002.1167009</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847696&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow52" style="display:inline;"><br /><div style="display:inline">It is very important to accurately track the mouth of a talking person for many applications, such as face recognition and human computer interaction. This is in general a difficult problem due to the complexity of shapes, colors, textures, and changing ...</div></span>
          <span id="toHide52" style="display:none;"><br /><div style="display:inline">It is very important to accurately track the mouth of a talking person for many applications, such as face recognition and human computer interaction. This is in general a difficult problem due to the complexity of shapes, colors, textures, and changing lighting conditions. In this paper we develop techniques for outer and inner lip tracking. From the tracking results FAPsare extracted which are used to drive an MPEG-4 decoder. A novel method consisting of a Gradient Vector Flow (GVF) snake with a parabolic template as an additional external force is proposed. Based on the results of the outer lip tracking, the inner lip is tracked using a similarity function and a temporal smoothness constraint. Numerical results are presented using the Bernstein database.</div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847700&CFID=105750519&CFTOKEN=91765132">Achieving Real-Time Lip Synch via SVM-Based Phoneme Classification and Lip Shape Refinement</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418597152&CFID=105750519&CFTOKEN=91765132">Taeyoon Kim</a>, 
                        <a href="author_page.cfm?id=81418598656&CFID=105750519&CFTOKEN=91765132">Yongsung Kang</a>, 
                        <a href="author_page.cfm?id=81319494773&CFID=105750519&CFTOKEN=91765132">Hanseok Ko</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 299</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167010" title="DOI">10.1109/ICMI.2002.1167010</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847700&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow53" style="display:inline;"><br /><div style="display:inline">In this paper, we develop a real time lip-synch system that activates 2-D avatar's lip motion in synch with incoming speech utterance. To realize the "real time" operation of the system, we contain the processing time by invoking merge and split procedure ...</div></span>
          <span id="toHide53" style="display:none;"><br /><div style="display:inline">In this paper, we develop a real time lip-synch system that activates 2-D avatar's lip motion in synch with incoming speech utterance. To realize the "real time" operation of the system, we contain the processing time by invoking merge and split procedure performing coarse-to-finephoneme classification. At each stage of phoneme classification, we apply the support vector machine (SVM) to constrain the computational load while attaining the desirable accuracy. The coarse-to-fine phoneme classification is accomplished via 2 stages of feature extraction, where each speech frame is acoustically analyzed first for 3 classes of lip opening using MFCC as feature and then a further refined classification for detailed lip shape using formant information. We implemented the system with a 2-D lip animation that shows the effectiveness of the proposed 2-stage procedure accomplishing the real-time lip-synch task.</div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847697&CFID=105750519&CFTOKEN=91765132">Multi-Modal Temporal Asynchronicity Modeling by Product HMMs for Robust</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418598364&CFID=105750519&CFTOKEN=91765132">Satoshi Nakamura</a>, 
                        <a href="author_page.cfm?id=81418597695&CFID=105750519&CFTOKEN=91765132">Ken'ichi Kumatani</a>, 
                        <a href="author_page.cfm?id=81100322256&CFID=105750519&CFTOKEN=91765132">Satoshi Tamura</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 305</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167011" title="DOI">10.1109/ICMI.2002.1167011</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847697&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow54" style="display:inline;"><br /><div style="display:inline">Recently demands for Audio-visual Speech Recognition (AVSR) has been increased in order to make the speech recognition system robust to acoustic noise. There are two kinds of research issues in the audio-visual speech recognition research such as integration ...</div></span>
          <span id="toHide54" style="display:none;"><br /><div style="display:inline">Recently demands for Audio-visual Speech Recognition (AVSR) has been increased in order to make the speech recognition system robust to acoustic noise. There are two kinds of research issues in the audio-visual speech recognition research such as integration modeling considering asynchronicity between modalities and adaptive information weighting according information reliability. This paper proposes a method to effectively integrate audio and visualinformation. Such integration inevitably necessitates modeling of the synchronization and asynchronization of the audio and visual information. To address the time lag and correlation problems in individual features between speech and lip movements, we introduce a type of integrated HMM modeling of audio-visual information based on a family of a product HMM. The proposed model can represent state synchronicity not only within a phoneme but also between phonemes. Furthermore, we also propose a rapid stream weight optimization based on GPD algorithm for noisy bi-modal speech recognition. Evaluation experiments show that the proposed method improves the recognition accuracy for noisy speech. In SNR=0dB our proposed method attained 16% higher performance compared to a product HMMs without the synchronicity re-estimation.</div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847774&CFID=105750519&CFTOKEN=91765132">A Multi-Modal Interface for an Interactive Simulated Vascular Reconstruction System</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100576679&CFID=105750519&CFTOKEN=91765132">E. V. Zudilova</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 313</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167013" title="DOI">10.1109/ICMI.2002.1167013</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847774&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow55" style="display:inline;"><br /><div style="display:inline">This paper is devoted to multi-modal interface design and implementation of a simulated vascular reconstruction system. It provides multi-modal interaction methods such as speech recognition, hand gestures, direct manipulation of virtual 3D objects and ...</div></span>
          <span id="toHide55" style="display:none;"><br /><div style="display:inline">This paper is devoted to multi-modal interface design and implementation of a simulated vascular reconstruction system. It provides multi-modal interaction methods such as speech recognition, hand gestures, direct manipulation of virtual 3D objects and measurement tools. The main challenge is that no general interface scenario in existence today can satisfy all the users of the system (radiologists, vascular surgeons, medical students, etc.). The potential users of the system can vary by their skills, expertise level, habits and psycho-motional characteristics. To make a multi-modal interface user-friendly is a crucial issue. In this paper we introduce an approach to develop such an efficient, user-friendly multi-modal interaction system. We focus on adaptive interaction as a possible solution to address the variety of end-users. Based on a user model, the adaptive user interface identifies each individual by means of a set of criteria and generates a customized exploration environment.</div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847778&CFID=105750519&CFTOKEN=91765132">Universal Interfaces to Multimedia Documents</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100288052&CFID=105750519&CFTOKEN=91765132">Ine Langer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 319</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167015" title="DOI">10.1109/ICMI.2002.1167015</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847778&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow56" style="display:inline;"><br /><div style="display:inline">Electronic documents theoretically have great advantages for people with print disabilities, although currently this potential is not being realized. This paper reports research to develop multimedia documents with universal interfaces which can be configured ...</div></span>
          <span id="toHide56" style="display:none;"><br /><div style="display:inline">Electronic documents theoretically have great advantages for people with print disabilities, although currently this potential is not being realized. This paper reports research to develop multimedia documents with universal interfaces which can be configured to the needs of people with a variety of print disabilities. The implications of enriching multimedia documents with additional and alternative single media objects is discussed and an implementation using HTML + TIME has been undertaken.</div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847770&CFID=105750519&CFTOKEN=91765132">A Video Based Interface to Textual Information for the Visually Impaired</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81339540222&CFID=105750519&CFTOKEN=91765132">Ali Zandifar</a>, 
                        <a href="author_page.cfm?id=81100166532&CFID=105750519&CFTOKEN=91765132">Antoine Chahine</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 325</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167016" title="DOI">10.1109/ICMI.2002.1167016</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847770&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow57" style="display:inline;"><br /><div style="display:inline">We describe the development of an interface to textual information for the visually impaired that uses video, image processing, optical-character-recognition (OCR) and text-to-speech(TTS). The video provides a sequence of low resolution images in which ...</div></span>
          <span id="toHide57" style="display:none;"><br /><div style="display:inline">We describe the development of an interface to textual information for the visually impaired that uses video, image processing, optical-character-recognition (OCR) and text-to-speech(TTS). The video provides a sequence of low resolution images in which text must be detected, rectified and converted into high resolution rectangular blocks that are capable of being analyzed via off-the-shelf OCR. To achieve this, various problems related to feature detection, mosaicing, auto-focus, zoom, and systems integration were solved in the development of the system, and these are described.</div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847779&CFID=105750519&CFTOKEN=91765132">Modular Approach of Multimodal Integration in a Virtual Environment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418599522&CFID=105750519&CFTOKEN=91765132">George N. Phillips Jr.</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 331</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167017" title="DOI">10.1109/ICMI.2002.1167017</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847779&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow58" style="display:inline;"><br /><div style="display:inline">We present a novel modular approach of integrating multiple input/output (I/O) modes in a virtual environment that imitate the natural, intuitive and effective human interaction behavior. The I/O modes that are used in this research are spatial tracking ...</div></span>
          <span id="toHide58" style="display:none;"><br /><div style="display:inline">We present a novel modular approach of integrating multiple input/output (I/O) modes in a virtual environment that imitate the natural, intuitive and effective human interaction behavior. The I/O modes that are used in this research are spatial tracking of two hands, fingers gesture recognition, head/body spatial tracking, voice recognition (discrete recognition for simple commands, and continuous recognition for natural language input), immersive stereo display and synthesized speech output. The intuitive natural interaction is achieved through several stages: identify all the tasks that need to be performed, group the similar tasks and assign them to a particular mode such that it imitates the physical world. This modular approach allows inclusion and removal of additional input and output modes as well as additional number of users easily. We described this multimodal interaction paradigm by applying it to a real world application: visualizing, modeling and fitting protein molecular structures in an immersive virtual environment.</div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847777&CFID=105750519&CFTOKEN=91765132">Mobile Multi-Modal Data Services for GPRS Phones and Beyond</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100421683&CFID=105750519&CFTOKEN=91765132">Georg Niklfeld</a>, 
                        <a href="author_page.cfm?id=81418596529&CFID=105750519&CFTOKEN=91765132">Michael Pucher</a>, 
                        <a href="author_page.cfm?id=81418594633&CFID=105750519&CFTOKEN=91765132">Robert Finan</a>, 
                        <a href="author_page.cfm?id=81418600072&CFID=105750519&CFTOKEN=91765132">Wolfgang Eckhart</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 337</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167018" title="DOI">10.1109/ICMI.2002.1167018</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847777&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow59" style="display:inline;"><br /><div style="display:inline">The paper discusses means to build multi-modal data services in existing GPRS infrastructures, and it puts the proposed simple solutions into the perspective of technologicalpossibilities that will become available in public mobile communications networks ...</div></span>
          <span id="toHide59" style="display:none;"><br /><div style="display:inline">The paper discusses means to build multi-modal data services in existing GPRS infrastructures, and it puts the proposed simple solutions into the perspective of technologicalpossibilities that will become available in public mobile communications networks over the next few years along the progression path from 2G/GSM systems, through GPRS, to 3G systems like UMTS, or equivalently to 802.11 networks. Three demonstrators are presented, which were developed by the authors in an application-oriented research project co-financed by telecommunications companies. The first two, push-to-talk address entry for a route-finder, and an open-microphone map-content navigator, simulate a UMTS or WLAN scenario. The third demonstrator implements a multi-modal map finder in a live public GPRS network usingWAP-Push. Some indications on usability are given. The paper argues for the importance of open, standards-based architectures that will spur attractive multi-modal services for the short term, as the current economic difficulties in the telecommunications industry put support for long term research into more advanced forms of multi-modality in question.</div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847772&CFID=105750519&CFTOKEN=91765132">Flexi-Modal and Multi-Machine User Interfaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100400744&CFID=105750519&CFTOKEN=91765132">Ben Bostwick</a>, 
                        <a href="author_page.cfm?id=81100536563&CFID=105750519&CFTOKEN=91765132">Edgar Seemann</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 343</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167019" title="DOI">10.1109/ICMI.2002.1167019</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847772&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow60" style="display:inline;"><br /><div style="display:inline">We describe our system which facilitates collaboration using multiple modalities, including speech, handwriting, gestures, gaze tracking, direct manipulation, large projected touch-sensitive displays, laser pointer tracking, regular monitors with a mouse ...</div></span>
          <span id="toHide60" style="display:none;"><br /><div style="display:inline">We describe our system which facilitates collaboration using multiple modalities, including speech, handwriting, gestures, gaze tracking, direct manipulation, large projected touch-sensitive displays, laser pointer tracking, regular monitors with a mouse and keyboard, and wirelessly-networked handhelds. Our system allows multiple, geographically dispersed participants to simultaneously and flexibly mix different modalities using the right interface at the right time on one or more machines. This paper discusses each of the modalities provided, how they were integrated in the system architecture, and how the user interface enabled one or more people to flexibly use one or more devices.</div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847776&CFID=105750519&CFTOKEN=91765132">A Real-Time Framework for Natural Multimodal Interaction with Large Screen Displays</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100539194&CFID=105750519&CFTOKEN=91765132">N. Krahnstoever</a>, 
                        <a href="author_page.cfm?id=81100493220&CFID=105750519&CFTOKEN=91765132">S. Kettebekov</a>, 
                        <a href="author_page.cfm?id=81408601901&CFID=105750519&CFTOKEN=91765132">M. Yeasin</a>, 
                        <a href="author_page.cfm?id=81100236445&CFID=105750519&CFTOKEN=91765132">R. Sharma</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 349</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167020" title="DOI">10.1109/ICMI.2002.1167020</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847776&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow61" style="display:inline;"><br /><div style="display:inline">This paper presents a framework for designing a natural multimodal human computer interaction (HCI) system. The core of the proposed framework is a principled method for combining information derived from audio and visual cues. To achieve natural interaction, ...</div></span>
          <span id="toHide61" style="display:none;"><br /><div style="display:inline">This paper presents a framework for designing a natural multimodal human computer interaction (HCI) system. The core of the proposed framework is a principled method for combining information derived from audio and visual cues. To achieve natural interaction, both audio and visual modalities are fused along with feedback through a large screen display. Careful design along with due considerations of possible aspects of a systems interaction cycle and integration has resulted in a successful system. The performance of the proposed framework has been validated through the development of several prototype systems as well as commercial applications for the retail and entertainment industry. To assess the impact of these multimodal systems (MMS), informal studies have been conducted. It was found that the system performed according to its specifications in 95% of the cases and that users showed ad-hoc proficiency, indicating natural acceptance of such systems.</div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847771&CFID=105750519&CFTOKEN=91765132">Embarking on Multimodal Interface Design</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100335897&CFID=105750519&CFTOKEN=91765132">Anoop K. Sinha</a>, 
                        <a href="author_page.cfm?id=81100103519&CFID=105750519&CFTOKEN=91765132">James A. Landay</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 355</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167021" title="DOI">10.1109/ICMI.2002.1167021</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847771&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow62" style="display:inline;"><br /><div style="display:inline">Designers are increasingly faced with the challenge of targeting multimodal applications, those that span heterogeneous devices and use multimodal input, but do not have tools to support them. We studied the early stage work practices of professional ...</div></span>
          <span id="toHide62" style="display:none;"><br /><div style="display:inline">Designers are increasingly faced with the challenge of targeting multimodal applications, those that span heterogeneous devices and use multimodal input, but do not have tools to support them. We studied the early stage work practices of professional multimodal interaction designers. We noted the variety of different artifacts produced, such as design sketches and paper prototypes. Additionally, we observed Wizard of Oz techniques that are sometimes used to simulate an interactive application from these sketches. These studies have led to our development of a technique for interface designers to consider as they embark on creating multimodal applications.</div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847775&CFID=105750519&CFTOKEN=91765132">Multi Modal User Interaction in an Automatic Pool Trainer</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81319495726&CFID=105750519&CFTOKEN=91765132">Lars Bo Larsen</a>, 
                        <a href="author_page.cfm?id=81418593304&CFID=105750519&CFTOKEN=91765132">Morten Damm Jensen</a>, 
                        <a href="author_page.cfm?id=81418599718&CFID=105750519&CFTOKEN=91765132">Wisdom Kobby Vodzi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 361</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167022" title="DOI">10.1109/ICMI.2002.1167022</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847775&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow63" style="display:inline;"><br /><div style="display:inline">This paper presents the human-computer interaction in an automatic pool trainer currently being developed at the Center for PersonKommunikation, Aalborg University.The aim of the system is to automate (parts of) the learning process, in this case of ...</div></span>
          <span id="toHide63" style="display:none;"><br /><div style="display:inline">This paper presents the human-computer interaction in an automatic pool trainer currently being developed at the Center for PersonKommunikation, Aalborg University.The aim of the system is to automate (parts of) the learning process, in this case of the game of pool. The Automated Pool Trainer (APT) utilises multi modal, agent driven user-system communication, to facilitate the user interaction.To allow the user the necessary freedom of movement when addressing the task, system output is presented on a wall-mounted screen and is augmented by a laser drawing lines and points directly on the pool table surface. User Interaction is either carried out via a spoken dialogue with an animated interface agent, or by using a touch screen panel.The paper describes the philosophy on which the system is designed, as well as the system architecture and individual modules. The user interaction is described and the paper concludes with a presentation of some test results and a discussion of the suitability of the presented and similar systems.</div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847773&CFID=105750519&CFTOKEN=91765132">Multimodal Contextual Car-Driver Interface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81452606879&CFID=105750519&CFTOKEN=91765132">Daniel Siewiorek</a>, 
                        <a href="author_page.cfm?id=81100393914&CFID=105750519&CFTOKEN=91765132">Asim Smailagic</a>, 
                        <a href="author_page.cfm?id=81418597158&CFID=105750519&CFTOKEN=91765132">Matthew Hornyak</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 367</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167023" title="DOI">10.1109/ICMI.2002.1167023</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847773&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow64" style="display:inline;"><br /><div style="display:inline">This paper focuses on the design and implementation of a Companion Contextual Car Driver interface that proactively assists the driver in managing information and communication. The prototype combines a smart car environment and driver state monitoring, ...</div></span>
          <span id="toHide64" style="display:none;"><br /><div style="display:inline">This paper focuses on the design and implementation of a Companion Contextual Car Driver interface that proactively assists the driver in managing information and communication. The prototype combines a smart car environment and driver state monitoring, incorporating a wide range of input-output modalities and a display hierarchy. Intelligent agents link information from many contexts, such as location and schedule, and transparently learn from the driver, interacting with the driver only when it is necessary.</div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847720&CFID=105750519&CFTOKEN=91765132">Requirements for Automatically Generating Multi-Modal Interfaces for Complex Appliances</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100204766&CFID=105750519&CFTOKEN=91765132">Jeffrey Nichols</a>, 
                        <a href="author_page.cfm?id=81100013136&CFID=105750519&CFTOKEN=91765132">Brad Myers</a>, 
                        <a href="author_page.cfm?id=81100116546&CFID=105750519&CFTOKEN=91765132">Thomas K. Harris</a>, 
                        <a href="author_page.cfm?id=81350571970&CFID=105750519&CFTOKEN=91765132">Roni Rosenfeld</a>, 
                        <a href="author_page.cfm?id=81100631361&CFID=105750519&CFTOKEN=91765132">Stefanie Shriver</a>, 
                        <a href="author_page.cfm?id=81100003896&CFID=105750519&CFTOKEN=91765132">Michael Higgins</a>, 
                        <a href="author_page.cfm?id=81100166305&CFID=105750519&CFTOKEN=91765132">Joseph Hughes</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 377</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167024" title="DOI">10.1109/ICMI.2002.1167024</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847720&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow65" style="display:inline;"><br /><div style="display:inline">Several industrial and academic research groups are working to simplify the control of appliances and services by creating a truly universal remote control. Unlike the preprogrammed remote controls available today, these new controllers download a specification ...</div></span>
          <span id="toHide65" style="display:none;"><br /><div style="display:inline">Several industrial and academic research groups are working to simplify the control of appliances and services by creating a truly universal remote control. Unlike the preprogrammed remote controls available today, these new controllers download a specification from the appliance or service and use it to automatically generate a remote control interface. This promises to be a useful approach because the specification can be made detailed enough to generate both speech and graphical interfaces. Unfortunately, generating good user interfaces can be difficult. Based on user studies and prototype implementations, this paper presents a set of requirements that we have found are needed for automatic interface generation systems to create high-quality user interfaces.</div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847715&CFID=105750519&CFTOKEN=91765132">Articulated Model Based People Tracking Using Motion Models</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81317498454&CFID=105750519&CFTOKEN=91765132">Huazhong Ning</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 383</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167025" title="DOI">10.1109/ICMI.2002.1167025</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847715&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow66" style="display:inline;"><br /><div style="display:inline">This paper focuses on acquisition of human motion data such as joint angles and velocity for applications of virtual reality, using both articulated body model and motion model in the CONDENSATION framework. Firstly, we learn a motion model represented ...</div></span>
          <span id="toHide66" style="display:none;"><br /><div style="display:inline">This paper focuses on acquisition of human motion data such as joint angles and velocity for applications of virtual reality, using both articulated body model and motion model in the CONDENSATION framework. Firstly, we learn a motion model represented by Gaussian distributions, and explore motion constraints by considering the dependency of motion parameters and represent them as conditional distributions. Then both of them are integrated into the dynamic model to concentrate factored sampling in the areas of state-space with mostposterior information. To measure the observing density with accuracy and robustness, a PEF (Pose Evaluation Function) modeled with a radial term is proposed. We also address the issue of automatic acquisition of initial model posture and recovery from severe failures. A large number of experiments on several persons demonstrate that our approach works well.</div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847714&CFID=105750519&CFTOKEN=91765132">Audiovisual Arrays for Untethered Spoken Interfaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100530904&CFID=105750519&CFTOKEN=91765132">Kevin Wilson</a>, 
                        <a href="author_page.cfm?id=81418594398&CFID=105750519&CFTOKEN=91765132">Vibhav Rangarajan</a>, 
                        <a href="author_page.cfm?id=81100662767&CFID=105750519&CFTOKEN=91765132">Neal Checka</a>, 
                        <a href="author_page.cfm?id=81100537374&CFID=105750519&CFTOKEN=91765132">Trevor Darrell</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 389</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167026" title="DOI">10.1109/ICMI.2002.1167026</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847714&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow67" style="display:inline;"><br /><div style="display:inline">When faced with a distant speaker at a known location in a noisy environment, a microphonearray can provide a significantly improved audio signal for speech recognition. Estimating thelocation of a speaker in a reverberant environment from audio information ...</div></span>
          <span id="toHide67" style="display:none;"><br /><div style="display:inline">When faced with a distant speaker at a known location in a noisy environment, a microphonearray can provide a significantly improved audio signal for speech recognition. Estimating thelocation of a speaker in a reverberant environment from audio information alone can be quitedifficult, so we use an array of video cameras to aid localization. Stereo processing techniques are used on pairs of cameras, and foreground 3-D points are grouped to estimate the trajectory of people as they move in an environment. These trajectories are used to guide a microphone array beamformer. Initial results using this system for speech recognition demonstrate increased recognition rates compared to non-array processing techniques.</div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847713&CFID=105750519&CFTOKEN=91765132">Fingerprint Classification by Directional Fields</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100216295&CFID=105750519&CFTOKEN=91765132">Sen Wang</a>, 
                        <a href="author_page.cfm?id=81418594195&CFID=105750519&CFTOKEN=91765132">Wei Wei Zhang</a>, 
                        <a href="author_page.cfm?id=81408602257&CFID=105750519&CFTOKEN=91765132">Yang Sheng Wang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 395</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167027" title="DOI">10.1109/ICMI.2002.1167027</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847713&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow68" style="display:inline;"><br /><div style="display:inline">Fingerprint classification provides an important fingerprint index and can reduce fingerprint matching time in large database. A good classification algorithm can give an accurate index that is able to search a fingerprint database more effectively.In ...</div></span>
          <span id="toHide68" style="display:none;"><br /><div style="display:inline">Fingerprint classification provides an important fingerprint index and can reduce fingerprint matching time in large database. A good classification algorithm can give an accurate index that is able to search a fingerprint database more effectively.In this paper, we present a fingerprint classification algorithm that is based on directional fields. We compute directional fields of fingerprint image and detect singular points (cores). Then, we extract features that we define from fingerprint image. We also use k-means classifier and 3-nearest neighbor to classify feature and distinguish which fingerprint is Arch, Left Loop, Right Loop, or Whorl. Experimental results show a significant improvement in fingerprint classification performance. Moreover, the time required for the classification algorithm is reduced.</div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847716&CFID=105750519&CFTOKEN=91765132">Towards Vision-Based 3-D People Tracking in a Smart Room</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100361914&CFID=105750519&CFTOKEN=91765132">Dirk Focken</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 400</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167028" title="DOI">10.1109/ICMI.2002.1167028</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847716&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow69" style="display:inline;"><br /><div style="display:inline">This paper presents our work on building a real time distributed system to track 3-D locations of people in an indoor environment, such as a smart room, using multiple calibrated cameras. In our system, each camera is connected to a dedicated computer ...</div></span>
          <span id="toHide69" style="display:none;"><br /><div style="display:inline">This paper presents our work on building a real time distributed system to track 3-D locations of people in an indoor environment, such as a smart room, using multiple calibrated cameras. In our system, each camera is connected to a dedicated computer on which foreground regions in the camera image are detected. This is done using an adpative background model. These detected foreground regions are broadcasted to a tracking agent, which computes believed 3-D locations of persons based on the detected image regions. We have implemented both a best-hypothesis heuristic tracking approach as well as a probabilistic multi-hypothesis tracker to find the object tracks from these 3-D locations. The two tracking approaches are evaluated on a sequence of two people walking in a conference room recorded with three cameras. The results suggest that the probabilistic tracker shows comparable performance to the heuristic tracker.</div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847710&CFID=105750519&CFTOKEN=91765132">Using TouchPad Pressure to Detect Negative Affect</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100393102&CFID=105750519&CFTOKEN=91765132">Helena M. Mentis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 406</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167029" title="DOI">10.1109/ICMI.2002.1167029</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847710&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow70" style="display:inline;"><br /><div style="display:inline">Humans naturally use behavioral cues in their interactions with other humans. The Media Equation proposes that these same cues are directed towards media, including computers. It is probable that detection of these cues by a computer during run-time ...</div></span>
          <span id="toHide70" style="display:none;"><br /><div style="display:inline">Humans naturally use behavioral cues in their interactions with other humans. The Media Equation proposes that these same cues are directed towards media, including computers. It is probable that detection of these cues by a computer during run-time could improve usability design and analysis. A preliminary experiment testing one of these cues, Synaptics TouchPad pressure, shows that behavioral cues can be used as a critical incident indicator by detecting negative affect.</div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847719&CFID=105750519&CFTOKEN=91765132">Designing Transition Networks for Multimodal VR-Interactions Using a Markup Language</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100286130&CFID=105750519&CFTOKEN=91765132">Marc Erich Latoschik</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 411</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167030" title="DOI">10.1109/ICMI.2002.1167030</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847719&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow71" style="display:inline;"><br /><div style="display:inline">This article presents one core component for enabling multimodal-speech and gesture-driven interaction in and for Virtual Environments. A so-called temporal Augmented Transition Network (tATN) is introduced. It allows to integrate and evaluate information ...</div></span>
          <span id="toHide71" style="display:none;"><br /><div style="display:inline">This article presents one core component for enabling multimodal-speech and gesture-driven interaction in and for Virtual Environments. A so-called temporal Augmented Transition Network (tATN) is introduced. It allows to integrate and evaluate information from speech, gesture, and a given application context using a combined syntactic/semantic parse approach. This tATN represents the target structure for a multimodal integration markup language (MIML). MIML centers around the specification of multimodal interactions by letting an application designer declare temporal and semantic relations between given input utterance percepts and certain application states in a declarative and portable manner. A subsequent parse pass translates MIML into corresponding tATNs which are directly loaded and executed by a simulation engines scripting facility.</div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847718&CFID=105750519&CFTOKEN=91765132">Musically Expressive Doll in Face-to-Face Communication</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100469647&CFID=105750519&CFTOKEN=91765132">Tomoko Yonezawa</a>, 
                        <a href="author_page.cfm?id=81100070056&CFID=105750519&CFTOKEN=91765132">Kenji Mase</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 417</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167031" title="DOI">10.1109/ICMI.2002.1167031</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847718&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow72" style="display:inline;"><br /><div style="display:inline">We propose an application that uses music as a multimodal expression to activate and support communication that runs parallel with traditional conversation. In this paper, we examine a personified doll-shaped interface designed for musical expression. ...</div></span>
          <span id="toHide72" style="display:none;"><br /><div style="display:inline">We propose an application that uses music as a multimodal expression to activate and support communication that runs parallel with traditional conversation. In this paper, we examine a personified doll-shaped interface designed for musical expression. To direct such gestures toward communication, we have adopted an augmented stuffed toy with tactile interaction as a musically expressive device. We constructed the doll with various sensors for user context recognition. This configuration enables translation of the interaction into melodic statements. We demonstrate the effect of the doll on face-to-face conversation by comparing the experimental results of different input interfaces and output sounds. Consequently, we have found that conversation with the doll was positively affected by the musical output, the doll interface, and their combination.</div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847717&CFID=105750519&CFTOKEN=91765132">Towards Monitoring Human Activities Using an Omnidirectional Camera</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418592425&CFID=105750519&CFTOKEN=91765132">Xilin Chen</a>, 
                        <a href="author_page.cfm?id=81350589970&CFID=105750519&CFTOKEN=91765132">Jie Yang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 423</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167032" title="DOI">10.1109/ICMI.2002.1167032</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847717&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow73" style="display:inline;"><br /><div style="display:inline">In this paper we propose an approach for monitoring human activities in an indoor environment using an omnidirectional camera. Robustly tracking people is prerequisite for modeling and recognizing human activities. An omnidirectional camera mounted on ...</div></span>
          <span id="toHide73" style="display:none;"><br /><div style="display:inline">In this paper we propose an approach for monitoring human activities in an indoor environment using an omnidirectional camera. Robustly tracking people is prerequisite for modeling and recognizing human activities. An omnidirectional camera mounted on the ceiling is less prone to problems of occlusion. We use the Markov Random Field (MRF) to present both background and foreground, and adapt models effectively against environment changes. We employ a deformable model to adapt the foreground models to optimally match objects in different position within a pattern of view of omnidirectional camera. In order to monitor human activity, we represent positions of people as spatial points and analyze moving trajectories within a time-spatial window. The method provides an efficient way to monitoring high-level human activities without exploring identities.</div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847712&CFID=105750519&CFTOKEN=91765132">Smart Platform " A Software Infrastructure for Smart Space (SISS)</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100418239&CFID=105750519&CFTOKEN=91765132">Weikai Xie</a>, 
                        <a href="author_page.cfm?id=81100647319&CFID=105750519&CFTOKEN=91765132">Yuanchun Shi</a>, 
                        <a href="author_page.cfm?id=81418594029&CFID=105750519&CFTOKEN=91765132">Guanyou Xu</a>, 
                        <a href="author_page.cfm?id=81331499083&CFID=105750519&CFTOKEN=91765132">Yanhua Mao</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 429</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167033" title="DOI">10.1109/ICMI.2002.1167033</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847712&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow74" style="display:inline;"><br /><div style="display:inline">A software infrastructure is fundamental to a Smart Space. Previous proposed software infrastructure for Smart Space (SISS) did not sufficiently address the issue of performance and usability. A new solution, Smart Platform, which is focused on improving ...</div></span>
          <span id="toHide74" style="display:none;"><br /><div style="display:inline">A software infrastructure is fundamental to a Smart Space. Previous proposed software infrastructure for Smart Space (SISS) did not sufficiently address the issue of performance and usability. A new solution, Smart Platform, which is focused on improving these aspects of a SISS, is presented in this paper. To optimize its intermodule communication performance, the stream-oriented communication is distinguished from the message-oriented ones, and a corresponding hybrid communication scheme is proposed. To improve the usability, a featuredloose-coupling structure, a straightforward Publish-and-Subscribe coordination model as well as a set of user-friendly deployment and development tools are developed. Besides, Smart Platform is intended as an open and generic SISS available for other research groups. Tothis end, XML-based message syntax and open wire-protocol based architecture are adopted to make sharing research efforts more easily.</div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847705&CFID=105750519&CFTOKEN=91765132">Do Multimodal Signals Need to Come from the Same Place? Crossmodal Attentional Links Between Proximal and Distal Surfaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81406599190&CFID=105750519&CFTOKEN=91765132">Rob Gray</a>, 
                        <a href="author_page.cfm?id=81100315053&CFID=105750519&CFTOKEN=91765132">Hong Z. Tan</a>, 
                        <a href="author_page.cfm?id=81331508496&CFID=105750519&CFTOKEN=91765132">J. Jay Young</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 437</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167035" title="DOI">10.1109/ICMI.2002.1167035</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847705&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow75" style="display:inline;"><br /><div style="display:inline">Previous research has shown that the use of multimodal signals can lead to faster and moreaccurate responses compared to purely unimodal displays. However, in most cases responsefacilitation only occurs when the signals are presented in roughly the same ...</div></span>
          <span id="toHide75" style="display:none;"><br /><div style="display:inline">Previous research has shown that the use of multimodal signals can lead to faster and moreaccurate responses compared to purely unimodal displays. However, in most cases responsefacilitation only occurs when the signals are presented in roughly the same spatial location. This would suggest a severe restriction on interface designers: to use multimodal displays effectively all signals must be presented from the same location on the display. We previously reported evidence that the use of haptic cues may provide a solution to this problem as haptic cues presented to a user's back can beused to redirectvisualattention to locations on a screen in front of the user [1]. In the present experiment we used a visual change detection task to investigate whether (i) this type of visual-haptic interaction is robust at low cue validity rates and (ii) similar effects occur for auditory cues. Valid haptic cues resulted in significantly faster change detection times even when they accurately indicated the location of the change on only 20% of the trials. Auditory cues had a much smaller effect on detection times at the high validity rate (80%) than haptic cues and did not significantly improve performance at the 20% validity rate. These results suggest that the use haptic attentional cues may be particularly effective in environments in which information cannot be presented in the same spatial location.</div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847703&CFID=105750519&CFTOKEN=91765132">CATCH-2004 Multi-Modal Browser: Overview Description with Usability Analysis</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100245025&CFID=105750519&CFTOKEN=91765132">Janne Bergman</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 442</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167036" title="DOI">10.1109/ICMI.2002.1167036</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847703&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow76" style="display:inline;"><br /><div style="display:inline">This paper takes a closer look at the user interface issues in our research multi-modal browser architecture. The browser framework, also briefly introduced in this paper, reuses single-modal browser technologies available for VoiceXML, WML, and HTML ...</div></span>
          <span id="toHide76" style="display:none;"><br /><div style="display:inline">This paper takes a closer look at the user interface issues in our research multi-modal browser architecture. The browser framework, also briefly introduced in this paper, reuses single-modal browser technologies available for VoiceXML, WML, and HTML browsing. User interface actions on a particular browser are captured, converted to events, and distributed to the other browsers participating (possibly on different hosts) in the multi-modal framework. We have defined a synchronization protocol, which distributes such events with the help of the central component called the Virtual Proxy. The choice of the architecture and the synchronization primitives have profound consequences on handling certain interesting UI use cases. We particularly address those specified by the W3C Multi-Modal Requirements, which are related to the design of possible strategies of dealing with simultaneous input, solving input inconsistencies, and defining synchronization points. The proposed approaches are illustrated by examples.</div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847706&CFID=105750519&CFTOKEN=91765132">Multimodal Interaction During Multiparty Dialogues: Initial Results</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100149852&CFID=105750519&CFTOKEN=91765132">Philip R. Cohen</a>, 
                        <a href="author_page.cfm?id=81100413045&CFID=105750519&CFTOKEN=91765132">Rachel Coulston</a>, 
                        <a href="author_page.cfm?id=81418598992&CFID=105750519&CFTOKEN=91765132">Kelly Krout</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 448</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167037" title="DOI">10.1109/ICMI.2002.1167037</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847706&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow77" style="display:inline;"><br /><div style="display:inline">Groups of people involved in collaboration on a task often incorporate the objects in their mutual environment into their discussion. With this comes physical reference to these 3-D objects, including: gesture, gaze, haptics, and possibly other modalities, ...</div></span>
          <span id="toHide77" style="display:none;"><br /><div style="display:inline">Groups of people involved in collaboration on a task often incorporate the objects in their mutual environment into their discussion. With this comes physical reference to these 3-D objects, including: gesture, gaze, haptics, and possibly other modalities, over and above the speech we commonly associate with human-human communication. From a technological perspective, this human style of communication not only poses the challenge for researchers to create multimodal systems capable of integrating input from various modalities, but also to do it well enough that it supports  but does not interfere with  the primary goal of the collaborators, which is their own human-human interaction. This paper offers a first steptowards building such multimodal systems for supporting face-to-face collaborative work by providing both qualitative and quantitative analyses of multiparty multimodal dialogues in a field setting.</div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847704&CFID=105750519&CFTOKEN=91765132">Multi-Modal Embodied Agents Scripting</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100512186&CFID=105750519&CFTOKEN=91765132">Yasmine Arafa</a>, 
                        <a href="author_page.cfm?id=81100094380&CFID=105750519&CFTOKEN=91765132">Abe Mamdani</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 454</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167038" title="DOI">10.1109/ICMI.2002.1167038</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847704&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow78" style="display:inline;"><br /><div style="display:inline">Embodied agents present ongoing challenging agenda for research in multi-modal user interfaces and human-computer-interaction. Such agent metaphors will only be widely applicable to online applications when there is a standardised way to map underlying ...</div></span>
          <span id="toHide78" style="display:none;"><br /><div style="display:inline">Embodied agents present ongoing challenging agenda for research in multi-modal user interfaces and human-computer-interaction. Such agent metaphors will only be widely applicable to online applications when there is a standardised way to map underlying engines with the visual presentation of the agents. This paper delineates the functions and specifications of a mark-up language for scripting the animation of virtual characters. The language is called: Character Mark-up Language (CML) and is an XML-based character attribute definition and animation scripting language designed to aid in the rapid incorporation of lifelike characters/agents into online applications or virtual reality worlds. This multi-modal scripting language is designed to be easily understandable by human animators and easilygenerated by a software process such as software agents. CML is constructed based jointly on motion and multi-modal capabilities of virtual life-like figures. The paper further illustrates the constructs of the language and describes a real-time execution architecture that demonstrates the use of such a language as a 4G language to easily utilise and integrate MPEG-4 media objects in online interfaces and virtual environments.</div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847708&CFID=105750519&CFTOKEN=91765132">A Methodology for Evaluating Multimodality in a Home Entertainment System</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100647559&CFID=105750519&CFTOKEN=91765132">Gregor Moehler</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 460</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167039" title="DOI">10.1109/ICMI.2002.1167039</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847708&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow79" style="display:inline;"><br /><div style="display:inline">Multimodality is likely to play a key role in household technology interfaces of the future offering as it can, enhanced efficiency, improved flexibility and increased user preference. These benefits are unlikely to be realized however unless such interfaces ...</div></span>
          <span id="toHide79" style="display:none;"><br /><div style="display:inline">Multimodality is likely to play a key role in household technology interfaces of the future offering as it can, enhanced efficiency, improved flexibility and increased user preference. These benefits are unlikely to be realized however unless such interfaces are well designedspecifically with regard to modality allocation and configuration.We report on a methodology aimed at evaluating modality usage, which involves a combination of two sets of heuristics, one derived from a description of modality properties, the other concerned with issues of usability. We describe how modality properties can be reformulated into a procedural style checklist and then describe the implementation of this methodology and the issues we were able to highlight in the context of the EMBASSI Home' system, a multimodal system which aims to provide a natural and intuitive interface to a potentially open-ended array of appliances within the home.</div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847702&CFID=105750519&CFTOKEN=91765132">Body-Based Interfaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100371667&CFID=105750519&CFTOKEN=91765132">Changseok Cho</a>, 
                        <a href="author_page.cfm?id=81100355697&CFID=105750519&CFTOKEN=91765132">Huichul Yang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 466</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167040" title="DOI">10.1109/ICMI.2002.1167040</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847702&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow80" style="display:inline;"><br /><div style="display:inline">This research explores different ways to use features of one's own body for interacting with computers. In the future, such "body-based" interfaces may be put into good use for wearable computing or virtual reality systems as part of a 3D multi-modal ...</div></span>
          <span id="toHide80" style="display:none;"><br /><div style="display:inline">This research explores different ways to use features of one's own body for interacting with computers. In the future, such "body-based" interfaces may be put into good use for wearable computing or virtual reality systems as part of a 3D multi-modal interface, freeing the user from holding interaction devices. We have identified four types of body-based interfaces: the Body-inspired-metaphor uses various parts of the body metaphorically for interaction; the Body-as-interaction-surface simply uses parts of the body as points of interaction; Mixed-modemixes the former two; Object-mapping spatially maps the interaction object to the human body. These four body-based interfaces were applied to three different applications (and associated tasks) and were tested for their performance and utility. It was generally found that, while the Body-inspired-metaphor produced the lowest error rate, it required a longer task completion time and caused more fatigue due to the longer hand moving distance. On the other hand, the Body-as-interaction-surface was the fastest, but produced many more errors.</div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847707&CFID=105750519&CFTOKEN=91765132">Evaluation of the Command and Control Cube</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418592562&CFID=105750519&CFTOKEN=91765132">Jerome Grosjean</a>, 
                        <a href="author_page.cfm?id=81100178417&CFID=105750519&CFTOKEN=91765132">Jean-Marie Burkhardt</a>, 
                        <a href="author_page.cfm?id=81100424950&CFID=105750519&CFTOKEN=91765132">Sabine Coquillart</a>, 
                        <a href="author_page.cfm?id=81452605387&CFID=105750519&CFTOKEN=91765132">Paul Richard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 473</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167041" title="DOI">10.1109/ICMI.2002.1167041</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847707&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow81" style="display:inline;"><br /><div style="display:inline">Application control in virtual environments (VE) is still an open field of research. The Command and Control Cube (C3) developed by Grosjean et al. is a quick access menu forthe VE configuration called workbench (a large screen displaying stereoscopic ...</div></span>
          <span id="toHide81" style="display:none;"><br /><div style="display:inline">Application control in virtual environments (VE) is still an open field of research. The Command and Control Cube (C3) developed by Grosjean et al. is a quick access menu forthe VE configuration called workbench (a large screen displaying stereoscopic images). The C3 presents two modes, one with the graphical display of the cubic structure associated to the C3 and a blind mode for expert users, with no feedback. In this paper we conduct formal tests of the C3 under four different conditions: the visual mode with the graphical display, the blind mode with no feedback and two additional conditions enhancing the expert blind mode: a tactile mode with the tactile feedback of a Cyberglove and a sound mode with a standard audio device. Results show that the addition of sound and tactil feedback is more disturbing to the users than the blind mode. The visual mode performs the best although the blind mode achieves some promising results.</div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847709&CFID=105750519&CFTOKEN=91765132">Interruptions as Multimodal Outputs: Which are the Less Disruptive?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100314760&CFID=105750519&CFTOKEN=91765132">Alexandre Stouffs</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 479</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167043" title="DOI">10.1109/ICMI.2002.1167043</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847709&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow82" style="display:inline;"><br /><div style="display:inline">This paper describes exploratory studies of interruption modalities and disruptiveness. Fiveinterruption modalities were compared: Heat, Smell, Sound, Vibration, and Light. Much more notable than the differences between modalities was the differences ...</div></span>
          <span id="toHide82" style="display:none;"><br /><div style="display:inline">This paper describes exploratory studies of interruption modalities and disruptiveness. Fiveinterruption modalities were compared: Heat, Smell, Sound, Vibration, and Light. Much more notable than the differences between modalities was the differences between people. We found that subjects' sensitiveness depended on their previous life exposure to the modalities.Individual differences greatly control the effect of interrupting stimuli .We show that is possible to build multimodal adaptive interruption interface, such interfaces would dynamically select the output interruption modality to use based on its effectiveness on a particular user.</div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847701&CFID=105750519&CFTOKEN=91765132">Experimentally Augmenting an Intelligent Tutoring System with Human-Supplied Capabilities: Adding Human-Provided Emotional Scaffolding to an Automated Reading Tutor that Listens</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100524361&CFID=105750519&CFTOKEN=91765132">Gregory Aist</a>, 
                        <a href="author_page.cfm?id=81100488480&CFID=105750519&CFTOKEN=91765132">Barry Kort</a>, 
                        <a href="author_page.cfm?id=81100514096&CFID=105750519&CFTOKEN=91765132">Rob Reilly</a>, 
                        <a href="author_page.cfm?id=81100200615&CFID=105750519&CFTOKEN=91765132">Jack Mostow</a>, 
                        <a href="author_page.cfm?id=81100496593&CFID=105750519&CFTOKEN=91765132">Rosalind Picard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 483</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167044" title="DOI">10.1109/ICMI.2002.1167044</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847701&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow83" style="display:inline;"><br /><div style="display:inline">This paper presents the first statistically reliable empirical evidence from a controlled study for the effect of human-provided emotional scaffolding on student persistence in an intelligent tutoring system. We describe an experiment that added human-provided ...</div></span>
          <span id="toHide83" style="display:none;"><br /><div style="display:inline">This paper presents the first statistically reliable empirical evidence from a controlled study for the effect of human-provided emotional scaffolding on student persistence in an intelligent tutoring system. We describe an experiment that added human-provided emotional scaffolding to an automated Reading Tutor that listens, and discuss the methodology we developed to conduct this experiment. Each student participated in one (experimental) session with emotional scaffolding, and in one (control) session without emotional scaffolding, counterbalanced by order of session. Each session was divided into several portions. After each portion of the session was completed, the Reading Tutor gave the student a choice: continue, or quit. We measured persistence as the number of portions the student completed. Human-provided emotional scaffolding added to the automated Reading Tutor resulted in increased student persistence, compared to the Reading Tutor alone. Increased persistence means increased time on task, which ought lead to improved learning. If these results for reading turn out to hold for other domains too, the implication for intelligent tutoring systems is that they should respond with not just cognitive support  but emotional scaffolding as well. Furthermore, the general technique of adding human-supplied capabilities to an existing intelligent tutoring system should prove useful for studying other ITSs too.This paper is a shortened and revised version of Aist et al. (same title). ITS Workshop on Empirical Methods for Tutorial Dialogue. June 4, 2002, San Sebastian, Spain.</div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847711&CFID=105750519&CFTOKEN=91765132">Individual Differences in Facial Expression: Stability over Time, Relation to Self-Reported Emotion, and Ability to Inform Person Identification</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100363022&CFID=105750519&CFTOKEN=91765132">Jeffrey F. Cohn</a>, 
                        <a href="author_page.cfm?id=81100587726&CFID=105750519&CFTOKEN=91765132">Karen Schmidt</a>, 
                        <a href="author_page.cfm?id=81100259511&CFID=105750519&CFTOKEN=91765132">Ralph Gross</a>, 
                        <a href="author_page.cfm?id=81418593457&CFID=105750519&CFTOKEN=91765132">Paul Ekman</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 491</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167045" title="DOI">10.1109/ICMI.2002.1167045</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847711&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow84" style="display:inline;"><br /><div style="display:inline">The face can communicate varied personal information including subjective emotion, communicative intent, and cognitive appraisal. Accurate interpretation by observer or computer interface depends on attention to dynamic properties of the expression, ...</div></span>
          <span id="toHide84" style="display:none;"><br /><div style="display:inline">The face can communicate varied personal information including subjective emotion, communicative intent, and cognitive appraisal. Accurate interpretation by observer or computer interface depends on attention to dynamic properties of the expression, context, and knowledge of what is normative for a given individual. In two separate studies, we investigated individual differences in the base rate of positive facial expression and in specific facial action units over intervals from 4- to 12 months. Facial expression was measured using convergent measures, including facial EMG, automatic feature-point tracking, and manual FACS coding. Individual differences in facial expression were stable over time, comparable in magnitude to stability of self-reported emotion, and sufficiently strong that individuals were recognized on the basis of their facial behavior alone at rates comparable to that for a commercial face recognition system (FaceIt from Identix). Facial action units convey unique information about person identity that can inform interpretation of psychological states, person recognition, and design of individuated avatars.</div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847765&CFID=105750519&CFTOKEN=91765132">Training a Talking Head</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81406592141&CFID=105750519&CFTOKEN=91765132">Michael M. Cohen</a>, 
                        <a href="author_page.cfm?id=81100458041&CFID=105750519&CFTOKEN=91765132">Dominic W. Massaro</a>, 
                        <a href="author_page.cfm?id=81418595112&CFID=105750519&CFTOKEN=91765132">Rashid Clark</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 499</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167046" title="DOI">10.1109/ICMI.2002.1167046</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847765&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow85" style="display:inline;"><br /><div style="display:inline">A Cyberware laser scan of DWM was made, Baldi's generic morphology was mapped into the form of DWM, this head was trained on real data recorded with Optotrak LED markers, and the quality of its speech was evaluated. Participants were asked to recognize ...</div></span>
          <span id="toHide85" style="display:none;"><br /><div style="display:inline">A Cyberware laser scan of DWM was made, Baldi's generic morphology was mapped into the form of DWM, this head was trained on real data recorded with Optotrak LED markers, and the quality of its speech was evaluated. Participants were asked to recognize auditory sentences presented alone in noise, aligned with the newly trained synthetic textured mapped target face, or the original natural face. There was a significant advantage when the noisy auditory sentence was paired with either head, with the synthetic textured mapped target face giving as much of an improvement as the original recordings of the natural face.</div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847764&CFID=105750519&CFTOKEN=91765132">Labial Coarticulation Modeling for Realistic Facial Animation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100428898&CFID=105750519&CFTOKEN=91765132">Emanuela Magno Caldognetto</a>, 
                        <a href="author_page.cfm?id=81100241526&CFID=105750519&CFTOKEN=91765132">Giulio Perin</a>, 
                        <a href="author_page.cfm?id=81100358107&CFID=105750519&CFTOKEN=91765132">Claudio Zmarich</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 505</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167047" title="DOI">10.1109/ICMI.2002.1167047</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847764&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow86" style="display:inline;"><br /><div style="display:inline">A modified version of the coarticulation model proposed by Cohen and Massaro is described. A semi-automatic minimization technique, working on real cinematic data, acquired by the ELITE opto-electronic system, was used to train the dynamic characteristics ...</div></span>
          <span id="toHide86" style="display:none;"><br /><div style="display:inline">A modified version of the coarticulation model proposed by Cohen and Massaro is described. A semi-automatic minimization technique, working on real cinematic data, acquired by the ELITE opto-electronic system, was used to train the dynamic characteristics of the model. Finally, the model was applied with success to GRETA an Italian talking head and few examples are illustrated to show the naturalness of the resulting animation technique1.</div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847763&CFID=105750519&CFTOKEN=91765132">Improved Information Maximization based Face and Facial Feature Detection from Real-time Video and Application in a Multi-Modal Person Identification System</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100313899&CFID=105750519&CFTOKEN=91765132">Ziyou Xiong</a>, 
                        <a href="author_page.cfm?id=81317492340&CFID=105750519&CFTOKEN=91765132">Yunqiang Chen</a>, 
                        <a href="author_page.cfm?id=81418594214&CFID=105750519&CFTOKEN=91765132">Roy Wang</a>, 
                        <a href="author_page.cfm?id=81361599374&CFID=105750519&CFTOKEN=91765132">Thomas S. Huang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 511</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167048" title="DOI">10.1109/ICMI.2002.1167048</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847763&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow87" style="display:inline;"><br /><div style="display:inline">In this paper an improved face detection method based on our previous Information-Based Maximum Discrimination approach is presented that maximizes the discrimination between face and non-face examples in a training set without even using color or motion ...</div></span>
          <span id="toHide87" style="display:none;"><br /><div style="display:inline">In this paper an improved face detection method based on our previous Information-Based Maximum Discrimination approach is presented that maximizes the discrimination between face and non-face examples in a training set without even using color or motion information. A short review of our previous method is given together with the description of the intuition behind and our recent improvement on its detection speed. A person identification system has been developed that performs multi-modal person identification in real-time video based on this newly improved face detection method together with speaker identification.</div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847769&CFID=105750519&CFTOKEN=91765132">Animating Arbitrary Topology 3D Facial Model Using the MPEG-4 FaceDefTables</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100606148&CFID=105750519&CFTOKEN=91765132">Dalong Jiang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 517</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167049" title="DOI">10.1109/ICMI.2002.1167049</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847769&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow88" style="display:inline;"><br /><div style="display:inline">In this paper, we put forward a method to animate arbitrary topology facial model (ATFM) based on the MPEG-4 standard. This paper deals mainly with the problem of building the FaceDefTables, which play a very important role in the MPEG-4 based facial ...</div></span>
          <span id="toHide88" style="display:none;"><br /><div style="display:inline">In this paper, we put forward a method to animate arbitrary topology facial model (ATFM) based on the MPEG-4 standard. This paper deals mainly with the problem of building the FaceDefTables, which play a very important role in the MPEG-4 based facial animation system. The FaceDefTables for our predefined standard facial model (SFM) are built by using the interpolation method. Since the FaceDefTables depend on facial models, the FaceDefTables for the SFM can be applied to only those facial models that have the same topology as the SFM. For those facial models that have different topology, we have to build the FaceDefTables accordingly. To acquire the FaceDefTables for ATFM, we first selectfeature points on ATFM, and then transform the SFM according to those feature points. At last, we project each vertex on the ATFM to the transformed SFM and build the FaceDefTables for the ATFM according to the projection position. With the FaceDefTables we built, realistic animation results have been acquired.</div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847768&CFID=105750519&CFTOKEN=91765132">An Improved Active Shape Model for Face Alignment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81418593401&CFID=105750519&CFTOKEN=91765132">Wei Wang</a>, 
                        <a href="author_page.cfm?id=81452594604&CFID=105750519&CFTOKEN=91765132">Shiguang Shan</a>, 
                        <a href="author_page.cfm?id=81100131951&CFID=105750519&CFTOKEN=91765132">Wen Gao</a>, 
                        <a href="author_page.cfm?id=81100130440&CFID=105750519&CFTOKEN=91765132">Bo Cao</a>, 
                        <a href="author_page.cfm?id=81100408791&CFID=105750519&CFTOKEN=91765132">Baocai Yin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 523</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167050" title="DOI">10.1109/ICMI.2002.1167050</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847768&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow89" style="display:inline;"><br /><div style="display:inline">In this paper, we present several improvements on the conventional Active Shape Models (ASM) for face alignment. Despite the accuracy and robustness of the ASMs in the image alignment, its performance depends heavily on the initial parameters of the ...</div></span>
          <span id="toHide89" style="display:none;"><br /><div style="display:inline">In this paper, we present several improvements on the conventional Active Shape Models (ASM) for face alignment. Despite the accuracy and robustness of the ASMs in the image alignment, its performance depends heavily on the initial parameters of the shape model, aswell as the local texture model for each landmark and the corresponding local matching strategy. In this work, to improve the ASMs for face alignment, several measures are taken. First, salient facial features, such as the eyes and the mouth, are localized based on a face detector. These salient features are then utilized to initialize the shape model and provide region constraints on the subsequent iterative shape searching. Secondly, we exploit the edge information to construct better local texture models for the landmarks on the face contour. The edge intensity at the contour landmark is used as a self-adaptive weight when calculating the Mahalanobis distance between the candidate profile and the reference one. Thirdly, to avoid their unreasonable shift from the pre-Iocalized salient features, landmarks around the salient features are adjusted before applying the global subs pace constraints. Experiments on a database containing 300 labeled face images show that the proposed method performs significantly better than traditional ASMs.</div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847767&CFID=105750519&CFTOKEN=91765132">Head-Pose Invariant Facial Expression Recognition Using Convolutional Neural Networks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100641612&CFID=105750519&CFTOKEN=91765132">Beat Fasel</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 529</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167051" title="DOI">10.1109/ICMI.2002.1167051</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847767&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow90" style="display:inline;"><br /><div style="display:inline">Automatic face analysis has to cope with pose and lighting variations. Especially pose variations are difficult to tackle and many face analysis methods require the use of sophisticated normalization and initialization procedures. We propose a data-driven ...</div></span>
          <span id="toHide90" style="display:none;"><br /><div style="display:inline">Automatic face analysis has to cope with pose and lighting variations. Especially pose variations are difficult to tackle and many face analysis methods require the use of sophisticated normalization and initialization procedures. We propose a data-driven face analysis approach that is not only capable of extracting features relevant to a given face analysis task, but is also more robust with regard to face location changes and scale variations when compared to classical methods such as e.g. MLPs. Our approach is based on convolutional neural networks that use multi-scale feature extractors, which allow for improved facial expression recognition results with faces subject to in-plane pose variations.</div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847766&CFID=105750519&CFTOKEN=91765132">An Improved Algorithm for Hairstyle Dynamics</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100394445&CFID=105750519&CFTOKEN=91765132">Dehui Kong</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 535</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.1167052" title="DOI">10.1109/ICMI.2002.1167052</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847766&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow91" style="display:inline;"><br /><div style="display:inline">This paper introduces an efficient and flexible hair modeling method to develop intricate hairstyle dynamics. A prominent contribution of the present work is that it proposes the evaluating approach of spring coefficient, i.e., spring coefficient can ...</div></span>
          <span id="toHide91" style="display:none;"><br /><div style="display:inline">This paper introduces an efficient and flexible hair modeling method to develop intricate hairstyle dynamics. A prominent contribution of the present work is that it proposes the evaluating approach of spring coefficient, i.e., spring coefficient can be obtained through the combination of large deflection deformation model and spring hinge model. This is based on the fact that there is a direct proportion between the spring coefficient and the stiffness coefficient, a variable determined by hair shape. What is more, the damping coefficient is no longer regarded as a constant, but a function of hair density, and this treatment has turned out to be a success in solving the problem of hair-hair collision. As a result, a dynamic model, which fits very well a great variety of hairstyles, is proposed.</div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=847782&CFID=105750519&CFTOKEN=91765132">Author Index</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Page: 541</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1109/ICMI.2002.10002" title="DOI">10.1109/ICMI.2002.10002</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=847782&type=pdf&CFID=105750519&CFTOKEN=91765132" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
</table>


</div> 
</div>


 <p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>



 <br />
<div class="footerbody" align="center" >
	

	The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2012 ACM, Inc.<br />
	<a href="http://www.acm.org/publications/policies/usage">Terms of Usage</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;	  
	<a href="http://www.acm.org/about/contact-us">Contact Us</a>

<br /><br />
Useful downloads: 
<a href="http://www.adobe.com/products/acrobat/readstep2.html"><img src="http://dl.acm.org/images/pdf_logo.gif" width="16" height="16" alt="" border="0" /> Adobe Acrobat</a>
&nbsp;&nbsp;
<a href="http://www.apple.com/quicktime/download/" target="_blank"><img src="http://dl.acm.org/images/qtlogo.gif" width="16" height="16" alt="" border="0" /> QuickTime</a>
&nbsp;&nbsp;
<a href="http://www.microsoft.com/windows/windowsmedia/download/default.asp" target="_blank"><img src="http://dl.acm.org/images/wmv.gif" width="16" height="15" alt="" border="0" /> Windows Media Player</a>
&nbsp;&nbsp;
<a href="http://www.real.com/" target="_blank"><img src="http://dl.acm.org/images/realplayer.gif" width="20" height="18" alt="" border="0" /> Real Player</a>

</div> 



<div  id="cf_window1338241011133" class="yuiextdlg">
	
	<div  id="theguide_title" class="x-dlg-hd">
		The ACM Guide to Computing Literature
	 </div>
	<div  id="theguide_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241011136" class="yuiextdlg">
	
	<div  id="thetags_title" class="x-dlg-hd">
		All Tags
	 </div>
	<div  id="thetags_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241011139" class="yuiextdlg">
	
	<div  id="theformats_title" class="x-dlg-hd">
		Export Formats
	 </div>
	<div  id="theformats_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241011141" class="yuiextdlg">
	
	<div  id="theexplaination_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theexplaination_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241011143" class="yuiextdlg">
	
	<div  id="theservices_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theservices_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241011145" class="yuiextdlg">
	
	<div  id="savetobinder_title" class="x-dlg-hd">
		Save to Binder
	 </div>
	<div  id="savetobinder_body" class="x-dlg-bd">
		
		
	 </div>
 </div> 

</body>
</html>