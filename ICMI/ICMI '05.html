


<!doctype html>


<head><script type="text/javascript">_cf_loadingtexthtml="<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";
_cf_jsonprefix='//';
_cf_clientid='1C3D484A061819BB22F770AC00932FA5';</script><script type="text/javascript" src="/CFIDE/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfform.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/masks.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/FCKeditor/fckeditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/adapter/yui/ext-yui-adapter.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/ext-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/resizable.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dragdrop/dragdrop.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/util.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/build/state/State-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/widget-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dialog/dialogs.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/cf/cf.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />



<title>Proceedings of the 7th international conference on Multimodal interfaces</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
	
  --></style>
 


<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>


<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  function setDiv() {
	var m = document.getElementById('divmain');
	var mh = m.offsetHeight;
	var t = document.getElementById('divtools');
	var th = t.offsetHeight;
	var tg = document.getElementById('divtags');
	var tgh = tg.offsetHeight;
	var calcheight = mh - th;
	if (tgh > calcheight  ){
	  var x = (th + tgh) - mh;
	  if ( (th + tgh) - mh < 65) {
	  }
	  else {
		 document.getElementById('divtags').innerHTML = ""; 
		 var tg = document.getElementById('divtags');
		 var tgh = tg.offsetHeight;
		 tg.style.height = tgh  + 'px';
	  }
	}
	else {
		tg.style.height = calcheight + 'px';
		document.getElementById('divtags').innerHTML = "";
	}

//  do I need to check after I resize to be sure I didn't go too big?
//	var tg2 = document.getElementById('divtags');
//	var tgh2 = tg.offsetHeight;	
//	if (tgh2 > mh + 65) {
//	  var y = mh + 65;
//	  alert('expanded too much ' + tgh2 + ' should be at most ' + y);
//	  tg.style.height = y + 'px';
//	  document.getElementById('divtags').innerHTML = "";
//	}

  }
</script>

<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>



<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Lazzari, Gianni; General Chair-Pianesi, Fabio; Program Chair-Crowley, James; Program Chair-Mase, Kenji; Program Chair-Oviatt, Sharon"> <meta name="citation_title" content="Proceedings of the 7th international conference on Multimodal interfaces"> <meta name="citation_date" content="10/04/2005"> <meta name="citation_isbn" content="1-59593-028-0"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1088463"> 



<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFFORM');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFDIV');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFTEXTAREA');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFWINDOW');
</script>

<script type="text/javascript">
	var _cf_window_init_1338241302944=function()
	{
		_cf_bind_init_1338241302945=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide_body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338241302945);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window1338241302943', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241302944);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241302947=function()
	{
		_cf_bind_init_1338241302948=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags_body','bindExpr':['showthetags.cfm?id=1088463']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338241302948);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1088463',{ modal:false, closable:true, divid:'cf_window1338241302946', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241302947);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241302950=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window1338241302949', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241302950);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241302952=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window1338241302951', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241302952);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241302954=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window1338241302953', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241302954);
</script>

<script type="text/javascript">
	var _cf_window_init_1338241302956=function()
	{
		_cf_bind_init_1338241302957=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder_body','bindExpr':['savetobinder.cfm?id=1088463']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338241302957);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1088463',{ modal:false, closable:true, divid:'cf_window1338241302955', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338241302956);
</script>
</head>

<body style="text-align:center" onLoad="window.focus();">

<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'http://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
                        
<script src='AC_RunActiveContent.js' type="text/javascript"></script>




<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>



<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
	
    <tr style="vertical-align:top">
		
		<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text"  ><img src="http://dl.acm.org/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
		</td>
        
        <td style="padding-left: 5px; padding-right:10px; padding-bottom:0px;" class="small-link-text">
        	<table style="width:100%; border-collapse:collapse; padding:0px">
			<tr><td style="text-align:center">
				
                            <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
                    
					</td>		
			</tr>
			</table> 
        </td>
		<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text">
			 <p style="margin-top:0px; margin-bottom:10px;">
					
                            <a href="https://dl.acm.org/signin.cfm?cfid=105751022&amp;cftoken=32053859" class="small-link-text" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
                            &nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm?cfid=105751022&amp;cftoken=32053859"  class="small-link-text" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
						
			 </p>
            
			<table style="padding: 5px; border-collapse:collapse; float:right">
				
				
                            
                            <tr>
                            <td class="small-link-text" style="text-align:right">
                            <form name="qiksearch" action="results.cfm?h=1&amp;cfid=105751022&amp;cftoken=32053859" method="post">
                           
                           
                            
                                     
                                    <span style="margin-left:0px"><label><input type="text" name="query" size="34" value=" " /></label>&nbsp;
                                    <input style="vertical-align:top;" type="image" alt="Search" name="Go" src="http://dl.acm.org/images/search_small.jpg" />
                                    
                                    </span>
							  </form>
                                </td>
                            </tr>
                          
				  
			</table>

			
			
		</td>

	</tr>
    
    
    <tr><td colspan="3" class="small-link-text" style="padding-bottom:5px; padding-top:0px; text-align:center">
		<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
         
         </td>
    </tr>
    </table>
	
<map name="port" id="port" > 
  <area shape="rect" coords="1,1,60,50" href="http://www.acm.org/" alt="ACM Home Page" />
  <area shape="rect" coords="65,1,275,68" href="http://dl.acm.org/dl.cfm?CFID=105751022&CFTOKEN=32053859" alt="ACM Digital Library Home Page" />
</map>

<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
	<tr style="vertical-align:top">
		<td style="padding-right:10px; text-align:left" class="small-link-text">
        	<div id="divmain" style="border:1px solid #356b20;">
				 
				<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
					
                    	<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;"><strong>Proceedings of the 7th international conference on Multimodal interfaces</strong></h1>
                        
                </div>
                
                  

<table class="medium-text" style="border-collapse:collapse; padding:0px;">

<col style="width:540px" />

<tr style="vertical-align:top">
  <td>
    <table style="border-collapse:collapse; padding:2px;" class="medium-text">
      <col style="width:80px;" />
      <col style="width:auto" />
      <tr style="vertical-align:top">
        
      </tr>
    </table>

	
        <table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
            <col style="width:80px" />
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             General Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81329490207&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105751022&amp;cftoken=32053859" title="Author Profile Page" target="_self">Gianni Lazzari</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1032475&CFID=105751022&CFTOKEN=32053859" title="Institutional Profile Page"><small>ITC-irst, Trento (Italy)</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100424906&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105751022&amp;cftoken=32053859" title="Author Profile Page" target="_self">Fabio Pianesi</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1032475&CFID=105751022&CFTOKEN=32053859" title="Institutional Profile Page"><small>ITC-irst, Trento (Italy)</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             Program Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100490913&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105751022&amp;cftoken=32053859" title="Author Profile Page" target="_self">James Crowley</a>
                
            </td>
            <td valign="bottom">
                
                        <small>I.N.P. Grenoble (France)</small>
                    	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100070056&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105751022&amp;cftoken=32053859" title="Author Profile Page" target="_self">Kenji Mase</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1015846&CFID=105751022&CFTOKEN=32053859" title="Institutional Profile Page"><small>Nagoya University (Japan)</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100656112&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105751022&amp;cftoken=32053859" title="Author Profile Page" target="_self">Sharon Oviatt</a>
                
            </td>
            <td valign="bottom">
                
                        <small>Oregon Health & Sciences University</small>
                    	
            </td>
            </tr>
            
        </table>
    

  </td>

  <td rowspan="20" nowrap="nowrap">
	<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
		<tr>
        	<td align="center" style="padding-bottom: 5px;">
			  
               <img src="http://portalparts.acm.org/1090000/1088463/thumb/1088463_thumb.jpg" title="Proceedings of the 7th international conference on Multimodal interfaces" height="100"  width="76" ALT="Proceedings of the 7th international conference on Multimodal interfaces" /> 
              </td>
              <td valign="top" align="left" nowrap="nowrap">
	             <img src="images/acm_mini.jpg" title="Published by ACM" alt="Published by ACM" /> 2005 Proceeding<br />
                 
        	 </td>
        </tr>
        
        <tr>
        	<td colspan="2" valign="baseline" style="padding-bottom:5px;">
            <img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
            <a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
            </td>
         </tr>
         <tr>
            <td  class="small-text" colspan="2" valign="top" style="padding-left:30px;">
				
	                    	&middot;&nbsp;Downloads (6 Weeks): 247<br />
    	                    &middot;&nbsp;Downloads (12 Months): 1,301<br />
                          
                        &middot;&nbsp;Citation Count: 317 
			</td>
         </tr>

	</table>
  </td>
</tr>
</table>

<br clear="all" />

<table border="0" class="medium-text" cellpadding="2" cellspacing="0">

<tr valign="top">
    <td colspan="3"><table border="0" class="medium-text" cellpadding="1" cellspacing="0">

<tr valign="top">
    <td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>

	<tr valign="top">
    	<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
	</tr>
    <tr valign="top">
	    <td style="padding-left:10px;">
		   ICMI '05 Seventh International Conference on Multimodal Interfaces 2005 
        </td>
	</tr>
    
    <tr valign="top">
	    <td style="padding-left:10px; padding-bottom:10px"> Trento, Italy &mdash; October 04 - 06, 2005
                    
                  <br />
                    
                  <a href="http://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text">  &copy;2005</span> 
                  <br />           
                  
      </td>
	</tr>
	 

</table></td>
</tr>

</table>

                  
                 <br clear="all" />
			</div>
			
		</td>
		<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
	
            <div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
              <div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;"><strong>Tools and Resources</strong></h1></div>


<ul title="Tools and Resources" style="list-style: none; list-style-position:inside;
margin-left: 0px;
padding-left: 0em;
text-indent: 5px;
margin-bottom: 0px;">


	
	

		
             <li style="list-style-image:url(img/shopping-cart16.gif);margin-top:10px;">
                  <span style="margin-left:6px;">
                     
                     <a href="https://dl.acm.org/purchase.cfm?id=1088463&CFID=105751022&CFTOKEN=32053859" class="small-link-text">Buy this Proceeding in Print</a>
                  
                  
                  </span>
              </li>
        
	
<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:6px;">
   <span class="small-link-text">TOC Service:</span>
   	  
	  <img src="http://dl.acm.org/images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
      <ul style="margin-left: 0; padding-left: 0; display:inline;">
      	
        <li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
        <li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');"  class="small-link-text">RSS</a></li>
		        
      </ul>
    </span>
</li>

        <li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:6px;">
        <a href="citation.cfm?id=1088463&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
         </span></li>
    




<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Export Formats:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1088463&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1088463&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1088463&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
      </ul>
    </span>
</li>



 
   <li style="list-style-image:url(img/calbullet.jpg);margin-top:15px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Upcoming Conference:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px; margin-left:25px;"><a href="http://www.acm.org/icmi/2012/ " title="INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION" class="small-link-text">ICMI '12</a></li>
      </ul>
    </span>
	</li>
    


</ul>           

  <!-- ADDTHIS BUTTON BEGIN -->
  
  <!-- ADDTHIS BUTTON END -->

<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>
  <!-- AddThis Button BEGIN -->



<!-- AddThis Button BEGIN -->
<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_slashdot"></a>
<a class="addthis_button_reddit"></a>


<span class="addthis_separator">|</span>
<a href="http://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>
<!-- AddThis Button END -->

  
 

  
  
            </div>
            
		</td>
	</tr>
    
</table>



</div>


<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">




<div id="fback" style="text-align:left; padding-top:10px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="feedback" border="0" /></a>
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design"><strong>Feedback</strong></a>

<span style="padding:10px;">|</span>




<span>Switch to <a href="citation.cfm?id=1088463&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>


</span>

 
<div class="small-text" style="margin-top:10px; margin-bottom:5px;"> 
<br />

    <a href="#abstract"  title="Abstract" style="padding:5px"><span>Abstract</span></a> |
    
    <a href="#formats"  title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
    
    <a href="#authors"  title="Authors" style="padding:5px"><span>Authors</span></a> |
    <a href="#references"  title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
    <a href="#citedby"  title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
    <a href="#indexterms"  title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
    <a href="#source"  title="Publication" style="padding:5px"><span>Publication</span></a> |
    <a href="#revs"  title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |               
	<a href="#comments"  title="Comments" style="padding:5px"><span>Comments</span></a>
	
     |               
	<a href="#prox"  title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
    
</div>
    
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;"/>



</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
       	
			<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

			

		
			
           
			
				
				<p>
					<div style="display:inline">It is our great pleasure to welcome you to the Seventh International Conference on Multimodal Interfaces (ICMI 2005) held on 4-6 October 2005 and sponsored by the Association for Computing Machinery (ACM) SIGCHI.The ICMI 2005 program consists of three days of intense activities with podium paper presentations, poster presentations, demonstrations, doctoral spotlight posters, industry exhibits, one panel and three invited speakers. Moreover, for the first time ICMI hosts joint workshops and tutorials on October 3 and October 7. This proceeding includes the papers selected for podium and poster presentations at the conference.There were 97 total submissions in the regular paper category. The review process involved a blind-review with a minimum of three reviewers, and a meta-review by the relevant area-chair. The program committee consisted of 21 area chairs who are the leading researchers in different topics related to multimodal interfaces. The final paper selection was done at a meeting of the area chairs and program chairs. This process led to the selection of 24 papers for podium presentation, 20 papers for poster presentation. The demonstrations were selected by the Demo chair and the Doctoral Spotlights were selected by the Doctoral Spotlight chair. As a result, 6 demonstrations and 7 doctoral student posters will be presented during the conference.The conference also features a panel discussion on Multinational Cooperation on Multimodal Interaction led by Mats Ljungqvist of the European Commission (CEC). There are three invited keynote addresses; Max Ernst of MPI for Biological Cybernetics speaking on The Puzzle of Sensory Perception: putting together multisensory information, Hiroshi Ishiguro of Osaka University & ATR Intelligent Robotics and Communications Laboratories speaking on Interactive humanoids and androids as ideal interfaces for humans, and Sandy Pentland of MIT Media Lab talking about Socially Aware Computation and Communication.</div>
				</p>
   				
           	</div>
			
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


  <div class="abstract">
        <SPAN><strong>FRONT MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1090000/1088463/fm/frontmatter.pdf?ip=188.194.239.219&CFID=105751022&CFTOKEN=32053859" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(title page, copyright, foreword, contents, committee) 
  </div>          
  
  <div style="margin-top: 10px;"  class="abstract">
        <SPAN><strong>BACK MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1090000/1088463/bm/backmatter.pdf?ip=188.194.239.219&CFID=105751022&CFTOKEN=32053859" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(author index) 
  </div>          
  
<div style="margin-top: 10px; height: auto; padding: 5px; ">
		
		
        


	</div>

<br clear="all" />
</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<dl title="Authors" style="margin-top:0px">




<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 General Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Gianni Lazzari" href="author_page.cfm?id=81329490207&CFID=105751022&CFTOKEN=32053859">Gianni Lazzari</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">2001-2002</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">3</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">17</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">3</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">3</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">29</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Gianni Lazzari" href="author_page.cfm?id=81329490207&amp;dsp=coll&amp;trk=1&amp;CFID=105751022&CFTOKEN=32053859" target="_self">View colleagues</a> of Gianni Lazzari
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Fabio Pianesi" href="author_page.cfm?id=81100424906&CFID=105751022&CFTOKEN=32053859">Fabio Pianesi</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1990-2011</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">54</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">194</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">35</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">201</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">1,387</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Fabio Pianesi" href="author_page.cfm?id=81100424906&amp;dsp=coll&amp;trk=1&amp;CFID=105751022&CFTOKEN=32053859" target="_self">View colleagues</a> of Fabio Pianesi
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              



<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 Program Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of James Crowley" href="author_page.cfm?id=81100490913&CFID=105751022&CFTOKEN=32053859">James Crowley</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1982-2009</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">76</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">598</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">9</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">456</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">3,527</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of James Crowley" href="author_page.cfm?id=81100490913&amp;dsp=coll&amp;trk=1&amp;CFID=105751022&CFTOKEN=32053859" target="_self">View colleagues</a> of James Crowley
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Kenji Mase" href="author_page.cfm?id=81100070056&CFID=105751022&CFTOKEN=32053859">Kenji Mase</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1991-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">84</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">276</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">38</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">118</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">674</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Kenji Mase" href="author_page.cfm?id=81100070056&amp;dsp=coll&amp;trk=1&amp;CFID=105751022&CFTOKEN=32053859" target="_self">View colleagues</a> of Kenji Mase
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Sharon Oviatt" href="author_page.cfm?id=81100656112&CFID=105751022&CFTOKEN=32053859">Sharon Oviatt</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1989-2009</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">62</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">1,229</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">42</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">339</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">2,826</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Sharon Oviatt" href="author_page.cfm?id=81100656112&amp;dsp=coll&amp;trk=1&amp;CFID=105751022&CFTOKEN=32053859" target="_self">View colleagues</a> of Sharon Oviatt
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              

</dl>
</div>

		  
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	References are not available

</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	Citings are not available
		
 </div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

Index Terms are not available


</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">



<table border="0" class="medium-text" cellpadding="0" cellspacing="5">



    <tr valign="top">
    	<td>Title</td> 
	    <td>
		   ICMI '05 Seventh International Conference on Multimodal Interfaces 2005 
        </td>
	</tr>
    <tr><td></td><td>Trento, Italy &mdash; October 04 - 06, 2005</td></tr> <tr><td>Pages</td><td>334</td></tr> 
                 <tr>
                 
                     <td>Sponsors</td>
                    
                  <td>
                  <a href="sig.cfm?id=SP923&CFID=105751022&CFTOKEN=32053859"> SIGCHI</a> ACM Special Interest Group on Computer-Human Interaction
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="http://www.acm.org/"> ACM</a> Association for Computing Machinery
                  </td>
                  </tr>
              
                  <tr><td>Publisher</td><td><a href="http://www.acm.org/publications">ACM</a> New York, NY, USA</td>
				  </tr>
             <tr><td>ISBN</td><td>1-59593-028-0 </td></tr> <tr><td>Order Number</td><td>102050</td></tr> 
			<tr valign="top">
        	<td>Conference</td>
            <td valign="top" align="left"  style="padding-bottom: 25px;">
	            <strong style="padding-right:10px">ICMI-MLMI</strong><a href="event.cfm?id=RE354&CFID=105751022&CFTOKEN=32053859" title="Multimodal Interfaces and Machine Learning for Multimodal Interaction">Multimodal Interfaces and Machine Learning for Multimodal Interaction</a>
                
                       
                        <a href="event.cfm?id=RE354&CFID=105751022&CFTOKEN=32053859" title="Multimodal Interfaces and Machine Learning for Multimodal Interaction"><img border="0" src="http://portalparts.acm.org/event_logos/RE354.jpg" title="ICMI-MLMI logo" height="14"  width="100" ALT="ICMI-MLMI logo" style="vertical-align:top"></a>
						 

        	 </td>
            </tr>
		    <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 272 of 669 submissions, 41%</td></tr>
                       <tr valign="top">
                        <td colspan="2" style="padding-left:25px;">
                        	<table>
                            	<tr><td>
                                        <!-- WebCharts3D v5.1(2077) -->
<IMG SRC="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=Images/8338505510525603.JPG" id="Images_8338505510525603_JPG" name="Images_8338505510525603_JPG" usemap="#Images_8338505510525603_JPG_map" border="0"/>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAB' id='GP1338241305859AAAB'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '03</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>130</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAC' id='GP1338241305859AAAC'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '03</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>45</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAD' id='GP1338241305859AAAD'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>102</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAE' id='GP1338241305859AAAE'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>40</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAF' id='GP1338241305859AAAF'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>99</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAG' id='GP1338241305859AAAG'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>55</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAH' id='GP1338241305859AAAH'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>IMCI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>100</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAI' id='GP1338241305859AAAI'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>IMCI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>44</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAJ' id='GP1338241305859AAAJ'><tr><td width='8'>&nbsp;</td><td width='84'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>ICMI-MLMI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>118</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAK' id='GP1338241305859AAAK'><tr><td width='8'>&nbsp;</td><td width='84'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>ICMI-MLMI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='84'>41</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAL' id='GP1338241305859AAAL'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>ICMI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>120</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338241305859AAAM' id='GP1338241305859AAAM'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>ICMI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>47</td></tr></table>
<MAP name='Images_8338505510525603_JPG_map'>
<AREA shape='rect' coords='0,0,1,1'/>
<AREA shape="rect" coords="290,159,308,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAM",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAM",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAM",event)'/>
<AREA shape="rect" coords="272,74,290,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAL",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAL",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAL",event)'/>
<AREA shape="rect" coords="243,166,261,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAK",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAK",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAK",event)'/>
<AREA shape="rect" coords="225,76,243,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAJ",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAJ",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAJ",event)'/>
<AREA shape="rect" coords="196,162,214,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAI",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAI",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAI",event)'/>
<AREA shape="rect" coords="178,97,196,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAH",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAH",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAH",event)'/>
<AREA shape="rect" coords="149,149,167,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAG",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAG",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAG",event)'/>
<AREA shape="rect" coords="131,98,149,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAF",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAF",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAF",event)'/>
<AREA shape="rect" coords="102,167,120,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAE",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAE",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAE",event)'/>
<AREA shape="rect" coords="84,95,102,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAD",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAD",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAD",event)'/>
<AREA shape="rect" coords="56,161,74,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAC",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAC",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAC",event)'/>
<AREA shape="rect" coords="38,62,56,213" onMouseover='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAB",event,true)' onMouseout='xx_set_visible("Images_8338505510525603_JPG","GP1338241305859AAAB",event,false)' onMousemove='xx_move_tag("Images_8338505510525603_JPG","GP1338241305859AAAB",event)'/>
<AREA shape="rect" coords="160,13,227,27"/>
<AREA shape="rect" coords="89,13,160,27"/>
</MAP>

<script language="javascript" src="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=script.js"></script>

                                      </td>
                                      <td style="padding-left:20px;">
                                             <table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
                                                <tr bgcolor="#ffffff">
                                                  <th style="width:50%">Year</th>
                                                  <th  align="right" style="width:15%">Submitted</th>
                                                  <th  align="right" style="width:15%">Accepted</th>
                                                  <th  align="center">Rate</th>
                                                </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>ICMI '03</td>
                                                            <td align="right">130</td>
                                                            <td align="right">45</td>
                                                            <td align="center">35%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>ICMI '06</td>
                                                            <td align="right">102</td>
                                                            <td align="right">40</td>
                                                            <td align="center">39%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>ICMI '07</td>
                                                            <td align="right">99</td>
                                                            <td align="right">55</td>
                                                            <td align="center">56%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>IMCI '08</td>
                                                            <td align="right">100</td>
                                                            <td align="right">44</td>
                                                            <td align="center">44%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>ICMI-MLMI '09</td>
                                                            <td align="right">118</td>
                                                            <td align="right">41</td>
                                                            <td align="center">35%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>ICMI '11</td>
                                                            <td align="right">120</td>
                                                            <td align="right">47</td>
                                                            <td align="center">39%</td>
                                                         </tr>
                                                
                                                 <tr bgcolor="#f0f0f0">
                                                    <td><strong>Overall</strong></td>
                                                    <td align="right">669</td>
                                                    <td align="right">272</td>
                                                    <td align="center">41%</td>
                                                  </tr>
                                                </table>
                                       </td>
                                     </tr>
                               </table>
                        </td>
                    </tr>
                     
                     
            
</table>


</table>




</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	<br />Reviews are not available for this item
        
        <div align="left" style="margin-top:30px">
					<a title="Computing Reviews" href="ocr_review_main.cfm?CFID=105751022&CFTOKEN=32053859">
                 <img src="http://dl.acm.org/images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
        
        
       		<ul style="list-style:disc; display:inline-block">
	            <li>Access <a href="ocr_review_main.cfm?CFID=105751022&CFTOKEN=32053859" target="_blank">critical reviews</a> of computing literature.</li>
            	<li><a href="http://www.computingreviews.com/Reviewer/"  target="_blank">Become a reviewer</a> for Computing Reviews</li>
            </ul>
        </div>
        
</div>



<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
         
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<div>
<div>
	
	<p style="margin-left:5px;">
    <strong>Be the first to comment</strong>
    	
          	To Post a comment please <a href="signin.cfm?CFID=105751022&CFTOKEN=32053859">sign in or create</a> a free Web account</a>
        
    
    
	 </p>
	   	
 
</div>


</div>

	
		<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
        
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" >

<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 7th international conference on Multimodal interfaces</h5>


<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>

<div style="clear:both">
    
        <div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1027933&picked=prox&CFID=105751022&CFTOKEN=32053859" title="previous: ICMI '04"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1180995&picked=prox&CFID=105751022&CFTOKEN=32053859" title="Next: ICMI '06">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
        
</div>


 
<table class="text12" border="0">

  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1088464&CFID=105751022&CFTOKEN=32053859">The "puzzle" of sensory perception: putting together multisensory information</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          
                        <a href="author_page.cfm?id=81100204061&CFID=105751022&CFTOKEN=32053859">Marc O. Ernst</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">Pages: 1 - 1</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:0">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088464" title="DOI">10.1145/1088463.1088464</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088464&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:0">
          <span id="toShow1" style="display:inline;"><br /><div style="display:inline">
For perceiving the environment our brain uses multiple sources
of sensory information derived from several different modalities,
including vision, touch and audition. The question how information
derived from these different sensory modalities converges ...</div></span>
          <span id="toHide1" style="display:none;"><br /><div style="display:inline">
<p>For perceiving the environment our brain uses multiple sources
of sensory information derived from several different modalities,
including vision, touch and audition. The question how information
derived from these different sensory modalities converges in the
brain to form a coherent and robust percept is central to
understanding the process of perception. My main research interest
is the study of human perception focusing on multimodal integration
and visual-haptic interaction. For this, I use quantitative
computational/statistical models together with psychophysical and
neuropsychological methods.</p>

<p>A desirable goal for the perceptual system is to maximize the
reliability of the various perceptual estimates. From a statistical
viewpoint the optimal strategy for achieving this goal is to
integrate all available sensory information. This may be done using
a "maximum-likelihood-estimation" (MLE) strategy. Then the combined
percept will be a weighted average across the individual estimates
with weights that are proportional to their reliabilities.</p>

<p>In a recent study we could show that humans actually integrate
visual and haptic information in such a statistically optimal
fashion (Ernst &amp; Banks, Nature, 2002). Others have now
demonstrated that this finding is true not only for the integration
across vision and touch, but also for the integration of
information across and within other modalities, such as audition or
vision. This suggests that maximum-likelihood-estimation is an
effective and widely used strategy exploited by the perceptual
system.</p>

<p>By integrating sensory information the brain may or may not
loose access to the individual input signals feeding into the
integrated percept. The degree to which the original information is
still accessible defines the strength of coupling between the
signals. We found that the strengths of coupling is varying
depending on the set of signals used; e.g. strong coupling for
stereo and texture signals to slant and weak coupling for visual
and haptic signals to size (Hillis, Ernst, Banks, &amp; Landy,
Science, 2002). As suggested by one of our recent learning studies,
the strength of coupling, which can be modeled using Bayesian
statistics, seems to depend on the natural statistical
co-occurrence between signals (J&auml;kel &amp; Ernst, in
prep.)</p>

<p>Important precondition for integrating signals is to know which
signals derived from the different modalities belong together and
how reliable these are. Recently we could show that touch can teach
the visual modality how to interpret its signals and their
reliabilities. More specifically, we could show that by exploiting
touch we can alter visual perception of slant (Ernst, Banks &amp;
Bulthoff, Nature Neuroscience, 2000). This finding contributes to a
very old debate postulating that we only perceive the world because
of our interactions with the environment. Similarly, in one of our
latest studies we could show that experience can change the
so-called "light-from-above" prior. Prior knowledge is essential
for the interpretation of sensory signals during perception.
Consequently, with the prior change we introduced a change in the
perception of shape (Adams, Graf &amp; Ernst, Nature Neuroscience,
2004).</p>

<p>Integration is only sensible if the information sources carry
redundant information. If the information sources are
complementary, different combination strategies have to be
exploited. Complementation of cross-modal information was
demonstrated in a recent study investigating visual-haptic shape
perception (Newell, Ernst, Tjan, &amp; Bulthoff, Psychological
Science, 2001).</p>
</div></span> <a id="expcoll1" href="JavaScript: expandcollapse('expcoll1',1)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Recognition and multimodal gesture patterns</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088468&CFID=105751022&CFTOKEN=32053859">Integrating sketch and speech inputs using spatial information</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100386457&CFID=105751022&CFTOKEN=32053859">Bee-Wah Lee</a>, 
                        <a href="author_page.cfm?id=81100426389&CFID=105751022&CFTOKEN=32053859">Alvin W. Yeo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 2 - 9</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088468" title="DOI">10.1145/1088463.1088468</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088468&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow3" style="display:inline;"><br /><div style="display:inline">Since the development of multimodal spatial query, the integration technique in determining the correct pair of multimodal inputs remains a problem in multimodal fusion. Although there exist integration techniques that have been proposed to resolve this ...</div></span>
          <span id="toHide3" style="display:none;"><br /><div style="display:inline">Since the development of multimodal spatial query, the integration technique in determining the correct pair of multimodal inputs remains a problem in multimodal fusion. Although there exist integration techniques that have been proposed to resolve this problem, they are limited to the interaction with predefined speech and sketch commands. Furthermore, they are only designed to resolve the spatial query with single speech input and single sketch input. Therefore, when it comes to the introduction of multiple speech and sketch inputs in a single query, all the existing integration techniques are unable to resolve it. To date, no integration technique has been found that can resolve the Multiple Sentences and Sketch Objects Spatial Query. In this paper, the limitations of the existing integration techniques are discussed. A new integration technique in resolving this problem is described and compared with the widely used integration technique, Unification-based Integration Technique.</div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088469&CFID=105751022&CFTOKEN=32053859">Distributed pointing for multimodal collaboration over sketched diagrams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100242831&CFID=105751022&CFTOKEN=32053859">Paulo Barthelmess</a>, 
                        <a href="author_page.cfm?id=81100460794&CFID=105751022&CFTOKEN=32053859">Ed Kaiser</a>, 
                        <a href="author_page.cfm?id=81320490993&CFID=105751022&CFTOKEN=32053859">Xiao Huang</a>, 
                        <a href="author_page.cfm?id=81100236659&CFID=105751022&CFTOKEN=32053859">David Demirdjian</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 10 - 17</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088469" title="DOI">10.1145/1088463.1088469</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088469&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow4" style="display:inline;"><br /><div style="display:inline">A problem faced by groups that are not co-located but need to collaborate on a common task is the reduced access to the rich multimodal communicative context that they would have access to if they were collaborating face-to-face. Collaboration support ...</div></span>
          <span id="toHide4" style="display:none;"><br /><div style="display:inline">A problem faced by groups that are not co-located but need to collaborate on a common task is the reduced access to the rich multimodal communicative context that they would have access to if they were collaborating face-to-face. Collaboration support tools aim to reduce the adverse effects of this restricted access to the fluid intermixing of speech, gesturing, writing and sketching by providing mechanisms to enhance the awareness of distributed participants of each others' actions.In this work we explore novel ways to leverage the capabilities of multimodal context-aware systems to bridge co-located and distributed collaboration contexts. We describe a system that allows participants at remote sites to collaborate in building a project schedule via sketching on multiple distributed whiteboards, and show how participants can be made aware of naturally occurring pointing gestures that reference diagram constituents as they are performed by remote participants.The system explores the multimodal fusion of pen, speech and 3D gestures, coupled to the dynamic construction of a semantic representation of the interaction, anchored on the sketched diagram, to provide feedback that overcomes some of the intrinsic ambiguities of pointing gestures.</div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088470&CFID=105751022&CFTOKEN=32053859">Contextual recognition of head gestures</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100300540&CFID=105751022&CFTOKEN=32053859">Louis-Philippe Morency</a>, 
                        <a href="author_page.cfm?id=81100223737&CFID=105751022&CFTOKEN=32053859">Candace Sidner</a>, 
                        <a href="author_page.cfm?id=81100399488&CFID=105751022&CFTOKEN=32053859">Christopher Lee</a>, 
                        <a href="author_page.cfm?id=81100537374&CFID=105751022&CFTOKEN=32053859">Trevor Darrell</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 18 - 24</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088470" title="DOI">10.1145/1088463.1088470</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088470&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow5" style="display:inline;"><br /><div style="display:inline">Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people. We investigate how dialog context from an embodied conversational agent (ECA) can improve visual recognition of user ...</div></span>
          <span id="toHide5" style="display:none;"><br /><div style="display:inline">Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people. We investigate how dialog context from an embodied conversational agent (ECA) can improve visual recognition of user gestures. We present a recognition framework which (1) extracts contextual features from an ECA's dialog manager, (2) computes a prediction of head nod and head shakes, and (3) integrates the contextual predictions with the visual observation of a vision-based head gesture recognizer. We found a subset of lexical, punctuation and timing features that are easily available in most ECA architectures and can be used to learn how to predict user feedback. Using a discriminative approach to contextual prediction and multi-modal integration, we were able to improve the performance of head gesture detection even when the topic of the test set was significantly different than the training set.</div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088471&CFID=105751022&CFTOKEN=32053859">Combining environmental cues & head gestures to interact with wearable devices</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100489150&CFID=105751022&CFTOKEN=32053859">M. Hanheide</a>, 
                        <a href="author_page.cfm?id=81100148661&CFID=105751022&CFTOKEN=32053859">C. Bauckhage</a>, 
                        <a href="author_page.cfm?id=81100458351&CFID=105751022&CFTOKEN=32053859">G. Sagerer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 25 - 31</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088471" title="DOI">10.1145/1088463.1088471</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088471&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow6" style="display:inline;"><br /><div style="display:inline">As wearable sensors and computing hardware are becoming a reality, new and unorthodox approaches to seamless human-computer interaction can be explored. This paper presents the prototype of a wearable, head-mounted device for advanced human-machine interaction ...</div></span>
          <span id="toHide6" style="display:none;"><br /><div style="display:inline">As wearable sensors and computing hardware are becoming a reality, new and unorthodox approaches to seamless human-computer interaction can be explored. This paper presents the prototype of a wearable, head-mounted device for advanced human-machine interaction that integrates speech recognition and computer vision with head gesture analysis based on inertial sensor data. We will focus on the innovative idea of integrating visual and inertial data processing for interaction. Fusing head gestures with results from visual analysis of the environment provides rich vocabularies for human-machine communication because it renders the environment into an interface: if objects or items in the surroundings are being associated with system activities, head gestures can trigger commands if the corresponding object is being looked at. We will explain the algorithmic approaches applied in our prototype and present experiments that highlight its potential for assistive technology. Apart from pointing out a new direction for seamless interaction in general, our approach provides a new and easy to use interface for disabled and paralyzed users in particular.</div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">POSTER SESSION: <strong>Posters</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088473&CFID=105751022&CFTOKEN=32053859">Automatic detection of interaction groups</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100523534&CFID=105751022&CFTOKEN=32053859">Oliver Brdiczka</a>, 
                        <a href="author_page.cfm?id=81100050878&CFID=105751022&CFTOKEN=32053859">J&#233;r&#244;me Maisonnasse</a>, 
                        <a href="author_page.cfm?id=81100299895&CFID=105751022&CFTOKEN=32053859">Patrick Reignier</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 32 - 36</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088473" title="DOI">10.1145/1088463.1088473</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088473&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow8" style="display:inline;"><br /><div style="display:inline">This paper addresses the problem of detecting interaction groups in an intelligent environment. To understand human activity, we need to identify human actors and their interpersonal links. An interaction group can be seen as basic entity, within which ...</div></span>
          <span id="toHide8" style="display:none;"><br /><div style="display:inline">This paper addresses the problem of detecting interaction groups in an intelligent environment. To understand human activity, we need to identify human actors and their interpersonal links. An interaction group can be seen as basic entity, within which individuals collaborate in order to achieve a common goal. In this regard, the dynamic change of interaction group configuration, i.e. the split and merge of interaction groups, can be seen as indicator of new activities. Our approach takes speech activity detection of individuals forming interaction groups as input. A classical HMM-based approach learning different HMM for the different group configurations did not produce promising results. We propose an approach for detecting interaction group configurations based on the assumption that conversational turn taking is synchronized inside groups. The proposed detector is based on one HMM constructed upon conversational hypotheses. The approach shows good results and thus confirms our conversational hypotheses.</div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088474&CFID=105751022&CFTOKEN=32053859">Meeting room configuration and multiple camera calibration in meeting analysis</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100313933&CFID=105751022&CFTOKEN=32053859">Yingen Xiong</a>, 
                        <a href="author_page.cfm?id=81100361835&CFID=105751022&CFTOKEN=32053859">Francis Quek</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 37 - 44</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088474" title="DOI">10.1145/1088463.1088474</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088474&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow9" style="display:inline;"><br /><div style="display:inline">In video based cross-model analysis of planning meeting, the meeting events are recorded by multiple cameras distributed in the entire meeting room. Subject's hand gestures, hand motion, head orientations, gaze targets, body poses are very important ...</div></span>
          <span id="toHide9" style="display:none;"><br /><div style="display:inline">In video based cross-model analysis of planning meeting, the meeting events are recorded by multiple cameras distributed in the entire meeting room. Subject's hand gestures, hand motion, head orientations, gaze targets, body poses are very important for the meeting event analysis. In order to register everything to the same global coordinate system, build 3D model, get 3D data from the video, we need to create a proper meeting room configuration and calibrate all cameras to obtain their intrinsic and extrinsic parameters. However, the calibration of multiple cameras distributed in the entire meeting room area is a challenging task because it is impossible to let all cameras in the meeting room see a reference object at the same time and wide field-of-view cameras suffer under radial distortion. In this paper, we propose a simple approach to create a good meeting room configuration and calibrate multiple cameras in the meeting room. The proposed approach includes several steps. First, we create stereo camera pairs according to the room configuration and the requirements of the targets, the participants of the meeting. Second, we apply Tsai's algorithm to calibrate each stereo camera pair and obtain the parameters in its own local coordinate system. Third, we use Vicon motion capture data to transfer all local coordinate systems of stereo camera pairs into a global coordinate system in the meeting room. We can obtain the positions, orientations, and parameters for all cameras in the same global coordinate system, so that we can register everything into this global coordinate system. Next, we do calibration error analysis for the current camera and meeting room configuration. We can obtain error distribution in the entire meeting room area. Finally, we improve the current camera and meeting room configuration according to the error distribution. By repeating these steps, we can obtain a good meeting room configuration and parameters of all cameras for this room configuration.</div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088475&CFID=105751022&CFTOKEN=32053859">A multimodal perceptual user interface for video-surveillance environments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100029346&CFID=105751022&CFTOKEN=32053859">Giancarlo Iannizzotto</a>, 
                        <a href="author_page.cfm?id=81100120660&CFID=105751022&CFTOKEN=32053859">Carlo Costanzo</a>, 
                        <a href="author_page.cfm?id=81100259989&CFID=105751022&CFTOKEN=32053859">Francesco La Rosa</a>, 
                        <a href="author_page.cfm?id=81100131891&CFID=105751022&CFTOKEN=32053859">Pietro Lanzafame</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 45 - 52</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088475" title="DOI">10.1145/1088463.1088475</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088475&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow10" style="display:inline;"><br /><div style="display:inline">In this paper a perceptual user interface (PUI) for video-surveillance environments is introduced. This system provides a tool for a video-surveillance control-room, and exploits a novel multimodal user interaction paradigm based on hand gesture and ...</div></span>
          <span id="toHide10" style="display:none;"><br /><div style="display:inline">In this paper a perceptual user interface (PUI) for video-surveillance environments is introduced. This system provides a tool for a video-surveillance control-room, and exploits a novel multimodal user interaction paradigm based on hand gesture and perceptual user interfaces. The proposed system, being simple and intuitive, is expected to be useful in the control of large and dynamic environments. To illustrate our work, we introduce a proof-of concept multimodal, bare-hand gesture-based application and discuss its implementation and the obtained experimental results.</div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088476&CFID=105751022&CFTOKEN=32053859">Inferring body pose using speech content</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81408592900&CFID=105751022&CFTOKEN=32053859">Sy Bor Wang</a>, 
                        <a href="author_page.cfm?id=81100236659&CFID=105751022&CFTOKEN=32053859">David Demirdjian</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 53 - 60</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088476" title="DOI">10.1145/1088463.1088476</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088476&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow11" style="display:inline;"><br /><div style="display:inline">Untethered multimodal interfaces are more attractive than tethered ones because they are more natural and expressive for interaction. Such interfaces usually require robust vision-based body pose estimation and gesture recognition. In interfaces where ...</div></span>
          <span id="toHide11" style="display:none;"><br /><div style="display:inline">Untethered multimodal interfaces are more attractive than tethered ones because they are more natural and expressive for interaction. Such interfaces usually require robust vision-based body pose estimation and gesture recognition. In interfaces where a user is interacting with a computer using speech and arm gestures, the user's spoken keywords can be recognized in conjuction with a hypothesis of body poses. This co-occurence can reduce the number of body pose hypothesis for the vision based tracker. In this paper we show that incorporating speech-based body pose constraints can increase the robustness and accuracy of vision-based tracking systems.Next, we describe an approach for gesture recognition. We show how Linear Discriminant Analysis (LDA), can be employed to estimate 'good features' that can be used in a standard HMM-based gesture recognition system. We show that, by applying our LDA scheme, recognition errors can be significantly reduced over a standard HMM-based technique.We applied both techniques in a <i>Virtual Home Desktop</i> scenario. Experiments where the users controlled a desktop system using gestures and speech were conducted and the results show that the speech recognised in conjunction with body poses has increased the accuracy of the vision-based tracking system.</div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088477&CFID=105751022&CFTOKEN=32053859">A joint particle filter for audio-visual speaker tracking</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100533040&CFID=105751022&CFTOKEN=32053859">Kai Nickel</a>, 
                        <a href="author_page.cfm?id=81100292149&CFID=105751022&CFTOKEN=32053859">Tobias Gehrig</a>, 
                        <a href="author_page.cfm?id=81100650593&CFID=105751022&CFTOKEN=32053859">Rainer Stiefelhagen</a>, 
                        <a href="author_page.cfm?id=81100307235&CFID=105751022&CFTOKEN=32053859">John McDonough</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 61 - 68</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088477" title="DOI">10.1145/1088463.1088477</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088477&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow12" style="display:inline;"><br /><div style="display:inline">In this paper, we present a novel approach for tracking a lecturer during the course of his speech. We use features from multiple cameras and microphones, and process them in a joint particle filter framework. The filter performs sampled projections ...</div></span>
          <span id="toHide12" style="display:none;"><br /><div style="display:inline">In this paper, we present a novel approach for tracking a lecturer during the course of his speech. We use features from multiple cameras and microphones, and process them in a joint particle filter framework. The filter performs sampled projections of 3D location hypotheses and scores them using features from both audio and video. On the video side, the features are based on foreground segmentation, multi-view face detection and upper body detection. On the audio side, the time delays of arrival between pairs of microphones are estimated with a generalized cross correlation function. Computationally expensive features are evaluated only at the particles' projected positions in the respective camera images, thus the complexity of the proposed algorithm is low. We evaluated the system on data that was recorded during actual lectures. The results of our experiments were 36 cm average error for video only tracking, 46 cm for audio only, and 31 cm for the combined audio-video system.</div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088478&CFID=105751022&CFTOKEN=32053859">The connector: facilitating context-aware communication</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100662732&CFID=105751022&CFTOKEN=32053859">M. Danninger</a>, 
                        <a href="author_page.cfm?id=81100254180&CFID=105751022&CFTOKEN=32053859">G. Flaherty</a>, 
                        <a href="author_page.cfm?id=81322489281&CFID=105751022&CFTOKEN=32053859">K. Bernardin</a>, 
                        <a href="author_page.cfm?id=81309488679&CFID=105751022&CFTOKEN=32053859">H. K. Ekenel</a>, 
                        <a href="author_page.cfm?id=81100565258&CFID=105751022&CFTOKEN=32053859">T. K&#246;hler</a>, 
                        <a href="author_page.cfm?id=81100032080&CFID=105751022&CFTOKEN=32053859">R. Malkin</a>, 
                        <a href="author_page.cfm?id=81100650593&CFID=105751022&CFTOKEN=32053859">R. Stiefelhagen</a>, 
                        <a href="author_page.cfm?id=81100599954&CFID=105751022&CFTOKEN=32053859">A. Waibel</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 69 - 75</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088478" title="DOI">10.1145/1088463.1088478</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088478&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow13" style="display:inline;"><br /><div style="display:inline">We present the Connector, a context-aware service that intelligently connects people. It maintains an awareness of its users' activities, preoccupations and social relationships to mediate a proper connection at the right time between them. In addition ...</div></span>
          <span id="toHide13" style="display:none;"><br /><div style="display:inline">We present the Connector, a context-aware service that intelligently connects people. It maintains an awareness of its users' activities, preoccupations and social relationships to mediate a proper connection at the right time between them. In addition to providing users with important contextual cues about the availability of potential callees, the Connector adapts the behavior of the contactee's device automatically in order to avoid inappropriate interruptions.To acquire relevant context information, perceptual components analyze sensor input obtained from a smart mobile phone and --- if available --- from a variety of audio-visual sensors built into a smart meeting room environment. The Connector also uses any available multimodal interface (e.g. a speech interface to the smart phone, steerable camera-projector, targeted loudspeakers) in the smart meeting room, to deliver information to users in the most unobtrusive way possible.</div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088479&CFID=105751022&CFTOKEN=32053859">A user interface framework for multimodal VR interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100286130&CFID=105751022&CFTOKEN=32053859">Marc Erich Latoschik</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 76 - 83</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088479" title="DOI">10.1145/1088463.1088479</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088479&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow14" style="display:inline;"><br /><div style="display:inline">This article presents a User Interface (UI) framework for multimodal interactions targeted at immersive virtual environments. Its configurable input and gesture processing components provide an advanced behavior graph capable of routing continuous data ...</div></span>
          <span id="toHide14" style="display:none;"><br /><div style="display:inline">This article presents a User Interface (UI) framework for multimodal interactions targeted at immersive virtual environments. Its configurable input and gesture processing components provide an advanced behavior graph capable of routing continuous data streams asynchronously. The framework introduces a Knowledge Representation Layer which augments objects of the simulated environment with Semantic Entities as a central object model that bridges and interfaces Virtual Reality (VR) and Artificial Intelligence (AI) representations. Specialized node types use these facilities to implement required processing tasks like gesture detection, preprocessing of the visual scene for multimodal integration, or translation of movements into multimodally initialized gestural interactions. A modified Augmented Transition Nettwork (ATN) approach accesses the knowledge layer as well as the preprocessing components to integrate linguistic, gestural, and context information in parallel. The overall framework emphasizes extensibility, adaptivity and reusability, e.g., by utilizing persistent and interchangeable XML-based formats to describe its processing stages.</div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088480&CFID=105751022&CFTOKEN=32053859">Multimodal output specification / simulation platform</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100198027&CFID=105751022&CFTOKEN=32053859">Cyril Rousseau</a>, 
                        <a href="author_page.cfm?id=81100418833&CFID=105751022&CFTOKEN=32053859">Yacine Bellik</a>, 
                        <a href="author_page.cfm?id=81100297884&CFID=105751022&CFTOKEN=32053859">Fr&#233;d&#233;ric Vernier</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 84 - 91</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088480" title="DOI">10.1145/1088463.1088480</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088480&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow15" style="display:inline;"><br /><div style="display:inline">The design of an output multimodal system is a complex task due to the richness of today interaction contexts. The diversity of environments, systems and user profiles requires a new generation of software tools to specify complete and valid output interactions. ...</div></span>
          <span id="toHide15" style="display:none;"><br /><div style="display:inline">The design of an output multimodal system is a complex task due to the richness of today interaction contexts. The diversity of environments, systems and user profiles requires a new generation of software tools to specify complete and valid output interactions. In this paper, we present a multimodal output specification and simulation platform. After introducing the design process which inspired this platform, we describe the two main platform's tools which respectively allow the outputs specification and the outputs simulation of a multimodal system. Finally, an application of the platform is illustrated through the outputs design on a mobile phone application.</div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088481&CFID=105751022&CFTOKEN=32053859">Migratory MultiModal interfaces in MultiDevice environments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100190874&CFID=105751022&CFTOKEN=32053859">Silvia Berti</a>, 
                        <a href="author_page.cfm?id=81100621721&CFID=105751022&CFTOKEN=32053859">Fabio Patern&#242;</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 92 - 99</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088481" title="DOI">10.1145/1088463.1088481</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088481&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow16" style="display:inline;"><br /><div style="display:inline">This paper describes an environment able to support migratory multimodal interfaces in multidevice environments. We introduce the software architecture and the device-independent languages used by our tool, which provides services enabling users to freely ...</div></span>
          <span id="toHide16" style="display:none;"><br /><div style="display:inline">This paper describes an environment able to support migratory multimodal interfaces in multidevice environments. We introduce the software architecture and the device-independent languages used by our tool, which provides services enabling users to freely move about, change device and continue the current task from the point where they left off in the previous device. Our environment currently supports interaction with applications through graphical and vocal modalities, either separately or together. Such applications are implemented in Web-based languages. We discuss how the features of the device at hand, desktop or mobile, are considered when generating the multimodal user interface.</div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088482&CFID=105751022&CFTOKEN=32053859">Exploring multimodality in the laboratory and the field</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100474822&CFID=105751022&CFTOKEN=32053859">Lynne Baillie</a>, 
                        <a href="author_page.cfm?id=81100589972&CFID=105751022&CFTOKEN=32053859">Raimund Schatz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 100 - 107</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088482" title="DOI">10.1145/1088463.1088482</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088482&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow17" style="display:inline;"><br /><div style="display:inline">There are new challenges to us, as researchers, on how to design and evaluate new mobile applications because they give users access to powerful computing devices through small interfaces, which typically have limited input facilities. One way of overcoming ...</div></span>
          <span id="toHide17" style="display:none;"><br /><div style="display:inline">There are new challenges to us, as researchers, on how to design and evaluate new mobile applications because they give users access to powerful computing devices through small interfaces, which typically have limited input facilities. One way of overcoming these shortcomings is to utilize the possibilities of multimodality. We report in this paper how we designed, developed, and evaluated a multimodal mobile application through a combination of laboratory and field studies. This is the first time, as far as we know, that a multimodal application has been developed in such a way. We did this so that we would understand more about where and when users envisioned using different modes of interaction and what problems they may encounter when using an application in context.</div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Visual attention</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088484&CFID=105751022&CFTOKEN=32053859">Understanding the effect of life-like interface agents through users' eye movements</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100606978&CFID=105751022&CFTOKEN=32053859">Helmut Prendinger</a>, 
                        <a href="author_page.cfm?id=81100447423&CFID=105751022&CFTOKEN=32053859">Chunling Ma</a>, 
                        <a href="author_page.cfm?id=81100469250&CFID=105751022&CFTOKEN=32053859">Jin Yingzi</a>, 
                        <a href="author_page.cfm?id=81100194166&CFID=105751022&CFTOKEN=32053859">Arturo Nakasone</a>, 
                        <a href="author_page.cfm?id=81100244448&CFID=105751022&CFTOKEN=32053859">Mitsuru Ishizuka</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 108 - 115</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088484" title="DOI">10.1145/1088463.1088484</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088484&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow19" style="display:inline;"><br /><div style="display:inline">We motivate an approach to evaluating the utility of life-like interface agents that is based on human eye movements rather than questionnaires. An eye tracker is employed to obtain quantitative evidence of a user's focus of attention. The salient feature ...</div></span>
          <span id="toHide19" style="display:none;"><br /><div style="display:inline">We motivate an approach to evaluating the utility of life-like interface agents that is based on human eye movements rather than questionnaires. An eye tracker is employed to obtain quantitative evidence of a user's focus of attention. The salient feature of our evaluation strategy is that it allows us to measure important properties of a user's interaction experience on a moment-by-moment basis in addition to a cumulative (spatial) analysis of the user's areas of interest. We describe an empirical study in which we compare attending behavior of subjects watching the presentation of an apartment by three types of media: an animated agent, a text box, and speech only. The investigation of users' eye movements reveals that agent behavior may trigger natural and social interaction behavior of human users.</div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088485&CFID=105751022&CFTOKEN=32053859">Analyzing and predicting focus of attention in remote collaborative tasks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100177074&CFID=105751022&CFTOKEN=32053859">Jiazhi Ou</a>, 
                        <a href="author_page.cfm?id=81100059735&CFID=105751022&CFTOKEN=32053859">Lui Min Oh</a>, 
                        <a href="author_page.cfm?id=81100583820&CFID=105751022&CFTOKEN=32053859">Susan R. Fussell</a>, 
                        <a href="author_page.cfm?id=81100159818&CFID=105751022&CFTOKEN=32053859">Tal Blum</a>, 
                        <a href="author_page.cfm?id=81350589970&CFID=105751022&CFTOKEN=32053859">Jie Yang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 116 - 123</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088485" title="DOI">10.1145/1088463.1088485</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088485&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow20" style="display:inline;"><br /><div style="display:inline">To overcome the limitations of current technologies for remote collaboration, we propose a system that changes a video feed based on task properties, people's actions, and message properties. First, we examined how participants manage different visual ...</div></span>
          <span id="toHide20" style="display:none;"><br /><div style="display:inline">To overcome the limitations of current technologies for remote collaboration, we propose a system that changes a video feed based on task properties, people's actions, and message properties. First, we examined how participants manage different visual resources in a laboratory experiment using a collaborative task in which one partner (the <i>helper</i>) instructs another (the <i>worker</i>) how to assemble online puzzles. We analyzed helpers' eye gaze as a function of the aforementioned parameters. Helpers gazed at the set of alternative pieces more frequently when it was harder for workers to differentiate these pieces, and less frequently over repeated trials. The results further suggest that a helper's desired focus of attention can be predicted based on task properties, his/her partner's actions, and message properties. We propose a conditional Markov model classifier to explore the feasibility of predicting gaze based on these properties. The accuracy of the model ranged from 65.40% for puzzles with easy-to-name pieces to 74.25% for puzzles with more difficult to name pieces. The results suggest that we can use our model to automatically manipulate video feeds to show what helpers want to see when they want to see it.</div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088486&CFID=105751022&CFTOKEN=32053859">Gaze-based selection of standard-size menu items</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100390422&CFID=105751022&CFTOKEN=32053859">Oleg &#352;pakov</a>, 
                        <a href="author_page.cfm?id=81100471877&CFID=105751022&CFTOKEN=32053859">Darius Miniotas</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 124 - 128</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088486" title="DOI">10.1145/1088463.1088486</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088486&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow21" style="display:inline;"><br /><div style="display:inline">With recent advances in eye tracking technology, eye gaze gradually gains acceptance as a pointing modality. Its relatively low accuracy, however, determines the need to use enlarged controls in eye-based interfaces rendering their design rather peculiar. ...</div></span>
          <span id="toHide21" style="display:none;"><br /><div style="display:inline">With recent advances in eye tracking technology, eye gaze gradually gains acceptance as a pointing modality. Its relatively low accuracy, however, determines the need to use enlarged controls in eye-based interfaces rendering their design rather peculiar. Another factor impairing pointing performance is deficient robustness of an eye tracker's calibration. To facilitate pointing at standard-size menus, we developed a technique that uses dynamic target expansion for on-line correction of the eye tracker's calibration. Correction is based on the relative change in the gaze point location upon the expansion. A user study suggests that the technique affords a dramatic six-fold improvement in selection accuracy. This is traded off against a much smaller reduction in performance speed (39%). The technique is thus believed to contribute to development of universal-access solutions supporting navigation through standard menus by eye gaze alone.</div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088487&CFID=105751022&CFTOKEN=32053859">Region extraction of a gaze object using the gaze point and view image sequences</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100090179&CFID=105751022&CFTOKEN=32053859">Norimichi Ukita</a>, 
                        <a href="author_page.cfm?id=81100261585&CFID=105751022&CFTOKEN=32053859">Tomohisa Ono</a>, 
                        <a href="author_page.cfm?id=81100643609&CFID=105751022&CFTOKEN=32053859">Masatsugu Kidode</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 129 - 136</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088487" title="DOI">10.1145/1088463.1088487</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088487&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow22" style="display:inline;"><br /><div style="display:inline">Analysis of the human gaze is a basic way to investigate human attention. Similarly, the view image of a human being includes the visual information of what he/she pays attention to.This paper proposes an interface system for extracting the region of ...</div></span>
          <span id="toHide22" style="display:none;"><br /><div style="display:inline">Analysis of the human gaze is a basic way to investigate human attention. Similarly, the view image of a human being includes the visual information of what he/she pays attention to.This paper proposes an interface system for extracting the region of an object viewed by a human from a view image sequence by analyzing the history of gaze points. All the gaze points, each of which is recorded as a 2D point in a view image, are transfered to an image in which the object region is extracted. These points are then divided into several groups based on their colors and positions. The gaze points in each group compose an initial region. After all the regions are extended, outlier regions are removed by comparing the colors and optical flows in the extended regions. All the remaining regions are merged into one in order to compose a gaze region.</div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088465&CFID=105751022&CFTOKEN=32053859">Interactive humanoids and androids as ideal interfaces for humans</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100572813&CFID=105751022&CFTOKEN=32053859">Hiroshi Ishiguro</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 137 - 137</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088465" title="DOI">10.1145/1088463.1088465</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088465&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow23" style="display:inline;"><br /><div style="display:inline">Many robotics researchers are exploring new possibilities of intelligent robots in our everyday life. Humanoid and androids, which have various modalities, can communicate with humans as new information media. In this talk, we argue how to develop the ...</div></span>
          <span id="toHide23" style="display:none;"><br /><div style="display:inline">Many robotics researchers are exploring new possibilities of intelligent robots in our everyday life. Humanoid and androids, which have various modalities, can communicate with humans as new information media. In this talk, we argue how to develop the interactive robots and how to evaluate them as introducing several robots developed in ATR Intelligent Robotics and Communications Laboratories and Department of Adaptive Machine Systems, Osaka University. Especially, we focus on a constructive approach to developing the interactive robots, cognitive studies using the humanoids and androids for evaluating the interactions, and long-term field experiments in an elementary school.The talk consists of two parts. There are two relationships between robots and humans: one is inter-personal and the other is social. In the inter-personal relationships, the appearance of the robot is a new and important research issues. In the social relationships, a function to recognize human relationships through interactions is needed for robots of the next generation. These two issues explore new possibilities of robots.In these issues, the appearance problem bridges between science and engineering. In the development of humanoids, both the appearance and behavior of the robots are significant issues. However, designing the robot's appearance, especially to give it a humanoid one, was always a role of the industrial designer. To tackle the problem of appearance and behavior, two approaches are necessary: one from robotics and the other from cognitive science. The approach from robotics tries to build very humanlike robots based on knowledge from cognitive science. The approach from cognitive science uses the robot for verifying hypotheses for understanding humans. We call this cross-interdisciplinary framework android science (www.androidscience.com).The speaker hopes that attendees catch new waves in robotics and media research and our feature life.</div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Semantics and dialog</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088489&CFID=105751022&CFTOKEN=32053859">Probabilistic grounding of situated speech using plan recognition and reference resolution</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100399942&CFID=105751022&CFTOKEN=32053859">Peter Gorniak</a>, 
                        <a href="author_page.cfm?id=81100653347&CFID=105751022&CFTOKEN=32053859">Deb Roy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 138 - 143</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088489" title="DOI">10.1145/1088463.1088489</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088489&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow25" style="display:inline;"><br /><div style="display:inline">Situated, spontaneous speech may be ambiguous along acoustic, lexical, grammatical and semantic dimensions. To understand such a seemingly difficult signal, we propose to model the ambiguity inherent in acoustic signals and in lexical and grammatical ...</div></span>
          <span id="toHide25" style="display:none;"><br /><div style="display:inline">Situated, spontaneous speech may be ambiguous along acoustic, lexical, grammatical and semantic dimensions. To understand such a seemingly difficult signal, we propose to model the ambiguity inherent in acoustic signals and in lexical and grammatical choices using compact, probabilistic representations of multiple hypotheses. To resolve semantic ambiguities we propose a situation model that captures aspects of the physical context of an utterance as well as the speaker's intentions, in our case represented by recognized plans. In a single, coherent Framework for Understanding Situated Speech (FUSS) we show how these two influences, acting on an ambiguous representation of the speech signal, complement each other to disambiguate form and content of situated speech. This method produces promising results in a game playing environment and leaves room for other types of situation models.</div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088490&CFID=105751022&CFTOKEN=32053859">Augmenting conversational dialogue by means of latent semantic googling</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100444909&CFID=105751022&CFTOKEN=32053859">Robin Senior</a>, 
                        <a href="author_page.cfm?id=81100497077&CFID=105751022&CFTOKEN=32053859">Roel Vertegaal</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 144 - 150</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088490" title="DOI">10.1145/1088463.1088490</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088490&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow26" style="display:inline;"><br /><div style="display:inline">This paper presents Latent Semantic Googling, a variant of Landauer's Latent Semantic Indexing that uses the Google search engine to judge the semantic closeness of sets of words and phrases. This concept is implemented via Ambient Google, ...</div></span>
          <span id="toHide26" style="display:none;"><br /><div style="display:inline">This paper presents Latent <i>Semantic Googling</i>, a variant of Landauer's Latent Semantic Indexing that uses the Google search engine to judge the semantic closeness of sets of words and phrases. This concept is implemented via <i>Ambient Google</i>, a system for augmenting conversations through the classification of discussed topics. Ambient Google uses a speech recognition engine to generate Google keyphrase queries directly from conversations. These queries are used to analyze the semantics of the conversation, and infer related topics that have been discussed. Conversations are visualized using a spring-model algorithm representing common topics. This allows users to browse their conversation as a contextual relationship between discussed topics, and augment their discussion through the use of related websites discovered by Google. An evaluation of Ambient Google is presented, discussing user reaction to the system.</div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088491&CFID=105751022&CFTOKEN=32053859">Human-style interaction with a robot for cooperative learning of scene objects</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100477264&CFID=105751022&CFTOKEN=32053859">Shuyin Li</a>, 
                        <a href="author_page.cfm?id=81100259030&CFID=105751022&CFTOKEN=32053859">Axel Haasch</a>, 
                        <a href="author_page.cfm?id=81100624850&CFID=105751022&CFTOKEN=32053859">Britta Wrede</a>, 
                        <a href="author_page.cfm?id=81100392182&CFID=105751022&CFTOKEN=32053859">Jannik Fritsch</a>, 
                        <a href="author_page.cfm?id=81100458351&CFID=105751022&CFTOKEN=32053859">Gerhard Sagerer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 151 - 158</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088491" title="DOI">10.1145/1088463.1088491</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088491&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow27" style="display:inline;"><br /><div style="display:inline">In research on human-robot interaction the interest is currently shifting from uni-modal dialog systems to multi-modal interaction schemes. We present a system for human-style interaction with a robot that is integrated on our mobile robot BIRON. To ...</div></span>
          <span id="toHide27" style="display:none;"><br /><div style="display:inline">In research on human-robot interaction the interest is currently shifting from uni-modal dialog systems to multi-modal interaction schemes. We present a system for human-style interaction with a robot that is integrated on our mobile robot BIRON. To model the dialog we adopt an extended grounding concept with a mechanism to handle multi-modal in- and output where object references are resolved by the interaction with an object attention system (OAS). The OAS integrates multiple input from, e.g., the object and gesture recognition systems and provides the information for a common representation. This representation can be accessed by both modules and combines symbolic verbal attributes with sensor-based features. We argue that such a representation is necessary to achieve a robust and efficient information processing.</div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088492&CFID=105751022&CFTOKEN=32053859">A look under the hood: design and development of the first SmartWeb system demonstrator</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100611895&CFID=105751022&CFTOKEN=32053859">Norbert Reithinger</a>, 
                        <a href="author_page.cfm?id=81100639440&CFID=105751022&CFTOKEN=32053859">Simon Bergweiler</a>, 
                        <a href="author_page.cfm?id=81100183214&CFID=105751022&CFTOKEN=32053859">Ralf Engel</a>, 
                        <a href="author_page.cfm?id=81100423026&CFID=105751022&CFTOKEN=32053859">Gerd Herzog</a>, 
                        <a href="author_page.cfm?id=81100257341&CFID=105751022&CFTOKEN=32053859">Norbert Pfleger</a>, 
                        <a href="author_page.cfm?id=81100431371&CFID=105751022&CFTOKEN=32053859">Massimo Romanelli</a>, 
                        <a href="author_page.cfm?id=81100457213&CFID=105751022&CFTOKEN=32053859">Daniel Sonntag</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 159 - 166</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088492" title="DOI">10.1145/1088463.1088492</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088492&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow28" style="display:inline;"><br /><div style="display:inline">Experience shows that decisions in the early phases of the development of a multimodal system prevail throughout the life-cycle of a project. The distributed architecture and the requirement for robust multimodal interaction in our project SmartWeb resulted ...</div></span>
          <span id="toHide28" style="display:none;"><br /><div style="display:inline">Experience shows that decisions in the early phases of the development of a multimodal system prevail throughout the life-cycle of a project. The distributed architecture and the requirement for robust multimodal interaction in our project SmartWeb resulted in an approach that uses and extends W3C standards like EMMA and RDFS. These standards for the interface structure and content allowed us to integrate available tools and techniques. However, the requirements in our system called for various extensions, e.g., to introduce result feedback tags for an extended version of EMMA. The interconnection framework depends on a commercial telephone voice dialog system platform for the dialog-centric components while the information access processes are linked using web service technology. Also in the area of this underlying infrastructure, enhancements and extensions were necessary. The first demonstration system is operable now and will be presented at the Football World Cup 2006 in Germany.</div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Recognizing communication patterns</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088494&CFID=105751022&CFTOKEN=32053859">Audio-visual cues distinguishing self- from system-directed speech in younger and older adults</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100062088&CFID=105751022&CFTOKEN=32053859">Rebecca Lunsford</a>, 
                        <a href="author_page.cfm?id=81100656112&CFID=105751022&CFTOKEN=32053859">Sharon Oviatt</a>, 
                        <a href="author_page.cfm?id=81100413045&CFID=105751022&CFTOKEN=32053859">Rachel Coulston</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 167 - 174</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088494" title="DOI">10.1145/1088463.1088494</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088494&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow30" style="display:inline;"><br /><div style="display:inline">In spite of interest in developing robust open-microphone engagement techniques for mobile use and natural field contexts, there currently are no reliable techniques available. One problem is the lack of empirically-grounded models as guidance for distinguishing ...</div></span>
          <span id="toHide30" style="display:none;"><br /><div style="display:inline">In spite of interest in developing robust open-microphone engagement techniques for mobile use and natural field contexts, there currently are no reliable techniques available. One problem is the lack of empirically-grounded models as guidance for distinguishing how users' audio-visual activity actually differs systematically when addressing a computer versus human partner. In particular, existing techniques have not been designed to handle high levels of user self talk as a source of "noise," and they typically assume that a user is addressing the system only when facing it while speaking. In the present research, data were collected during two related studies in which adults aged 18-89 interacted multimodally using speech and pen with a simulated map system. Results revealed that people engaged in self talk prior to addressing the system over 30% of the time, with no decrease in younger adults' rate of self talk compared with elders. Speakers' amplitude was lower during 96% of their self talk, with a substantial 26 dBr amplitude separation observed between self- and system-directed speech. The magnitude of speaker's amplitude separation ranged from approximately 10-60 dBr and diminished with age, with 79% of the variance predictable simply by knowing a person's age. In contrast to the clear differentiation of intended addressee revealed by amplitude separation, gaze at the system was not a reliable indicator of speech directed to the system, with users looking at the system over 98% of the time during both self- and system-directed speech. Results of this research have implications for the design of more effective open-microphone engagement for mobile and pervasive systems.</div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088495&CFID=105751022&CFTOKEN=32053859">Identifying the intended addressee in mixed human-human and human-computer interaction from non-verbal features</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100340372&CFID=105751022&CFTOKEN=32053859">Koen van Turnhout</a>, 
                        <a href="author_page.cfm?id=81100650173&CFID=105751022&CFTOKEN=32053859">Jacques Terken</a>, 
                        <a href="author_page.cfm?id=81100418421&CFID=105751022&CFTOKEN=32053859">Ilse Bakx</a>, 
                        <a href="author_page.cfm?id=81100262920&CFID=105751022&CFTOKEN=32053859">Berry Eggen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 175 - 182</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088495" title="DOI">10.1145/1088463.1088495</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088495&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow31" style="display:inline;"><br /><div style="display:inline">Against the background of developments in the area of speech-based and multimodal interfaces, we present research on determining the addressee of an utterance in the context of mixed human-human and multimodal human-computer interaction. Working with ...</div></span>
          <span id="toHide31" style="display:none;"><br /><div style="display:inline">Against the background of developments in the area of speech-based and multimodal interfaces, we present research on determining the addressee of an utterance in the context of mixed human-human and multimodal human-computer interaction. Working with data that are taken from realistic scenarios, we explore several features with respect to their relevance to the question who is the addressee of an utterance: eye gaze both of speaker and listener, dialogue history and utterance length. With respect to eye gaze, we inspect the detailed timing of shifts in eye gaze between different communication partners (human or computer). We show that these features result in an improved classification of utterances in terms of addressee-hood relative to a simple classification algorithm that assumes that "the addressee is where the eye is", and compare our results to alternative approaches.</div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088496&CFID=105751022&CFTOKEN=32053859">Multimodal multispeaker probabilistic tracking in meetings</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100273781&CFID=105751022&CFTOKEN=32053859">Daniel Gatica-Perez</a>, 
                        <a href="author_page.cfm?id=81100387025&CFID=105751022&CFTOKEN=32053859">Guillaume Lathoud</a>, 
                        <a href="author_page.cfm?id=81452616802&CFID=105751022&CFTOKEN=32053859">Jean-Marc Odobez</a>, 
                        <a href="author_page.cfm?id=81100410555&CFID=105751022&CFTOKEN=32053859">Iain McCowan</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 183 - 190</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088496" title="DOI">10.1145/1088463.1088496</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088496&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow32" style="display:inline;"><br /><div style="display:inline">Tracking speakers in multiparty conversations constitutes a fundamental task for automatic meeting analysis. In this paper, we present a probabilistic approach to jointly track the location and speaking activity of multiple speakers in a multisensor ...</div></span>
          <span id="toHide32" style="display:none;"><br /><div style="display:inline">Tracking speakers in multiparty conversations constitutes a fundamental task for automatic meeting analysis. In this paper, we present a probabilistic approach to jointly track the location and speaking activity of multiple speakers in a multisensor meeting room, equipped with a small microphone array and multiple uncalibrated cameras. Our framework is based on a mixed-state dynamic graphical model defined on a multiperson state-space, which includes the explicit definition of a proximity-based interaction model. The model integrates audio-visual (AV) data through a novel observation model. Audio observations are derived from a source localization algorithm. Visual observations are based on models of the shape and spatial structure of human heads. Approximate inference in our model, needed given its complexity, is performed with a Markov Chain Monte Carlo particle filter (MCMC-PF), which results in high sampling efficiency. We present results -based on an objective evaluation procedure-that show that our framework (1) is capable of locating and tracking the position and speaking activity of multiple meeting participants engaged in real conversations with good accuracy; (2) can deal with cases of visual clutter and partial occlusion; and (3) significantly outperforms a traditional sampling-based approach.</div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088497&CFID=105751022&CFTOKEN=32053859">A probabilistic inference of multiparty-conversation structure based on Markov-switching models of gaze patterns, head directions, and utterances</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100500908&CFID=105751022&CFTOKEN=32053859">Kazuhiro Otsuka</a>, 
                        <a href="author_page.cfm?id=81100337373&CFID=105751022&CFTOKEN=32053859">Yoshinao Takemae</a>, 
                        <a href="author_page.cfm?id=81100230754&CFID=105751022&CFTOKEN=32053859">Junji Yamato</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 191 - 198</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088497" title="DOI">10.1145/1088463.1088497</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088497&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow33" style="display:inline;"><br /><div style="display:inline">A novel probabilistic framework is proposed for inferring the structure of conversation in face-to-face multiparty communication, based on gaze patterns, head directions and the presence/absence of utterances. As the structure of conversation, this study ...</div></span>
          <span id="toHide33" style="display:none;"><br /><div style="display:inline">A novel probabilistic framework is proposed for inferring the structure of conversation in face-to-face multiparty communication, based on gaze patterns, head directions and the presence/absence of utterances. As the structure of conversation, this study focuses on the combination of participants and their participation roles. First, we assess the gaze patterns that frequently appear in conversations, and define typical types of conversation structure, called conversational regime, and hypothesize that the regime represents the high-level process that governs how people interact during conversations. Next, assuming that the regime changes over time exhibit Markov properties, we propose a probabilistic conversation model based on Markov-switching; the regime controls the dynamics of utterances and gaze patterns, which stochastically yield measurable head-direction changes. Furthermore, a Gibbs sampler is used to realize the Bayesian estimation of regime, gaze pattern, and model parameters from observed head directions and utterances. Experiments on four-person conversations confirm the effectiveness of the framework in identifying conversation structures.</div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088466&CFID=105751022&CFTOKEN=32053859">Socially aware computation and communication</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81452609331&CFID=105751022&CFTOKEN=32053859">Alex (Sandy) Pentland</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 199 - 199</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088466" title="DOI">10.1145/1088463.1088466</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088466&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow34" style="display:inline;"><br /><div style="display:inline">By building machines that understand social signaling and social context, we can dramatically improve collective decision making and help keep remote users 'in the loop.' I will describe three systems that have a substantial understanding of social context, ...</div></span>
          <span id="toHide34" style="display:none;"><br /><div style="display:inline">By building machines that understand social signaling and social context, we can dramatically improve collective decision making and help keep remote users 'in the loop.' I will describe three systems that have a substantial understanding of social context, and use this understanding to improve human group performance. The first system is able to interpret social displays of interest and attraction, and uses this information to improve conferences and meetings. The second is able to infer friendship, acquaitance, and workgroup relationships, and uses this to help people build social capital. The third is able to examine human interactions and categorize participants attitudes (attentive, agreeable, determined, interested, etc), and uses this information to proactively promote group cohesion and to match participants on the basis of their compatiblity.</div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Affective interaction</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088499&CFID=105751022&CFTOKEN=32053859">Synthetic characters as multichannel interfaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100196401&CFID=105751022&CFTOKEN=32053859">Elena Not</a>, 
                        <a href="author_page.cfm?id=81100652541&CFID=105751022&CFTOKEN=32053859">Koray Balci</a>, 
                        <a href="author_page.cfm?id=81100424906&CFID=105751022&CFTOKEN=32053859">Fabio Pianesi</a>, 
                        <a href="author_page.cfm?id=81100153077&CFID=105751022&CFTOKEN=32053859">Massimo Zancanaro</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 200 - 207</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088499" title="DOI">10.1145/1088463.1088499</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088499&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow36" style="display:inline;"><br /><div style="display:inline">Synthetic characters are an effective modality to convey messages to the user, provide visual feedback about the system internal understanding of the communication, and engage the user in the dialogue through emotional involvement. In this paper we argue ...</div></span>
          <span id="toHide36" style="display:none;"><br /><div style="display:inline">Synthetic characters are an effective modality to convey messages to the user, provide visual feedback about the system internal understanding of the communication, and engage the user in the dialogue through emotional involvement. In this paper we argue for a fine-grain distinction of the expressive capabilities of synthetic agents: avatars should not be considered as an indivisible modality but as the synergic contribution of different communication channels that, properly synchronized, generate an overall communication performance. In this view, we propose SMIL-AGENT as a representation and scripting language for synthetic characters, which abstracts away from the specific implementation and context of use of the character. SMIL-AGENT has been defined starting from SMIL 0.1 standard specification and aims at providing a high-level standardized language for presentations by different synthetic agents within diverse communication and application contexts.</div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088500&CFID=105751022&CFTOKEN=32053859">XfaceEd: authoring tool for embodied conversational agents</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100652541&CFID=105751022&CFTOKEN=32053859">Koray Balci</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 208 - 213</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088500" title="DOI">10.1145/1088463.1088500</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088500&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow37" style="display:inline;"><br /><div style="display:inline">In this paper, XfaceEd, our open source, platform independent tool for authoring 3D embodied conversational agents (ECAs) is presented. Following MPEG-4 Facial Animation (FA) standard, XfaceEd provides an easy to use interface to generate MPEG-4 ready ...</div></span>
          <span id="toHide37" style="display:none;"><br /><div style="display:inline">In this paper, XfaceEd, our open source, platform independent tool for authoring 3D embodied conversational agents (ECAs) is presented. Following MPEG-4 Facial Animation (FA) standard, XfaceEd provides an easy to use interface to generate MPEG-4 ready ECAs from static 3D models. Users can set MPEG-4 Facial Definition Points (FDP) and Facial Animation Parameter Units (FAPU), define the zone of influence of each feature point and how this influence is propagated among the neighboring vertices. As an alternative to MPEG-4, one can also specify morph targets for different categories such as visemes, emotions and expressions, in order to achieve facial animation using the keyframe interpolation technique. Morph targets from different categories are blended to create more lifelike behaviour.Results can be previewed and parameters can be tweaked real time within the application for fine tuning. Changes made take into effect immediately, which in turn ensures rapid production. The final output is a configuration file XML format and can be interpreted by XfacePlayer or other applications for easy authoring of embodied conversational agents for multimodal environments.</div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088501&CFID=105751022&CFTOKEN=32053859">A first evaluation study of a database of kinetic facial expressions (DaFEx)</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100497910&CFID=105751022&CFTOKEN=32053859">Alberto Battocchi</a>, 
                        <a href="author_page.cfm?id=81100424906&CFID=105751022&CFTOKEN=32053859">Fabio Pianesi</a>, 
                        <a href="author_page.cfm?id=81100179029&CFID=105751022&CFTOKEN=32053859">Dina Goren-Bar</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 214 - 221</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088501" title="DOI">10.1145/1088463.1088501</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088501&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow38" style="display:inline;"><br /><div style="display:inline">In this paper we present DaFEx (Database of Facial Expressions), a database created with the purpose of providing a benchmark for the evaluation of the facial expressivity of Embodied Conversational Agents (ECAs). DaFEx consists of 1008 short videos ...</div></span>
          <span id="toHide38" style="display:none;"><br /><div style="display:inline">In this paper we present DaFEx (Database of Facial Expressions), a database created with the purpose of providing a benchmark for the evaluation of the facial expressivity of Embodied Conversational Agents (ECAs). DaFEx consists of 1008 short videos containing emotional facial expressions of the 6 Ekman's emotions plus the neutral expression. The facial expressions were recorded by 8 professional actors (male and female) in two acting conditions ("utterance" and "no- utterance") and at 3 intensity levels (high, medium, low). The properties of DaFEx were studied by having 80 subjects classify the emotion expressed in the videos. High rates of accuracy were obtained for most of the emotions displayed. We also tested the effect of the intensity level, of the articulatory movements due to speech, and of the actors' and subjects' gender, on classification accuracy. The results showed that decoding accuracy decreases with the intensity of emotions; that the presence of articulatory movements negatively affects the recognition of fear, surprise and of the neutral expression, while it improves the recognition of anger; and that facial expressions seem to be recognized (slightly) better when acted by actresses than by actors.</div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088502&CFID=105751022&CFTOKEN=32053859">Hapticat: exploration of affective touch</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100236385&CFID=105751022&CFTOKEN=32053859">Steve Yohanan</a>, 
                        <a href="author_page.cfm?id=81100176886&CFID=105751022&CFTOKEN=32053859">Mavis Chan</a>, 
                        <a href="author_page.cfm?id=81100348532&CFID=105751022&CFTOKEN=32053859">Jeremy Hopkins</a>, 
                        <a href="author_page.cfm?id=81100447361&CFID=105751022&CFTOKEN=32053859">Haibo Sun</a>, 
                        <a href="author_page.cfm?id=81100608981&CFID=105751022&CFTOKEN=32053859">Karon MacLean</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 222 - 229</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088502" title="DOI">10.1145/1088463.1088502</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088502&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow39" style="display:inline;"><br /><div style="display:inline">This paper describes the Hapticat, a device we developed to study affect through touch. Though intentionally not highly zoomorphic, the device borrows behaviors from pets and the rich manner in which they haptically communicate with humans. The ...</div></span>
          <span id="toHide39" style="display:none;"><br /><div style="display:inline">This paper describes the <i>Hapticat</i>, a device we developed to study affect through touch. Though intentionally not highly zoomorphic, the device borrows behaviors from pets and the rich manner in which they haptically communicate with humans. The Hapticat has four degrees of freedom to express itself: a pair of ear-like appendages, a breathing mechanism, a purring mechanism, and a warming element. Combinations of levels for these controls are used to define the five active haptic responses: <i>playing dead</i>, <i>asleep</i>, <i>content</i>, <i>happy</i>, and <i>upset</i>,. In the paper we present the design considerations and implementation details of the device. We also detail a preliminary observational study where participants interacted with the Hapticat through touch. To compare the effects of haptic feedback, the device presented either active haptic renderings or none at all. Participants reported which of the five responses they believed the Hapticat rendered, as well as their degree of affect to the device. We observed that participants' expectations of the device's response to various haptic stimuli correlated with our mappings. We also observed that participants were able to reasonably recognize three of the five response renderings, while having difficulty discriminating between <i>happy</i> and <i>content</i> states. Finally, we found that participants registered a broader range of affect when active haptic renderings were applied as compared to when none were presented.</div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">POSTER SESSION: <strong>Posters</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088504&CFID=105751022&CFTOKEN=32053859">Using observations of real designers at work to inform the development of a novel haptic modeling system</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100029382&CFID=105751022&CFTOKEN=32053859">Umberto Giraudo</a>, 
                        <a href="author_page.cfm?id=81100131727&CFID=105751022&CFTOKEN=32053859">Monica Bordegoni</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 230 - 235</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088504" title="DOI">10.1145/1088463.1088504</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088504&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow41" style="display:inline;"><br /><div style="display:inline">Gestures, besides speech, represent the mostly used means of expression by humans. For what regards the product design field, designers have multiple ways for communicating their ideas and concepts. One of them concerns the model making activity, where ...</div></span>
          <span id="toHide41" style="display:none;"><br /><div style="display:inline">Gestures, besides speech, represent the mostly used means of expression by humans. For what regards the product design field, designers have multiple ways for communicating their ideas and concepts. One of them concerns the model making activity, where designers make explicit their concepts by using some appropriate tools and specific hand movements on plastic material with the intent of obtaining a shape. Some studies have demonstrated that visual, tactile and kinesthetic feedbacks are equally important in the shape creation and evaluation process [1]. The European project "Touch and Design" (T'nD) (www.kaemart.it/touch-and-design) proposes the implementation of an innovative virtual clay modeling system based on novel haptic interaction modality oriented to industrial designers. In order to develop an intuitive and easy-to-use system, a study of designers' hand modeling activities has been carried out by the project industrial partners supported by cognitive psychologists. The users' manual operators and tools have been translated into corresponding haptic tools and multimodal interaction modalities in the virtual free-form shape modeling system. The paper presents the project research activities and the results achieved so far.</div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088505&CFID=105751022&CFTOKEN=32053859">A comparison of two methods of scaling on form perception via a haptic interface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100148863&CFID=105751022&CFTOKEN=32053859">Mounia Ziat</a>, 
                        <a href="author_page.cfm?id=81100149318&CFID=105751022&CFTOKEN=32053859">Olivier Gapenne</a>, 
                        <a href="author_page.cfm?id=81339530370&CFID=105751022&CFTOKEN=32053859">John Stewart</a>, 
                        <a href="author_page.cfm?id=81100232040&CFID=105751022&CFTOKEN=32053859">Charles Lenay</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 236 - 243</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088505" title="DOI">10.1145/1088463.1088505</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088505&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow42" style="display:inline;"><br /><div style="display:inline">In this fundamental study, we compare two scaling methods by focusing on the subjects' strategies which are using a sensory substitution device. Method 1 consists in a reduction of the sensor size and its displacement speed. Here, speed reduction is ...</div></span>
          <span id="toHide42" style="display:none;"><br /><div style="display:inline">In this fundamental study, we compare two scaling methods by focusing on the subjects' strategies which are using a sensory substitution device. Method 1 consists in a reduction of the sensor size and its displacement speed. Here, speed reduction is obtained by a "human" movement reduction (hand speed reduction). Method 2 consists in a classical increase of the image dimension. The experimental device couples the pen on a graphics tablet with tactile sensory stimulators. These latter are activated when the sensor crosses the figure on the computer screen. This virtual sensor (square matrix composed of 16 elementary fields) is displaced when the pen, guided by a human hand displacements, moves on the graphics tablet. Even if it seems that there is no difference between the two methods, the results show that the recognition rate is closely dependent on the figure size and the strategies used by the subjects are more suitable for method 2 than the method 1. In fact, half of the subjects found that method 1 inhibits their movements and the majority of them don't feel the scaling effect, whereas this is clearly felt in method 2.</div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088506&CFID=105751022&CFTOKEN=32053859">An initial usability assessment for symbolic haptic rendering of music parameters</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100651938&CFID=105751022&CFTOKEN=32053859">Meghan Allen</a>, 
                        <a href="author_page.cfm?id=81100547447&CFID=105751022&CFTOKEN=32053859">Jennifer Gluck</a>, 
                        <a href="author_page.cfm?id=81100608981&CFID=105751022&CFTOKEN=32053859">Karon MacLean</a>, 
                        <a href="author_page.cfm?id=81100640541&CFID=105751022&CFTOKEN=32053859">Erwin Tang</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 244 - 251</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088506" title="DOI">10.1145/1088463.1088506</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088506&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow43" style="display:inline;"><br /><div style="display:inline">Current methods of playlist creation and maintenance do not support user needs, especially in a mobile context. Furthermore, they do not scale: studies show that users with large mp3 collections have abandoned the concept of playlists. To remedy the ...</div></span>
          <span id="toHide43" style="display:none;"><br /><div style="display:inline">Current methods of playlist creation and maintenance do not support user needs, especially in a mobile context. Furthermore, they do not scale: studies show that users with large mp3 collections have abandoned the concept of playlists. To remedy the usability problems associated with playlist creation and navigation - in particular, reliance on visual feedback and the absence of rapid content scanning mechanisms - we propose a system that utilizes the haptic channel. A necessary first step in this objective is the creation of a haptic mapping for music. In this paper, we describe an exploratory study addressed at understanding the feasibility, with respect to learnability and usability, of efficient, eyes-free playlist navigation based on symbolic haptic renderings of key song parameters. Users were able to learn haptic mappings for music parameters to usable accuracy with 4 minutes of training. These results indicate promise for the approach and support for continued effort in both improving the rendering scheme and implementing the haptic playlist system.</div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088507&CFID=105751022&CFTOKEN=32053859">Tangible user interfaces for 3D clipping plane interaction with volumetric data: a case study</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100510581&CFID=105751022&CFTOKEN=32053859">Wen Qi</a>, 
                        <a href="author_page.cfm?id=81100097605&CFID=105751022&CFTOKEN=32053859">Jean-Bernard Martens</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 252 - 258</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088507" title="DOI">10.1145/1088463.1088507</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088507&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow44" style="display:inline;"><br /><div style="display:inline">Visualization via direct volume rendering is a potentially very powerful technique for exploring and interacting with large amounts of scientific data. However, the available two-dimensional (2D) interfaces make three-dimensional (3D) manipulation with ...</div></span>
          <span id="toHide44" style="display:none;"><br /><div style="display:inline">Visualization via direct volume rendering is a potentially very powerful technique for exploring and interacting with large amounts of scientific data. However, the available two-dimensional (2D) interfaces make three-dimensional (3D) manipulation with such data very difficult. Many usability problems during interaction in turn discourage the widespread use of volume rendering as a scientific tool. In this paper, we present a more in-depth investigation into one specific interface aspect, i.e., the positioning of a clipping plane within volume-rendered data. More specifically, we propose three different interface prototypes that have been realized with the help of wireless vision-based tracking. These three prototypes combine aspects of 2D graphical user interfaces with 3D tangible interaction devices. They allow to experience and compare different user interface strategies for performing the clipping plane interaction task. They also provide a basis for carrying out user evaluations in the near future.</div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088508&CFID=105751022&CFTOKEN=32053859">A transformational approach for multimodal web user interfaces based on UsiXML</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100034166&CFID=105751022&CFTOKEN=32053859">Adrian Stanciulescu</a>, 
                        <a href="author_page.cfm?id=81100487053&CFID=105751022&CFTOKEN=32053859">Quentin Limbourg</a>, 
                        <a href="author_page.cfm?id=81100053193&CFID=105751022&CFTOKEN=32053859">Jean Vanderdonckt</a>, 
                        <a href="author_page.cfm?id=81314484150&CFID=105751022&CFTOKEN=32053859">Benjamin Michotte</a>, 
                        <a href="author_page.cfm?id=81100606880&CFID=105751022&CFTOKEN=32053859">Francisco Montero</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 259 - 266</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088508" title="DOI">10.1145/1088463.1088508</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088508&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow45" style="display:inline;"><br /><div style="display:inline">A transformational approach for developing multimodal web user interfaces is presented that progressively moves from a task model and a domain model to a final user interface. This approach consists of three steps: deriving one or many abstract user ...</div></span>
          <span id="toHide45" style="display:none;"><br /><div style="display:inline">A transformational approach for developing multimodal web user interfaces is presented that progressively moves from a task model and a domain model to a final user interface. This approach consists of three steps: deriving one or many abstract user interfaces from a task model and a domain model, deriving one or many concrete user interfaces from each abstract one, and producing the code of the corresponding final user interfaces. To ensure these steps, transformations are encoded as graph transformations performed on the involved models expressed in their graph equivalent. For each step, a graph grammar gathers relevant graph transformations for accomplishing the sub-steps. The final user interface is multimodal as it involves graphical (keyboard, mouse) and vocal interaction. The approach outlined in the paper is illustrated throughout a running example for a graphical interface, a vocal interface, and two multimodal interfaces with graphical and vocal predominances, respectively.</div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088509&CFID=105751022&CFTOKEN=32053859">A pattern mining method for interpretation of interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81341494196&CFID=105751022&CFTOKEN=32053859">Tomoyuki Morita</a>, 
                        <a href="author_page.cfm?id=81341491389&CFID=105751022&CFTOKEN=32053859">Yasushi Hirano</a>, 
                        <a href="author_page.cfm?id=81100087571&CFID=105751022&CFTOKEN=32053859">Yasuyuki Sumi</a>, 
                        <a href="author_page.cfm?id=81100490660&CFID=105751022&CFTOKEN=32053859">Shoji Kajita</a>, 
                        <a href="author_page.cfm?id=81100070056&CFID=105751022&CFTOKEN=32053859">Kenji Mase</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 267 - 273</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088509" title="DOI">10.1145/1088463.1088509</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088509&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow46" style="display:inline;"><br /><div style="display:inline">This paper proposes a novel mining method for multimodal interactions to extract important patterns of group activities. These extracted patterns can be used as machine-readable event indices in developing an interaction corpus based on a huge collection ...</div></span>
          <span id="toHide46" style="display:none;"><br /><div style="display:inline">This paper proposes a novel mining method for multimodal interactions to extract important patterns of group activities. These extracted patterns can be used as machine-readable event indices in developing an interaction corpus based on a huge collection of human interaction data captured by various sensors. The event indices can be used, for example, to summarize a set of events and to search for particular events because they contain various pieces of context information. The proposed method extracts simultaneously occurring patterns of primitive events in interaction, such as gaze and speech, that in combination occur more consistently than randomly. The proposed method provides a statistically plausible definition of interaction events that is not possible through intuitive top-down definitions. We demonstrate the effectiveness of our method for the data captured in an experimental setup of a poster-exhibition scene. Several interesting patterns are extracted by the method, and we examined their interpretations.</div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088510&CFID=105751022&CFTOKEN=32053859">A study of manual gesture-based selection for the PEMMI multimodal transport management interface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100107483&CFID=105751022&CFTOKEN=32053859">Fang Chen</a>, 
                        <a href="author_page.cfm?id=81317492457&CFID=105751022&CFTOKEN=32053859">Eric Choi</a>, 
                        <a href="author_page.cfm?id=81339498387&CFID=105751022&CFTOKEN=32053859">Julien Epps</a>, 
                        <a href="author_page.cfm?id=81344493991&CFID=105751022&CFTOKEN=32053859">Serge Lichman</a>, 
                        <a href="author_page.cfm?id=81341496177&CFID=105751022&CFTOKEN=32053859">Natalie Ruiz</a>, 
                        <a href="author_page.cfm?id=81324493592&CFID=105751022&CFTOKEN=32053859">Yu Shi</a>, 
                        <a href="author_page.cfm?id=81309495249&CFID=105751022&CFTOKEN=32053859">Ronnie Taib</a>, 
                        <a href="author_page.cfm?id=81100657783&CFID=105751022&CFTOKEN=32053859">Mike Wu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 274 - 281</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088510" title="DOI">10.1145/1088463.1088510</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088510&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow47" style="display:inline;"><br /><div style="display:inline">Operators of traffic control rooms are often required to quickly respond to critical incidents using a complex array of multiple keyboards, mice, very large screen monitors and other peripheral equipment. To support the aim of finding more natural interfaces ...</div></span>
          <span id="toHide47" style="display:none;"><br /><div style="display:inline">Operators of traffic control rooms are often required to quickly respond to critical incidents using a complex array of multiple keyboards, mice, very large screen monitors and other peripheral equipment. To support the aim of finding more natural interfaces for this challenging application, this paper presents PEMMI (Perceptually Effective Multimodal Interface), a transport management system control prototype taking video-based manual gesture and speech recognition as inputs. A specific theme within this research is determining the optimum strategy for gesture input in terms of both single-point input selection and suitable multimodal feedback for selection. It has been found that users tend to prefer larger selection areas for targets in gesture interfaces, and tend to select within 44% of this selection radius. The minimum effective size for targets when using 'device-free' gesture interfaces was found to be 80 pixels (on a 1280x1024 screen). This paper also shows that feedback on gesture input via large screens is enhanced by the use of both audio and visual cues to guide the user's multimodal input. Audio feedback in particular was found to improve user response time by an average of 20% over existing gesture selection strategies for multimodal tasks.</div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088511&CFID=105751022&CFTOKEN=32053859">Recognition of sign language subwords based on boosted hidden Markov models</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81452601980&CFID=105751022&CFTOKEN=32053859">Liang-Guo Zhang</a>, 
                        <a href="author_page.cfm?id=81371590726&CFID=105751022&CFTOKEN=32053859">Xilin Chen</a>, 
                        <a href="author_page.cfm?id=81451596938&CFID=105751022&CFTOKEN=32053859">Chunli Wang</a>, 
                        <a href="author_page.cfm?id=81100127466&CFID=105751022&CFTOKEN=32053859">Yiqiang Chen</a>, 
                        <a href="author_page.cfm?id=81100131951&CFID=105751022&CFTOKEN=32053859">Wen Gao</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 282 - 287</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088511" title="DOI">10.1145/1088463.1088511</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088511&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow48" style="display:inline;"><br /><div style="display:inline">Sign language recognition (SLR) plays an important role in human-computer interaction (HCI), especially for the convenient communication between deaf and hearing society. How to enhance the traditional hidden Markov models (HMM) based SLR is an important ...</div></span>
          <span id="toHide48" style="display:none;"><br /><div style="display:inline">Sign language recognition (SLR) plays an important role in human-computer interaction (HCI), especially for the convenient communication between deaf and hearing society. How to enhance the traditional hidden Markov models (HMM) based SLR is an important issue in the SLR community. And how to refine the boundaries of the classifiers to effectively characterize the property of spread-out of the training samples is another significant issue. In this paper, a new classification framework applying adaptive boosting (AdaBoost) strategy to continuous HMM (CHMM) training procedure at the subwords classification level for SLR is presented. The ensemble of multiple composite CHMMs for each subword trained in boosting iterations tends to concentrate more on the hard-to-classify samples so as to generate more complex decision boundary than that of the single HMM classifier. Experimental results on the vocabulary of frequently used Chinese sign language (CSL) subwords show that the proposed boosted CHMM outperforms the conventional CHMM for SLR.</div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088512&CFID=105751022&CFTOKEN=32053859">Gesture-driven American sign language phraselator</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100298281&CFID=105751022&CFTOKEN=32053859">Jose L. Hernandez-Rebollar</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 288 - 292</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088512" title="DOI">10.1145/1088463.1088512</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088512&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow49" style="display:inline;"><br /><div style="display:inline">This paper describes a portable American Sign Language (ASL)-to-English phraselator. This wearable device is based on an Acceleglove originally developed for recognizing the hand alphabet, and a two-link arm skeleton that detects hand location and movement ...</div></span>
          <span id="toHide49" style="display:none;"><br /><div style="display:inline">This paper describes a portable American Sign Language (ASL)-to-English phraselator. This wearable device is based on an Acceleglove originally developed for recognizing the hand alphabet, and a two-link arm skeleton that detects hand location and movement with respect to the body. Therefore, this phraselator is able to recognize finger-spelled words as well as hand gestures and translate them into spoken voice through a speech synthesizer. To speed-up the recognition process, a simple prediction algorithm has been introduced so the phraselator predicts words based on the current letter being inputted, or complete sentences based on the current sign being translated. The user selects the rest of the sentence (or word) by means of a predefined hand gesture for the phraselator to speak out the sentence in English or Spanish. New words of phrases are automatically added to the lexicon for future predictions.</div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088513&CFID=105751022&CFTOKEN=32053859">Interactive vision to detect target objects for helper robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100310995&CFID=105751022&CFTOKEN=32053859">Altab Hossain</a>, 
                        <a href="author_page.cfm?id=81100364518&CFID=105751022&CFTOKEN=32053859">Rahmadi Kurnia</a>, 
                        <a href="author_page.cfm?id=81100584849&CFID=105751022&CFTOKEN=32053859">Akio Nakamura</a>, 
                        <a href="author_page.cfm?id=81100082332&CFID=105751022&CFTOKEN=32053859">Yoshinori Kuno</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 293 - 300</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088513" title="DOI">10.1145/1088463.1088513</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088513&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow50" style="display:inline;"><br /><div style="display:inline">An effective human-robot interaction is essential for wide penetration of service robots into the market. Such robots need vision systems to recognize objects. It is, however, difficult to realize vision systems that can work in various conditions. More ...</div></span>
          <span id="toHide50" style="display:none;"><br /><div style="display:inline">An effective human-robot interaction is essential for wide penetration of service robots into the market. Such robots need vision systems to recognize objects. It is, however, difficult to realize vision systems that can work in various conditions. More robust techniques of object recognition and image segmentation are essential. Thus, we have proposed to use the human user's assistance for object recognition through speech. The robot asks a question to which the user can easily answer and whose answer can efficiently reduce the number of candidate objects even if there are occluded objects and/or objects composed of multicolor parts in the scene. It considers the characteristics of features used for object recognition such as the easiness for humans to specify them by word, thus generating a user-friendly and efficient sequence of questions. Experimental results show that the robot can detect target objects by asking the questions generated by the method.</div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Tangible interfaces and universal access</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088515&CFID=105751022&CFTOKEN=32053859">The contrastive evaluation of unimodal and multimodal interfaces for voice otput communication aids</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100649832&CFID=105751022&CFTOKEN=32053859">Melanie Baljko</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 301 - 308</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088515" title="DOI">10.1145/1088463.1088515</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088515&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow52" style="display:inline;"><br /><div style="display:inline">For computational Augmentative and Alternative Communication (AAC) aids, it has often been asserted that multimodal interfaces have benefits over unimodal ones. Several such benefits have been described informally, but, to date, few have actually been ...</div></span>
          <span id="toHide52" style="display:none;"><br /><div style="display:inline">For computational Augmentative and Alternative Communication (AAC) aids, it has often been asserted that multimodal interfaces have benefits over unimodal ones. Several such benefits have been described informally, but, to date, few have actually been formalized or quantified. In this paper, some of the special considerations of this application domain are described. Next, the hypothesized benefits of <i>semantically nonredundant multimodal input actions</i> over unimodal input actions are described formally. The notion of <i>information rate</i>, already well established as a dependent variable in evaluations of AAC devices, is quantified in this paper, using the formalisms provided by Information Theory (as opposed to other, idiosyncratic approaches that have been employed previously). A comparative analysis was performed between interfaces that afford unimodal input actions and those that afford <i>semantically nonredundant multimodal</i> input actions. This analysis permitted generalized conclusions, which have been synthesized with those of another, recently-completed analysis in which unimodal and <i>semantically redundant</i> multimodal input actions were compared. A reinterpretation of Keates and Robinson's empirical data (1998) shows that their criticism of multimodal interfaces for AAC devices, in part, was unfounded.</div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088516&CFID=105751022&CFTOKEN=32053859">Agent-based architecture for implementing multimodal learning environments for visually impaired children</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100502884&CFID=105751022&CFTOKEN=32053859">Rami Saarinen</a>, 
                        <a href="author_page.cfm?id=81100614150&CFID=105751022&CFTOKEN=32053859">Janne J&#228;rvi</a>, 
                        <a href="author_page.cfm?id=81100035638&CFID=105751022&CFTOKEN=32053859">Roope Raisamo</a>, 
                        <a href="author_page.cfm?id=81100218156&CFID=105751022&CFTOKEN=32053859">Jouni Salo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 309 - 316</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088516" title="DOI">10.1145/1088463.1088516</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088516&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow53" style="display:inline;"><br /><div style="display:inline">Visually impaired children have a great disadvantage in the modern society since their ability to use modern computer technology is limited due to inappropriate user interfaces. The aim of the work presented in this paper was to develop a multimodal ...</div></span>
          <span id="toHide53" style="display:none;"><br /><div style="display:inline">Visually impaired children have a great disadvantage in the modern society since their ability to use modern computer technology is limited due to inappropriate user interfaces. The aim of the work presented in this paper was to develop a multimodal software architecture and applications to support visually impaired children and to enable them to interact equally with sighted children in learning situations. The architecture is based on software agents, and has specific support for visual, auditory and haptic interaction. It has been used successfully with different groups of 7-8-year-old and 12-year-old visually impaired children. In this paper we focus on the enabling software technology and interaction techniques aimed to realize our goal.</div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088517&CFID=105751022&CFTOKEN=32053859">Perceiving ordinal data haptically under workload</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100640680&CFID=105751022&CFTOKEN=32053859">Anthony Tang</a>, 
                        <a href="author_page.cfm?id=81100654521&CFID=105751022&CFTOKEN=32053859">Peter McLachlan</a>, 
                        <a href="author_page.cfm?id=81100637461&CFID=105751022&CFTOKEN=32053859">Karen Lowe</a>, 
                        <a href="author_page.cfm?id=81100513358&CFID=105751022&CFTOKEN=32053859">Chalapati Rao Saka</a>, 
                        <a href="author_page.cfm?id=81100608981&CFID=105751022&CFTOKEN=32053859">Karon MacLean</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 317 - 324</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088517" title="DOI">10.1145/1088463.1088517</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088517&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow54" style="display:inline;"><br /><div style="display:inline">Visual information overload is a threat to the interpretation of displays presenting large data sets or complex application environments. To combat this problem, researchers have begun to explore how haptic feedback can be used as another means for information ...</div></span>
          <span id="toHide54" style="display:none;"><br /><div style="display:inline">Visual information overload is a threat to the interpretation of displays presenting large data sets or complex application environments. To combat this problem, researchers have begun to explore how haptic feedback can be used as another means for information transmission. In this paper, we show that people can perceive and accurately process haptically rendered ordinal data while under cognitive workload. We evaluate three haptic models for rendering ordinal data with participants who were performing a taxing visual tracking task. The evaluation demonstrates that information rendered by these models is perceptually available even when users are visually busy. This preliminary research has promising implications for haptic augmentation of visual displays for information visualization.</div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1088518&CFID=105751022&CFTOKEN=32053859">Virtual tangible widgets: seamless universal interaction with personal sensing devices</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100283498&CFID=105751022&CFTOKEN=32053859">Eiji Tokunaga</a>, 
                        <a href="author_page.cfm?id=81339509881&CFID=105751022&CFTOKEN=32053859">Hiroaki Kimura</a>, 
                        <a href="author_page.cfm?id=81100603920&CFID=105751022&CFTOKEN=32053859">Nobuyuki Kobayashi</a>, 
                        <a href="author_page.cfm?id=81100441392&CFID=105751022&CFTOKEN=32053859">Tatsuo Nakajima</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 325 - 332</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1088463.1088518" title="DOI">10.1145/1088463.1088518</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1088518&type=pdf&CFID=105751022&CFTOKEN=32053859" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow55" style="display:inline;"><br /><div style="display:inline">Using a single personal device as an universal controller for diverse services is a promising approach to solving the problem of too many controllers in ubiquitous multimodal environments. However, the current approaches to universal controllers cannot ...</div></span>
          <span id="toHide55" style="display:none;"><br /><div style="display:inline">Using a single personal device as an universal controller for diverse services is a promising approach to solving the problem of too many controllers in ubiquitous multimodal environments. However, the current approaches to universal controllers cannot provide intuitive control because they are restricted to traditional mobile user interfaces such as small keys or small touch panels. We propose <i>Vidgets</i>, which is short for virtual tangible widgets}, as an approach to selecting and controlling ubiquitous services with virtually implemented tangible user interfaces based on a single sensing personal device equipped with a digital camera and several physical sensors. We classify the use of the universal controller into the three stages: (a) searching for a service, (b) grasping the service and (c) using the service. User studies with our prototype implementation indicate that the smooth transition and integration of the three stages improve the overall interaction with our universal controller.</div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
          </div>
		  </td>
          </tr>          
          
</table>


</div> 
</div>


 <p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>



 <br />
<div class="footerbody" align="center" >
	

	The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2012 ACM, Inc.<br />
	<a href="http://www.acm.org/publications/policies/usage">Terms of Usage</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;	  
	<a href="http://www.acm.org/about/contact-us">Contact Us</a>

<br /><br />
Useful downloads: 
<a href="http://www.adobe.com/products/acrobat/readstep2.html"><img src="http://dl.acm.org/images/pdf_logo.gif" width="16" height="16" alt="" border="0" /> Adobe Acrobat</a>
&nbsp;&nbsp;
<a href="http://www.apple.com/quicktime/download/" target="_blank"><img src="http://dl.acm.org/images/qtlogo.gif" width="16" height="16" alt="" border="0" /> QuickTime</a>
&nbsp;&nbsp;
<a href="http://www.microsoft.com/windows/windowsmedia/download/default.asp" target="_blank"><img src="http://dl.acm.org/images/wmv.gif" width="16" height="15" alt="" border="0" /> Windows Media Player</a>
&nbsp;&nbsp;
<a href="http://www.real.com/" target="_blank"><img src="http://dl.acm.org/images/realplayer.gif" width="20" height="18" alt="" border="0" /> Real Player</a>

</div> 



<div  id="cf_window1338241302943" class="yuiextdlg">
	
	<div  id="theguide_title" class="x-dlg-hd">
		The ACM Guide to Computing Literature
	 </div>
	<div  id="theguide_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241302946" class="yuiextdlg">
	
	<div  id="thetags_title" class="x-dlg-hd">
		All Tags
	 </div>
	<div  id="thetags_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241302949" class="yuiextdlg">
	
	<div  id="theformats_title" class="x-dlg-hd">
		Export Formats
	 </div>
	<div  id="theformats_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241302951" class="yuiextdlg">
	
	<div  id="theexplaination_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theexplaination_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241302953" class="yuiextdlg">
	
	<div  id="theservices_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theservices_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338241302955" class="yuiextdlg">
	
	<div  id="savetobinder_title" class="x-dlg-hd">
		Save to Binder
	 </div>
	<div  id="savetobinder_body" class="x-dlg-bd">
		
		
	 </div>
 </div> 

</body>
</html>