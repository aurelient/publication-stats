


<!doctype html>


<head><script type="text/javascript">_cf_loadingtexthtml="<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";
_cf_jsonprefix='//';
_cf_clientid='F364F1B9113A5DEDFDDF3734853AFA49';</script><script type="text/javascript" src="/CFIDE/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfform.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/masks.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/FCKeditor/fckeditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/adapter/yui/ext-yui-adapter.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/ext-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/resizable.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dragdrop/dragdrop.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/util.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/build/state/State-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/widget-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dialog/dialogs.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/cf/cf.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />



<title>Proceedings of the 6th international conference on Human-robot interaction</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
	
  --></style>
 


<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>


<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  function setDiv() {
	var m = document.getElementById('divmain');
	var mh = m.offsetHeight;
	var t = document.getElementById('divtools');
	var th = t.offsetHeight;
	var tg = document.getElementById('divtags');
	var tgh = tg.offsetHeight;
	var calcheight = mh - th;
	if (tgh > calcheight  ){
	  var x = (th + tgh) - mh;
	  if ( (th + tgh) - mh < 65) {
	  }
	  else {
		 document.getElementById('divtags').innerHTML = ""; 
		 var tg = document.getElementById('divtags');
		 var tgh = tg.offsetHeight;
		 tg.style.height = tgh  + 'px';
	  }
	}
	else {
		tg.style.height = calcheight + 'px';
		document.getElementById('divtags').innerHTML = "";
	}

//  do I need to check after I resize to be sure I didn't go too big?
//	var tg2 = document.getElementById('divtags');
//	var tgh2 = tg.offsetHeight;	
//	if (tgh2 > mh + 65) {
//	  var y = mh + 65;
//	  alert('expanded too much ' + tgh2 + ' should be at most ' + y);
//	  tg.style.height = y + 'px';
//	  document.getElementById('divtags').innerHTML = "";
//	}

  }
</script>

<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>



<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Billard, Aude; General Chair-Kahn, Peter; Program Chair-Adams, Julie A.; Program Chair-Trafton, Greg"> <meta name="citation_title" content="Proceedings of the 6th international conference on Human-robot interaction"> <meta name="citation_date" content="03/06/2011"> <meta name="citation_isbn" content="978-1-4503-0561-7"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1957656"> 



<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFFORM');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFDIV');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFTEXTAREA');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFWINDOW');
</script>

<script type="text/javascript">
	var _cf_window_init_1338242483147=function()
	{
		_cf_bind_init_1338242483148=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide_body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242483148);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window1338242483146', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242483147);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242483150=function()
	{
		_cf_bind_init_1338242483151=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags_body','bindExpr':['showthetags.cfm?id=1957656']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242483151);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1957656',{ modal:false, closable:true, divid:'cf_window1338242483149', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242483150);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242483153=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window1338242483152', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242483153);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242483155=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window1338242483154', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242483155);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242483157=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window1338242483156', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242483157);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242483159=function()
	{
		_cf_bind_init_1338242483160=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder_body','bindExpr':['savetobinder.cfm?id=1957656']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242483160);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1957656',{ modal:false, closable:true, divid:'cf_window1338242483158', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242483159);
</script>
</head>

<body style="text-align:center" onLoad="window.focus();">

<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'http://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
                        
<script src='AC_RunActiveContent.js' type="text/javascript"></script>




<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>



<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
	
    <tr style="vertical-align:top">
		
		<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text"  ><img src="http://dl.acm.org/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
		</td>
        
        <td style="padding-left: 5px; padding-right:10px; padding-bottom:0px;" class="small-link-text">
        	<table style="width:100%; border-collapse:collapse; padding:0px">
			<tr><td style="text-align:center">
				
                            <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
                    
					</td>		
			</tr>
			</table> 
        </td>
		<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text">
			 <p style="margin-top:0px; margin-bottom:10px;">
					
                            <a href="https://dl.acm.org/signin.cfm?cfid=105753156&amp;cftoken=18642945" class="small-link-text" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
                            &nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm?cfid=105753156&amp;cftoken=18642945"  class="small-link-text" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
						
			 </p>
            
			<table style="padding: 5px; border-collapse:collapse; float:right">
				
				
                            
                            <tr>
                            <td class="small-link-text" style="text-align:right">
                            <form name="qiksearch" action="results.cfm?h=1&amp;cfid=105753156&amp;cftoken=18642945" method="post">
                           
                           
                            
                                     
                                    <span style="margin-left:0px"><label><input type="text" name="query" size="34" value=" " /></label>&nbsp;
                                    <input style="vertical-align:top;" type="image" alt="Search" name="Go" src="http://dl.acm.org/images/search_small.jpg" />
                                    
                                    </span>
							  </form>
                                </td>
                            </tr>
                          
				  
			</table>

			
			
		</td>

	</tr>
    
    
    <tr><td colspan="3" class="small-link-text" style="padding-bottom:5px; padding-top:0px; text-align:center">
		<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
         
         </td>
    </tr>
    </table>
	
<map name="port" id="port" > 
  <area shape="rect" coords="1,1,60,50" href="http://www.acm.org/" alt="ACM Home Page" />
  <area shape="rect" coords="65,1,275,68" href="http://dl.acm.org/dl.cfm?CFID=105753156&CFTOKEN=18642945" alt="ACM Digital Library Home Page" />
</map>

<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
	<tr style="vertical-align:top">
		<td style="padding-right:10px; text-align:left" class="small-link-text">
        	<div id="divmain" style="border:1px solid #356b20;">
				 
				<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
					
                    	<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;"><strong>Proceedings of the 6th international conference on Human-robot interaction</strong></h1>
                        
                </div>
                
                  

<table class="medium-text" style="border-collapse:collapse; padding:0px;">

<col style="width:540px" />

<tr style="vertical-align:top">
  <td>
    <table style="border-collapse:collapse; padding:2px;" class="medium-text">
      <col style="width:80px;" />
      <col style="width:auto" />
      <tr style="vertical-align:top">
        
      </tr>
    </table>

	
        <table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
            <col style="width:80px" />
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             General Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100342762&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105753156&amp;cftoken=18642945" title="Author Profile Page" target="_self">Aude Billard</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1022648&CFID=105753156&CFTOKEN=18642945" title="Institutional Profile Page"><small>EPFL, Switzerland</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81408592441&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105753156&amp;cftoken=18642945" title="Author Profile Page" target="_self">Peter Kahn</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1031804&CFID=105753156&CFTOKEN=18642945" title="Institutional Profile Page"><small>University of Washington, USA</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             Program Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100313646&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105753156&amp;cftoken=18642945" title="Author Profile Page" target="_self">Julie A. Adams</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1031836&CFID=105753156&CFTOKEN=18642945" title="Institutional Profile Page"><small>Vanderbilt University, USA</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81414599084&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105753156&amp;cftoken=18642945" title="Author Profile Page" target="_self">Greg Trafton</a>
                
            </td>
            <td valign="bottom">
                
                        <small>US Naval Research Laboratories, USA</small>
                    	
            </td>
            </tr>
            
        </table>
    

  </td>

  <td rowspan="20" nowrap="nowrap">
	<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
		<tr>
        	<td align="center" style="padding-bottom: 5px;">
			  
               <img src="http://portalparts.acm.org/1960000/1957656/thumb/cover_thumb.jpg" title="Proceedings of the 6th international conference on Human-robot interaction" height="100"  width="77" ALT="Proceedings of the 6th international conference on Human-robot interaction" /> 
              </td>
              <td valign="top" align="left" nowrap="nowrap">
	             <img src="images/acm_mini.jpg" title="Published by ACM" alt="Published by ACM" /> 2011 Proceeding<br />
                 
        	 </td>
        </tr>
        
        <tr>
        	<td colspan="2" valign="baseline" style="padding-bottom:5px;">
            <img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
            <a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
            </td>
         </tr>
         <tr>
            <td  class="small-text" colspan="2" valign="top" style="padding-left:30px;">
				
	                    	&middot;&nbsp;Downloads (6 Weeks): 851<br />
    	                    &middot;&nbsp;Downloads (12 Months): 10,682<br />
                          
                        &middot;&nbsp;Citation Count: 30 
			</td>
         </tr>

	</table>
  </td>
</tr>
</table>

<br clear="all" />

<table border="0" class="medium-text" cellpadding="2" cellspacing="0">

<tr valign="top">
    <td colspan="3"><table border="0" class="medium-text" cellpadding="1" cellspacing="0">

<tr valign="top">
    <td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>

	<tr valign="top">
    	<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
	</tr>
    <tr valign="top">
	    <td style="padding-left:10px;">
		   <a href="http://hri2011.net" title="Conference Website"  target="_self" class="link-text">HRI'11</a> International Conference on Human-Robot Interaction 
        </td>
	</tr>
    
    <tr valign="top">
	    <td style="padding-left:10px; padding-bottom:10px"> Lausanne, Switzerland &mdash; March 06 - 09, 2011
                    
                  <br />
                    
                  <a href="http://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text">  &copy;2011</span> 
                  <br />           
                  
      </td>
	</tr>
	 

</table></td>
</tr>

</table>

                  
                 <br clear="all" />
			</div>
			
		</td>
		<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
	
            <div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
              <div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;"><strong>Tools and Resources</strong></h1></div>


<ul title="Tools and Resources" style="list-style: none; list-style-position:inside;
margin-left: 0px;
padding-left: 0em;
text-indent: 5px;
margin-bottom: 0px;">


<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:6px;">
   <span class="small-link-text">TOC Service:</span>
   	  
	  <img src="http://dl.acm.org/images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
      <ul style="margin-left: 0; padding-left: 0; display:inline;">
      	
        <li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
        <li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');"  class="small-link-text">RSS</a></li>
		        
      </ul>
    </span>
</li>

        <li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:6px;">
        <a href="citation.cfm?id=1957656&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
         </span></li>
    




<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Export Formats:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1957656&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1957656&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1957656&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
      </ul>
    </span>
</li>



 
   <li style="list-style-image:url(img/calbullet.jpg);margin-top:15px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Upcoming Conference:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px; margin-left:25px;"><a href="http://humanrobotinteraction.org/2013/" title="ACM/IEEE International Conference on Human-Robot Interaction" class="small-link-text">HRI'13</a></li>
      </ul>
    </span>
	</li>
    


</ul>           

  <!-- ADDTHIS BUTTON BEGIN -->
  
  <!-- ADDTHIS BUTTON END -->

<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>
  <!-- AddThis Button BEGIN -->



<!-- AddThis Button BEGIN -->
<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_slashdot"></a>
<a class="addthis_button_reddit"></a>


<span class="addthis_separator">|</span>
<a href="http://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>
<!-- AddThis Button END -->

  
 

  
  
            </div>
            
		</td>
	</tr>
    
</table>



</div>


<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">




<div id="fback" style="text-align:left; padding-top:10px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="feedback" border="0" /></a>
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design"><strong>Feedback</strong></a>

<span style="padding:10px;">|</span>




<span>Switch to <a href="citation.cfm?id=1957656&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>


</span>

 
<div class="small-text" style="margin-top:10px; margin-bottom:5px;"> 
<br />

    <a href="#abstract"  title="Abstract" style="padding:5px"><span>Abstract</span></a> |
    
    <a href="#formats"  title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
    
    <a href="#authors"  title="Authors" style="padding:5px"><span>Authors</span></a> |
    <a href="#references"  title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
    <a href="#citedby"  title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
    <a href="#indexterms"  title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
    <a href="#source"  title="Publication" style="padding:5px"><span>Publication</span></a> |
    <a href="#revs"  title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |               
	<a href="#comments"  title="Comments" style="padding:5px"><span>Comments</span></a>
	
     |               
	<a href="#prox"  title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
    
</div>
    
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;"/>



</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
       	
			<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

			

		
			
           
			
				
				<p>
					<div style="display:inline"><p>It is our great pleasure to welcome you to the 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI 2011). HRI is a single-track, highly selective annual conference that showcases the very best research and thinking in human-robot interaction. HRI is inherently interdisciplinary and multidisciplinary, reflecting work from researchers in robotics, psychology, cognitive science, HCI, human factors, linguistics, artificial intelligence, organizational behavior, and anthropology.</p> <p>The theme of HRI 2011 is "Real World HRI." The theme is intended to highlight HRI in which basic scientific research is further tested in real world settings or applied to questions that arise in real world settings. One central aspect of this type of research, in contrast to other realms of applied research, is that it is theoretically driven and feeds back to our theoretical understandings. As such, real world research fortifies our understanding of people, robots, and interaction between the two. This year's conference seeks to take up grand challenges of deploying real world human-robot systems.</p> <p>This year we have three keynote speakers. They will discuss their work on gesture (Sotaro Kita), biologically inspired computational vision (Randy O'Reilly), and cognitive robotics (Angelo Cangelosi). We also have a panel to highlight the conference theme: HRI in the real world. This panel brings together leaders from business and industrial robotics that are relying on current robotic technology to accomplish work in the world today.</p> <p>The call for papers attracted 149 full paper submissions (eight page papers) from Asia, Europe, the Middle East, and North America. The program committee conducted a rigorous review process for full papers, accepting 33 full papers for oral presentation and publication in the proceedings. This year, taking advantage of having both ACM and IEEE as the sponsor, all papers are archived in both the ACM Digital Library and IEEE Xplore.</p> <p>Furthermore, 123 late-breaking reports (two page brief papers) were screened for relevance and lightly reviewed; 99 were accepted for presentation at the HRI conference as posters, exposing a broader perspective of solutions, challenges and issues in HRI. They will be made available in the IEEE Xplore as well as the ACM Digital Library. Finally, a total of 18 videos (out of 36 submissions) were accepted based on importance, novelty and entertainment value. Four videos will be shown throughout the conference and the remaining videos will be shown in a special video session.</p> <p>Presented papers describe novel interaction techniques, the design of new robots, experimental evaluations of people and robots, and robots in real-world settings.</p> <p>This year the local hosts will provide three research laboratory tours during the lunch breaks. We hope that visitors enjoy the opportunity to experience the research ideas of the local hosts.</p></div>
				</p>
   				
           	</div>
			
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


  <div class="abstract">
        <SPAN><strong>FRONT MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1960000/1957656/fm/frontmatter.pdf?ip=188.194.239.219&CFID=105753156&CFTOKEN=18642945" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(cd label, copyright, welcome, contents, organization, reviewers, sponsors) 
  </div>          
  
  <div style="margin-top: 10px;"  class="abstract">
        <SPAN><strong>BACK MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1960000/1957656/bm/backmatter.pdf?ip=188.194.239.219&CFID=105753156&CFTOKEN=18642945" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(author index) 
  </div>          
  
<div style="margin-top: 10px; height: auto; padding: 5px; ">
		
		
        


	</div>

<br clear="all" />
</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<dl title="Authors" style="margin-top:0px">




<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 General Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Aude Billard" href="author_page.cfm?id=81100342762&CFID=105753156&CFTOKEN=18642945">Aude Billard</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1997-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">34</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">140</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">12</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">58</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">459</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Aude Billard" href="author_page.cfm?id=81100342762&amp;dsp=coll&amp;trk=1&amp;CFID=105753156&CFTOKEN=18642945" target="_self">View colleagues</a> of Aude Billard
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Peter Kahn" href="author_page.cfm?id=81408592441&CFID=105753156&CFTOKEN=18642945">Peter Kahn</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1992-2011</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">20</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">152</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">14</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">110</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">801</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Peter Kahn" href="author_page.cfm?id=81408592441&amp;dsp=coll&amp;trk=1&amp;CFID=105753156&CFTOKEN=18642945" target="_self">View colleagues</a> of Peter Kahn
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              



<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 Program Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Julie A. Adams" href="author_page.cfm?id=81100313646&CFID=105753156&CFTOKEN=18642945">Julie A. Adams</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1995-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">21</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">37</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">12</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">50</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">415</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Julie A. Adams" href="author_page.cfm?id=81100313646&amp;dsp=coll&amp;trk=1&amp;CFID=105753156&CFTOKEN=18642945" target="_self">View colleagues</a> of Julie A. Adams
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Greg Trafton" href="author_page.cfm?id=81414599084&CFID=105753156&CFTOKEN=18642945">Greg Trafton</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">2010-2010</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">1</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">0</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">1</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">8</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">31</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Greg Trafton" href="author_page.cfm?id=81414599084&amp;dsp=coll&amp;trk=1&amp;CFID=105753156&CFTOKEN=18642945" target="_self">View colleagues</a> of Greg Trafton
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              

</dl>
</div>

		  
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	References are not available

</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	Citings are not available
		
 </div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

Index Terms are not available


</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">



<table border="0" class="medium-text" cellpadding="0" cellspacing="5">



    <tr valign="top">
    	<td>Title</td> 
	    <td>
		   <a href="http://hri2011.net" title="Conference Website"  target="_self" class="link-text">HRI'11</a> International Conference on Human-Robot Interaction 
        </td>
	</tr>
    <tr><td></td><td>Lausanne, Switzerland &mdash; March 06 - 09, 2011</td></tr> <tr><td>Pages</td><td>506</td></tr> 
                 <tr>
                 
                     <td>Sponsors</td>
                    
                  <td>
                  <a href="sig.cfm?id=SP918&CFID=105753156&CFTOKEN=18642945"> SIGART</a> ACM Special Interest Group on Artificial Intelligence
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="sig.cfm?id=SP923&CFID=105753156&CFTOKEN=18642945"> SIGCHI</a> ACM Special Interest Group on Computer-Human Interaction
                  </td>
                  </tr>
              
                 <tr>
                 
                     <td>In-Cooperations</td>
                    
                  <td>
                  <a name="sponsor"> Human Factors &amp; Ergonomics Soc</a> Human Factors &amp; Ergonomics Soc
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a name="sponsor"> RA</a> IEEE Robotics and Automation Society
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a name="sponsor"> IEEE Systems, Man and Cybernetics Society</a>
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a name="sponsor"> The Association for the Advancement of Artificial Intelligence (AAAI)</a>
                  </td>
                  </tr>
              
                  <tr><td>Publisher</td><td><a href="http://www.acm.org/publications">ACM</a> New York, NY, USA</td>
				  </tr>
             <tr><td>ISBN</td><td> 978-1-4503-0561-7</td></tr> <tr><td>Order Number</td><td>609114</td></tr> 
			<tr valign="top">
        	<td>Conference</td>
            <td valign="top" align="left"  style="padding-bottom: 25px;">
	            <strong style="padding-right:10px">HRI</strong><a href="event.cfm?id=RE285&CFID=105753156&CFTOKEN=18642945" title="ACM/IEEE International Conference on Human-Robot Interaction">ACM/IEEE International Conference on Human-Robot Interaction</a>
                
                       
                        <a href="event.cfm?id=RE285&CFID=105753156&CFTOKEN=18642945" title="ACM/IEEE International Conference on Human-Robot Interaction"><img border="0" src="http://portalparts.acm.org/event_logos/677/677.jpg" title="HRI logo" height="62"  width="100" ALT="HRI logo" style="vertical-align:top"></a>
						 

        	 </td>
            </tr>
		    <tr><td colspan="2">Paper Acceptance Rate 33 of 149 submissions, 22%</td></tr> <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 227 of 905 submissions, 25%</td></tr>
                       <tr valign="top">
                        <td colspan="2" style="padding-left:25px;">
                        	<table>
                            	<tr><td>
                                        <!-- WebCharts3D v5.1(2077) -->
<IMG SRC="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=Images/7140434620525717.JPG" id="Images_7140434620525717_JPG" name="Images_7140434620525717_JPG" usemap="#Images_7140434620525717_JPG_map" border="0"/>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAB' id='GP1338242483512AAAB'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>140</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAC' id='GP1338242483512AAAC'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>41</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAD' id='GP1338242483512AAAD'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>101</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAE' id='GP1338242483512AAAE'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>22</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAF' id='GP1338242483512AAAF'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>134</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAG' id='GP1338242483512AAAG'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>48</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAH' id='GP1338242483512AAAH'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>120</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAI' id='GP1338242483512AAAI'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>23</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAJ' id='GP1338242483512AAAJ'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>124</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAK' id='GP1338242483512AAAK'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>26</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAL' id='GP1338242483512AAAL'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>149</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAM' id='GP1338242483512AAAM'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>33</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAN' id='GP1338242483512AAAN'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>137</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242483512AAAO' id='GP1338242483512AAAO'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>34</td></tr></table>
<MAP name='Images_7140434620525717_JPG_map'>
<AREA shape='rect' coords='0,0,1,1'/>
<AREA shape="rect" coords="293,179,309,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAO",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAO",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAO",event)'/>
<AREA shape="rect" coords="277,74,293,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAN",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAN",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAN",event)'/>
<AREA shape="rect" coords="253,180,269,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAM",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAM",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAM",event)'/>
<AREA shape="rect" coords="237,62,253,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAL",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAL",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAL",event)'/>
<AREA shape="rect" coords="213,187,229,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAK",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAK",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAK",event)'/>
<AREA shape="rect" coords="197,87,213,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAJ",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAJ",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAJ",event)'/>
<AREA shape="rect" coords="173,190,189,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAI",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAI",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAI",event)'/>
<AREA shape="rect" coords="157,91,173,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAH",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAH",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAH",event)'/>
<AREA shape="rect" coords="133,165,149,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAG",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAG",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAG",event)'/>
<AREA shape="rect" coords="117,77,133,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAF",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAF",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAF",event)'/>
<AREA shape="rect" coords="93,191,109,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAE",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAE",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAE",event)'/>
<AREA shape="rect" coords="77,111,93,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAD",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAD",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAD",event)'/>
<AREA shape="rect" coords="53,172,69,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAC",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAC",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAC",event)'/>
<AREA shape="rect" coords="37,71,53,213" onMouseover='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAB",event,true)' onMouseout='xx_set_visible("Images_7140434620525717_JPG","GP1338242483512AAAB",event,false)' onMousemove='xx_move_tag("Images_7140434620525717_JPG","GP1338242483512AAAB",event)'/>
<AREA shape="rect" coords="160,13,227,27"/>
<AREA shape="rect" coords="89,13,160,27"/>
</MAP>

<script language="javascript" src="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=script.js"></script>

                                      </td>
                                      <td style="padding-left:20px;">
                                             <table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
                                                <tr bgcolor="#ffffff">
                                                  <th style="width:50%">Year</th>
                                                  <th  align="right" style="width:15%">Submitted</th>
                                                  <th  align="right" style="width:15%">Accepted</th>
                                                  <th  align="center">Rate</th>
                                                </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '06</td>
                                                            <td align="right">140</td>
                                                            <td align="right">41</td>
                                                            <td align="center">29%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '07</td>
                                                            <td align="right">101</td>
                                                            <td align="right">22</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '08</td>
                                                            <td align="right">134</td>
                                                            <td align="right">48</td>
                                                            <td align="center">36%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '09</td>
                                                            <td align="right">120</td>
                                                            <td align="right">23</td>
                                                            <td align="center">19%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '10</td>
                                                            <td align="right">124</td>
                                                            <td align="right">26</td>
                                                            <td align="center">21%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '11</td>
                                                            <td align="right">149</td>
                                                            <td align="right">33</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '12</td>
                                                            <td align="right">137</td>
                                                            <td align="right">34</td>
                                                            <td align="center">25%</td>
                                                         </tr>
                                                
                                                 <tr bgcolor="#ffffff">
                                                    <td><strong>Overall</strong></td>
                                                    <td align="right">905</td>
                                                    <td align="right">227</td>
                                                    <td align="center">25%</td>
                                                  </tr>
                                                </table>
                                       </td>
                                     </tr>
                               </table>
                        </td>
                    </tr>
                     
                     
            
</table>


</table>




</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	<br />Reviews are not available for this item
        
        <div align="left" style="margin-top:30px">
					<a title="Computing Reviews" href="ocr_review_main.cfm?CFID=105753156&CFTOKEN=18642945">
                 <img src="http://dl.acm.org/images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
        
        
       		<ul style="list-style:disc; display:inline-block">
	            <li>Access <a href="ocr_review_main.cfm?CFID=105753156&CFTOKEN=18642945" target="_blank">critical reviews</a> of computing literature.</li>
            	<li><a href="http://www.computingreviews.com/Reviewer/"  target="_blank">Become a reviewer</a> for Computing Reviews</li>
            </ul>
        </div>
        
</div>



<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
         
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<div>
<div>
	
	<p style="margin-left:5px;">
    <strong>Be the first to comment</strong>
    	
          	To Post a comment please <a href="signin.cfm?CFID=105753156&CFTOKEN=18642945">sign in or create</a> a free Web account</a>
        
    
    
	 </p>
	   	
 
</div>


</div>

	
		<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
        
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" >

<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 6th international conference on Human-robot interaction</h5>


<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>

<div style="clear:both">
    
        <div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1734454&picked=prox&CFID=105753156&CFTOKEN=18642945" title="previous: HRI '10"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=2157689&picked=prox&CFID=105753156&CFTOKEN=18642945" title="Next: HRI '12">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
        
</div>


 
<table class="text12" border="0">

  <tr>
  <td></td>
  <td colspan="2">TUTORIAL SESSION: <strong>Tutorials &#38; workshops</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957658&CFID=105753156&CFTOKEN=18642945">Tutorial: brain mediated human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100335472&CFID=105753156&CFTOKEN=18642945">Jose del R. Millan</a>, 
                        <a href="author_page.cfm?id=81318493385&CFID=105753156&CFTOKEN=18642945">Ricardo Chavarriaga</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 1-2</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957658" title="DOI">10.1145/1957656.1957658</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957658&ftid=929234&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow2" style="display:inline;"><br /><div style="display:inline">The use of brain-generated signals for human-robot interaction has gained increasing attention in the last years. Indeed brain-controlled robots can potentially be employed to substitute motor capabilities (e.g. brain-controlled prosthetics for amputees ...</div></span>
          <span id="toHide2" style="display:none;"><br /><div style="display:inline"><p>The use of brain-generated signals for human-robot interaction has gained increasing attention in the last years. Indeed brain-controlled robots can potentially be employed to substitute motor capabilities (e.g. brain-controlled prosthetics for amputees or patients with spinal cord injuries); to help in the restoration of such functions (e.g. as a tool for stroke rehabilitation) as well as non-clinical applications like telepresence or entertainment. This half-day tutorial gives an introduction to the field of brain-computer interfaces and presents several design principles required to successfully employ them for robot control.</p></div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957659&CFID=105753156&CFTOKEN=18642945">Robots with children: practices for human-robot symbiosis</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100260147&CFID=105753156&CFTOKEN=18642945">Naomi Miyake</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105753156&CFTOKEN=18642945">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100447769&CFID=105753156&CFTOKEN=18642945">Kerstin Dautenhahn</a>, 
                        <a href="author_page.cfm?id=81336491870&CFID=105753156&CFTOKEN=18642945">Tatsuya Nomura</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 3-4</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957659" title="DOI">10.1145/1957656.1957659</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957659&ftid=929235&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow3" style="display:inline;"><br /><div style="display:inline">On considering symbiosis of humans and robots, its benefits and risks should be taken into account for persons in weaker positions of the society, in particular, children. On the other hand, several robotics applications have been developed, including ...</div></span>
          <span id="toHide3" style="display:none;"><br /><div style="display:inline"><p>On considering symbiosis of humans and robots, its benefits and risks should be taken into account for persons in weaker positions of the society, in particular, children. On the other hand, several robotics applications have been developed, including education and welfare for children. In this stage, it is important that more researchers from interdisciplinary research fields, including robotics, computer science, psychology, sociology, and pedagogy, share an opportunity to discuss about the potential of "robots with children". This half-day workshop aims at providing with the forum where researchers from these interdisciplinary fields discuss about how symbiosis of robots and children should and can be realized, from the perspectives of engineering, psychology, education, and welfare.</p></div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957660&CFID=105753156&CFTOKEN=18642945">Social robotic telepresence</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100038794&CFID=105753156&CFTOKEN=18642945">Silvia Coradeschi</a>, 
                        <a href="author_page.cfm?id=81316489383&CFID=105753156&CFTOKEN=18642945">Amy Loutfi</a>, 
                        <a href="author_page.cfm?id=81482662265&CFID=105753156&CFTOKEN=18642945">Annica Kristoffersson</a>, 
                        <a href="author_page.cfm?id=81100311441&CFID=105753156&CFTOKEN=18642945">Gabriella Cortellessa</a>, 
                        <a href="author_page.cfm?id=81331503418&CFID=105753156&CFTOKEN=18642945">Kerstin Severinson Eklundh</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 5-6</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957660" title="DOI">10.1145/1957656.1957660</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957660&ftid=929236&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow4" style="display:inline;"><br /><div style="display:inline">Robotic telepresence, also known as telerobotics is a subfield of telepresence whose aim is to increase presence via embodiment in a robotic platform. In particular, robotic telepresence can be an effective tool to enhance social interaction suited to ...</div></span>
          <span id="toHide4" style="display:none;"><br /><div style="display:inline"><p>Robotic telepresence, also known as telerobotics is a subfield of telepresence whose aim is to increase presence via embodiment in a robotic platform. In particular, robotic telepresence can be an effective tool to enhance social interaction suited to certain groups of users such as the elderly. The aim of this workshop is to address various aspects important for social robotic telepresence which include but are not limited to, (1) the mechanical design, (2) the user interface design, (3) the interaction between the remotely embodied person and the locally embodied person and (4) the perception of social robotic telepresence systems. Furthermore, we are interested in discovering the added value of spatial presence in the context of social telepresence and comparisons between robotic and non-robotic systems are of interest. We welcome contributions concerning results reached from the above mentioned areas of interest, user evaluation and methodologies, as well as reports from the deployment of social robotic solutions into real world contexts.</p></div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957661&CFID=105753156&CFTOKEN=18642945">The role of expectations in intuitive human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100047384&CFID=105753156&CFTOKEN=18642945">Verena Hafner</a>, 
                        <a href="author_page.cfm?id=81414610931&CFID=105753156&CFTOKEN=18642945">Manja Lohse</a>, 
                        <a href="author_page.cfm?id=81100571957&CFID=105753156&CFTOKEN=18642945">Joachim Meyer</a>, 
                        <a href="author_page.cfm?id=81456631539&CFID=105753156&CFTOKEN=18642945">Yukie Nagai</a>, 
                        <a href="author_page.cfm?id=81100624850&CFID=105753156&CFTOKEN=18642945">Britta Wrede</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 7-8</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957661" title="DOI">10.1145/1957656.1957661</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957661&ftid=929237&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow5" style="display:inline;"><br /><div style="display:inline">Human interaction is highly intuitive: we infer reactions of our opponents mainly from what we have learned in years of experience and often assume that other people have the same knowledge about certain situations, abilities, and expectations as we ...</div></span>
          <span id="toHide5" style="display:none;"><br /><div style="display:inline"><p>Human interaction is highly intuitive: we infer reactions of our opponents mainly from what we have learned in years of experience and often assume that other people have the same knowledge about certain situations, abilities, and expectations as we do. In human-robot interaction (HRI) we cannot take for granted that this is equally true since HRI is asymmetrical. In other words, robots have different abilities, knowledge, and expectations than humans. They need to react appropriately to human expectations and behaviour. With this respect, scientific advances have been made to date for applications in entertainment and service robotics that largely depend on intuitive interaction. However, HRI today is often still unnatural, slow, and unsatisfactory for the human interlocutor. Both the sensorimotor interaction with environment and interlocutor, and the social aspects of the interaction still need to be researched and improved. Therefore, this full-day workshop aims to bring together researchers from different scientific fields to discuss these crosscutting issues and to exchange views on what are the preconditions and principles of intuitive interaction.</p></div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957662&CFID=105753156&CFTOKEN=18642945">HRI pioneers workshop 2011</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100575483&CFID=105753156&CFTOKEN=18642945">Thomas Kollar</a>, 
                        <a href="author_page.cfm?id=81482654534&CFID=105753156&CFTOKEN=18642945">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81482659586&CFID=105753156&CFTOKEN=18642945">Jason Monast</a>, 
                        <a href="author_page.cfm?id=81384590877&CFID=105753156&CFTOKEN=18642945">Anja Austermann</a>, 
                        <a href="author_page.cfm?id=81482645098&CFID=105753156&CFTOKEN=18642945">David Lu</a>, 
                        <a href="author_page.cfm?id=81100489893&CFID=105753156&CFTOKEN=18642945">Mitesh Patel</a>, 
                        <a href="author_page.cfm?id=81350587405&CFID=105753156&CFTOKEN=18642945">Elena Gribovskaya</a>, 
                        <a href="author_page.cfm?id=81482647529&CFID=105753156&CFTOKEN=18642945">Chandan Datta</a>, 
                        <a href="author_page.cfm?id=81388598666&CFID=105753156&CFTOKEN=18642945">Richard Kelley</a>, 
                        <a href="author_page.cfm?id=81414597701&CFID=105753156&CFTOKEN=18642945">Hirotaka Osawa</a>, 
                        <a href="author_page.cfm?id=81456606751&CFID=105753156&CFTOKEN=18642945">Lanny Lin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 9-10</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957662" title="DOI">10.1145/1957656.1957662</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957662&ftid=929238&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow6" style="display:inline;"><br /><div style="display:inline">The 2011 HRI Pioneers Workshop will be conducted in conjunction with the 2011 ACM/IEEE International Conference on Human-Robot Interaction (HRI). The 2011 HRI Pioneers Workshop will provide a forum for graduate students and postdocs to learn about the ...</div></span>
          <span id="toHide6" style="display:none;"><br /><div style="display:inline"><p>The 2011 HRI Pioneers Workshop will be conducted in conjunction with the 2011 ACM/IEEE International Conference on Human-Robot Interaction (HRI). The 2011 HRI Pioneers Workshop will provide a forum for graduate students and postdocs to learn about the current state of HRI, to present their work and to network with one another and with select senior researchers in a setting that is less formal and more interactive than the main conference. Workshop participants will discuss important issues and open challenges in the field, encouraging the formation of collaborative relationships across disciplines and geographic boundaries.</p></div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Telepresence</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Brian Scassellati 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957664&CFID=105753156&CFTOKEN=18642945">Exploring use cases for telepresence robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414601137&CFID=105753156&CFTOKEN=18642945">Katherine M. Tsui</a>, 
                        <a href="author_page.cfm?id=81443595444&CFID=105753156&CFTOKEN=18642945">Munjal Desai</a>, 
                        <a href="author_page.cfm?id=81100443257&CFID=105753156&CFTOKEN=18642945">Holly A. Yanco</a>, 
                        <a href="author_page.cfm?id=81482654802&CFID=105753156&CFTOKEN=18642945">Chris Uhlik</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 11-18</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957664" title="DOI">10.1145/1957656.1957664</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957664&ftid=929239&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow8" style="display:inline;"><br /><div style="display:inline">Telepresence robots can be thought of as embodied video conferencing on wheels. Companies producing these robots imagine them being used in a wide variety of situations (e.g., ad-hoc conversations at the office, inspections and troubleshooting at factories, ...</div></span>
          <span id="toHide8" style="display:none;"><br /><div style="display:inline"><p>Telepresence robots can be thought of as embodied video conferencing on wheels. Companies producing these robots imagine them being used in a wide variety of situations (e.g., ad-hoc conversations at the office, inspections and troubleshooting at factories, and patient rounds at medical facilities). In July and August 2010, we examined office-related use cases in a series of studies using two prototype robots (Anybots' QB and VGo Communications' VGo). In this paper, we present two studies: conference room meetings (<i>n</i>=6) and moving hallway conversations (<i>n</i>=24). We discuss who might benefit from using telepresence robots, in what scenarios, and the features that telepresence robots must incorporate for use in ad-hoc interactions.</p></div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957665&CFID=105753156&CFTOKEN=18642945">Mobile remote presence systems for older adults: acceptance, benefits, and concerns</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482646303&CFID=105753156&CFTOKEN=18642945">Jenay M. Beer</a>, 
                        <a href="author_page.cfm?id=81100499212&CFID=105753156&CFTOKEN=18642945">Leila Takayama</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 19-26</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957665" title="DOI">10.1145/1957656.1957665</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957665&ftid=929240&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow9" style="display:inline;"><br /><div style="display:inline">While much of human-robot interaction research focuses upon people interacting with autonomous robots, there is also much to be gained from exploring human interpersonal interaction through robots. The current study focuses on mobile remote presence ...</div></span>
          <span id="toHide9" style="display:none;"><br /><div style="display:inline"><p>While much of human-robot interaction research focuses upon people interacting with autonomous robots, there is also much to be gained from exploring human interpersonal interaction through robots. The current study focuses on mobile remote presence (MRP) systems as used by a population who could potentially benefit from more social connectivity and communication with remote people - older adults. Communication technologies are important for ensuring safety, independence, and social support for older adults, thereby potentially improving their quality of life and maintaining their independence [24]. However, before such technologies would be accepted and used by older adults, it is critical to understand their perceptions of the benefits, concerns, and adoption criteria for MRP systems. As such, we conducted a needs assessment with twelve volunteer participants (ages 63-88), who were given first-hand experience with both meeting a visitor via the MRP system and driving the MRP system to visit that person. The older adult participants identified benefits such as being able to see and be seen via the MRP system, reducing travel costs and hassles, and reducing social isolation. Among the concerns identified were etiquette of using the MRP, personal privacy, and overuse of the system. Some new use-cases were identified that have not yet been explored in prior work, for example, going to museums, attending live performances, and visiting friends who are hospitalized. The older adults in the current study preferred to operate the MRP themselves, rather than to be visited by others operating the MRP system. More findings are discussed in terms of their implications for design.</p></div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957666&CFID=105753156&CFTOKEN=18642945">Projector robot for augmented children's play</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482643414&CFID=105753156&CFTOKEN=18642945">Jong-gil Ahn</a>, 
                        <a href="author_page.cfm?id=81482650785&CFID=105753156&CFTOKEN=18642945">Hyeonsuk Yang</a>, 
                        <a href="author_page.cfm?id=81452598848&CFID=105753156&CFTOKEN=18642945">Gerard J. Kim</a>, 
                        <a href="author_page.cfm?id=81482645615&CFID=105753156&CFTOKEN=18642945">Namgyu Kim</a>, 
                        <a href="author_page.cfm?id=81482652495&CFID=105753156&CFTOKEN=18642945">Kyoung Choi</a>, 
                        <a href="author_page.cfm?id=81482654720&CFID=105753156&CFTOKEN=18642945">Hyemin Yeon</a>, 
                        <a href="author_page.cfm?id=81453627084&CFID=105753156&CFTOKEN=18642945">Eunja Hyun</a>, 
                        <a href="author_page.cfm?id=81482653084&CFID=105753156&CFTOKEN=18642945">Miheon Jo</a>, 
                        <a href="author_page.cfm?id=81414594930&CFID=105753156&CFTOKEN=18642945">Jeonghye Han</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 27-28</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957666" title="DOI">10.1145/1957656.1957666</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957666&ftid=929241&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=1957666&ftid=929242&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow10" style="display:inline;"><br /><div style="display:inline">Participating in a play is one of integral curriculum for young children at nurseries and kindergartens. At the same time, it is not very easy to successfully run and manage a play for young children due to their low age and immaturity. Scripts are difficult ...</div></span>
          <span id="toHide10" style="display:none;"><br /><div style="display:inline"><p>Participating in a play is one of integral curriculum for young children at nurseries and kindergartens. At the same time, it is not very easy to successfully run and manage a play for young children due to their low age and immaturity. Scripts are difficult to memorize and children's attention span is quite short. We are exploring the use of a robot and augmented reality (AR) technology to assist the nursery teachers in hopes to alleviate the difficult and complicated task of running the play, and also as a way to increase the learning effect by promoting the concentration and immersion (by the presence of the robot and novelty of the augmented display) [1, 2, 3]. For this purpose, we have devised a semi-autonomous remote-controlled projector robot with the capabilities of background projection and control, generating the synthesized augmented view, camera/movement control, producing story narration and various special effects. We have recently deployed the robot assistant for a play ('Three Little Pigs') at an actual nursery to observe and investigate various aspects of human robot interaction. For instance, the robot interacts with the actors on stage, leading and guiding them by showing (with small display on the robot) the synthesized augmented view, script guidance, and putting forth and changing the backdrop projection. It also assumes the role of the "camera man" and may instigate minute interplay with the actor as it zooms in and out on actors (by remote control). Our initial observation indicated that the use of the robot and AR indeed exhibited very high potential in drawing the attention of the children and enhancing the educational effect, but required the right amount of autonomy and external control and an intuitive interface.</p></div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>People and robots working together</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Frank Pollick 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957668&CFID=105753156&CFTOKEN=18642945">Improved human-robot team performance using chaski, a human-inspired plan execution system</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482652282&CFID=105753156&CFTOKEN=18642945">Julie Shah</a>, 
                        <a href="author_page.cfm?id=81482658042&CFID=105753156&CFTOKEN=18642945">James Wiken</a>, 
                        <a href="author_page.cfm?id=81100005312&CFID=105753156&CFTOKEN=18642945">Brian Williams</a>, 
                        <a href="author_page.cfm?id=81100258451&CFID=105753156&CFTOKEN=18642945">Cynthia Breazeal</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 29-36</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957668" title="DOI">10.1145/1957656.1957668</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957668&ftid=929243&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow12" style="display:inline;"><br /><div style="display:inline">We describe the design and evaluation of Chaski, a robot plan execution system that uses insights from human-human teaming to make human-robot teaming more natural and fluid. Chaski is a task-level executive that enables a robot to collaboratively ...</div></span>
          <span id="toHide12" style="display:none;"><br /><div style="display:inline"><p>We describe the design and evaluation of <i>Chaski</i>, a robot plan execution system that uses insights from human-human teaming to make human-robot teaming more natural and fluid. Chaski is a task-level executive that enables a robot to collaboratively execute a shared plan with a person. The system chooses and schedules the robot's actions, adapts to the human partner, and acts to minimize the human's idle time.</p> <p>We evaluate Chaski in human subject experiments in which a person works with a mobile and dexterous robot to collaboratively assemble structures using building blocks. We measure team performance outcomes for robots controlled by Chaski compared to robots that are verbally commanded, step-by-step by the human teammate. We show that Chaski reduces the human's idle time by 85%, a statistically significant difference. This result supports the hypothesis that human-robot team performance is improved when a robot emulates the effective coordination behaviors observed in human teams.</p></div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957669&CFID=105753156&CFTOKEN=18642945">A conversational robot in an elderly care center: an ethnographic study</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482657868&CFID=105753156&CFTOKEN=18642945">Alessandra Maria Sabelli</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105753156&CFTOKEN=18642945">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105753156&CFTOKEN=18642945">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 37-44</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957669" title="DOI">10.1145/1957656.1957669</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957669&ftid=929244&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow13" style="display:inline;"><br /><div style="display:inline">This paper reports an ethnographic study on the use of a conversational robot. We placed a robot for 3.5 months in an elderly care center. Assuming a real deployment scenario, the robot was managed by a single non-programmer person during the field trial, ...</div></span>
          <span id="toHide13" style="display:none;"><br /><div style="display:inline"><p>This paper reports an ethnographic study on the use of a conversational robot. We placed a robot for 3.5 months in an elderly care center. Assuming a real deployment scenario, the robot was managed by a single non-programmer person during the field trial, who teleoperated the robot and updated the contents. The robot was designed to engage in daily greetings and chatting with elderly people. Through the ethnographic approach, we clarified how the elderly people interacted with this conversational robot, how the deployment process adopted to introduce the robot was designed, and how the organization's personnel involved themselves in this deployment.</p></div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957670&CFID=105753156&CFTOKEN=18642945">Evaluating the applicability of current models of workload to peer-based human-robot teams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456634287&CFID=105753156&CFTOKEN=18642945">Caroline E. Harriott</a>, 
                        <a href="author_page.cfm?id=81482645412&CFID=105753156&CFTOKEN=18642945">Tao Zhang</a>, 
                        <a href="author_page.cfm?id=81100313646&CFID=105753156&CFTOKEN=18642945">Julie A. Adams</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 45-52</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957670" title="DOI">10.1145/1957656.1957670</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957670&ftid=929245&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow14" style="display:inline;"><br /><div style="display:inline">Human-Robot peer-based teams are evolving from a far-off possibility into a reality. Human Performance Moderator Functions (HPMFs) can be used to predict human behavior by incorporating the effects of internal and external influences such as fatigue ...</div></span>
          <span id="toHide14" style="display:none;"><br /><div style="display:inline"><p>Human-Robot peer-based teams are evolving from a far-off possibility into a reality. Human Performance Moderator Functions (HPMFs) can be used to predict human behavior by incorporating the effects of internal and external influences such as fatigue and workload. The applicability of HPMFs to human-robot teams is not proven. The presented research focuses on determining the applicability of workload HPMFs in team tasks for first response mass casualty triage incidents between a Human-Human and a Human-Robot team. A model representing workload for each team was developed using IMPRINT Pro. The results from an empirical evaluation were compared to the model results. While significant differences between the two conditions were not found in all data, there was a general trend that workload in the human-robot condition was slightly lower than the workload experienced in the human-human condition. This trend was predicted by the IMPRINT Pro models. These results are the first to indicate that existing HPMFs can be applied to human-robot peer-based teams.</p></div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Anthropomorphic</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Vanessa Evers 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957672&CFID=105753156&CFTOKEN=18642945">Interpersonal variation in understanding robots as social actors</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81442595547&CFID=105753156&CFTOKEN=18642945">Kerstin Fischer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 53-60</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957672" title="DOI">10.1145/1957656.1957672</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957672&ftid=929246&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow16" style="display:inline;"><br /><div style="display:inline">In this paper, I investigate interpersonal variation in verbal HRI with respect to the computers-as-social-actors hypothesis. The analysis of a corpus of verbal human-robot interactions shows that only a subgroup of the users treat the robot as a social ...</div></span>
          <span id="toHide16" style="display:none;"><br /><div style="display:inline"><p>In this paper, I investigate interpersonal variation in verbal HRI with respect to the computers-as-social-actors hypothesis. The analysis of a corpus of verbal human-robot interactions shows that only a subgroup of the users treat the robot as a social actor. Thus, taking interpersonal variation into account reveals that not all users transfer social behaviors from human interactions into HRI. This casts doubts on the suggestion that the social responses to computers and robots reported on previously are due to mindlessness. At the same time, participants' understanding of robots as social or non-social actors can be shown to have a considerable influence on their linguistic behavior throughout the dialogs.</p></div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957673&CFID=105753156&CFTOKEN=18642945">Effects of anticipated human-robot interaction and predictability of robot behavior on perceptions of anthropomorphism</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81474702897&CFID=105753156&CFTOKEN=18642945">Friederike Eyssel</a>, 
                        <a href="author_page.cfm?id=81482657814&CFID=105753156&CFTOKEN=18642945">Dieta Kuchenbrandt</a>, 
                        <a href="author_page.cfm?id=81482659413&CFID=105753156&CFTOKEN=18642945">Simon Bobinger</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 61-68</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957673" title="DOI">10.1145/1957656.1957673</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957673&ftid=929247&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow17" style="display:inline;"><br /><div style="display:inline">Recent research has shown that anthropomorphism represents a means to facilitate HRI. Under which conditions do people anthropomorphize robots and other nonhuman agents? This research question was investigated in an experiment that manipulated participants' ...</div></span>
          <span id="toHide17" style="display:none;"><br /><div style="display:inline"><p>Recent research has shown that anthropomorphism represents a means to facilitate HRI. Under which conditions do people anthropomorphize robots and other nonhuman agents? This research question was investigated in an experiment that manipulated participants' anticipation of a prospective human-robot interaction (HRI) with a robot whose behavior was characterized by either low or high predictability. We examined effects of these factors on perceptions of anthropomorphism and acceptance of the robot. Innovatively, the present research demonstrates that anticipation of HRI with an unpredictable agent increased anthropomorphic inferences and acceptance of the robot. Implications for future research on psychological determinants of anthropomorphism are discussed.</p></div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957674&CFID=105753156&CFTOKEN=18642945">Expressing thought: improving robot readability with animation principles</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100499212&CFID=105753156&CFTOKEN=18642945">Leila Takayama</a>, 
                        <a href="author_page.cfm?id=81482656804&CFID=105753156&CFTOKEN=18642945">Doug Dooley</a>, 
                        <a href="author_page.cfm?id=81100098733&CFID=105753156&CFTOKEN=18642945">Wendy Ju</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 69-76</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957674" title="DOI">10.1145/1957656.1957674</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957674&ftid=927095&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow18" style="display:inline;"><br /><div style="display:inline">The animation techniques of anticipation and reaction can help create robot behaviors that are human readable such that people can figure out what the robot is doing, reasonably predict what the robot will do next, and ultimately interact with the robot ...</div></span>
          <span id="toHide18" style="display:none;"><br /><div style="display:inline"><p>The animation techniques of anticipation and reaction can help create robot behaviors that are human readable such that people can figure out what the robot is doing, reasonably predict what the robot will do next, and ultimately interact with the robot in an effective way. By showing forethought before action and expressing a reaction to the task outcome (success or failure), we prototyped a set of human-robot interaction behaviors. In a 2 (forethought vs. none: between) x 2 (reaction to outcome vs. none: between) x 2 (success vs. failure task outcome: within) experiment, we tested the influences of forethought and reaction upon people's perceptions of the robot and the robot's readability. In this online video prototype experiment (<i>N</i>=273), we have found support for the hypothesis that perceptions of robots are influenced by robots showing forethought, the task outcome (success or failure), and showing goal-oriented reactions to those task outcomes. Implications for theory and design are discussed.</p></div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Life-like motion</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Tony Belpaeme 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957676&CFID=105753156&CFTOKEN=18642945">Spatiotemporal correspondence as a metric for human-like robot motion</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81460656159&CFID=105753156&CFTOKEN=18642945">Michael J. Gielniak</a>, 
                        <a href="author_page.cfm?id=81310502411&CFID=105753156&CFTOKEN=18642945">Andrea L. Thomaz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 77-84</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957676" title="DOI">10.1145/1957656.1957676</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957676&ftid=929249&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957676&ftid=929250&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow20" style="display:inline;"><br /><div style="display:inline">Coupled degrees-of-freedom exhibit correspondence, in that their trajectories influence each other. In this paper we add evidence to the hypothesis that spatiotemporal correspondence (STC) of distributed actuators is a component of human-like motion. ...</div></span>
          <span id="toHide20" style="display:none;"><br /><div style="display:inline"><p>Coupled degrees-of-freedom exhibit correspondence, in that their trajectories influence each other. In this paper we add evidence to the hypothesis that spatiotemporal correspondence (STC) of distributed actuators is a component of human-like motion. We demonstrate a method for making robot motion more human-like, by optimizing with respect to a nonlinear STC metric. Quantitative evaluation of STC between coordinated robot motion, human motion capture data, and retargeted human motion capture data projected onto an anthropomorphic robot suggests that coordinating robot motion with respect to the STC metric makes the motion more human-like. A user study based on mimicking shows that STC-optimized motion is (1) more often recognized as a common human motion, (2) more accurately identified as the originally intended motion, and (3) mimicked more accurately than a non-optimized version. We conclude that coordinating robot motion with respect to the STC metric makes the motion more human-like. Finally, we present and discuss data on potential reasons why coordinating motion increases recognition and ability to mimic.</p></div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957677&CFID=105753156&CFTOKEN=18642945">An assistive tele-operated anthropomorphic robot hand: osaka city university hand ii</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482660092&CFID=105753156&CFTOKEN=18642945">Raafat Ahmed Abdel El-Azim Mahmoud</a>, 
                        <a href="author_page.cfm?id=81482641430&CFID=105753156&CFTOKEN=18642945">Atsushi Ueno</a>, 
                        <a href="author_page.cfm?id=81100424963&CFID=105753156&CFTOKEN=18642945">Shoji Tatsumi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 85-92</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957677" title="DOI">10.1145/1957656.1957677</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957677&ftid=929251&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow21" style="display:inline;"><br /><div style="display:inline">This paper presents an anthropomorphic robot hand called Osaka-City-University-Hand II, which is an improved version of Osaka-City-University-Hand I. The cosmetic and the function of our proposed hand allow us to use the hand as a prosthetic hand in ...</div></span>
          <span id="toHide21" style="display:none;"><br /><div style="display:inline"><p>This paper presents an anthropomorphic robot hand called Osaka-City-University-Hand II, which is an improved version of Osaka-City-University-Hand I. The cosmetic and the function of our proposed hand allow us to use the hand as a prosthetic hand in addition to as an open platform of robot hands for robotics research. Distributed tactile and force sensors are appended to the OCU-Hand II as a feedback system in order to grasp an object firmly. A new control strategy is to be adopted, in which a master light-weight glove is to be used to drive the OCU-Hand II as a slave. In the tele-operating task, arithmetic operations are to be done to the outputs of the feedback sensors in order to increase the resolution of the master-slave driving technique as well as to overcome the hardware amplification defects. In order to use OCU-Hand II as a helper equipment, a novel and unique assistive mode is included within master-slave control strategy, where the OCU-Hand II is assisting and helping its operators in order to reduce the load on the operator and perform the usual operations in a better manner or even faster than the usual. During the assistive mode the operator is enabled to perform different tasks other than the master-slave driving while he is still putting on the master-glove.</p></div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957678&CFID=105753156&CFTOKEN=18642945">Effects related to synchrony and repertoire in perceptions of robot dance</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414620434&CFID=105753156&CFTOKEN=18642945">Eleanor Avrunin</a>, 
                        <a href="author_page.cfm?id=81418596943&CFID=105753156&CFTOKEN=18642945">Justin Hart</a>, 
                        <a href="author_page.cfm?id=81482652506&CFID=105753156&CFTOKEN=18642945">Ashley Douglas</a>, 
                        <a href="author_page.cfm?id=81482644693&CFID=105753156&CFTOKEN=18642945">Brian Scassellati</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 93-100</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957678" title="DOI">10.1145/1957656.1957678</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957678&ftid=929252&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow22" style="display:inline;"><br /><div style="display:inline">In this work we identify low-level aspects of robot motion that can be exploited to create impressions of agency and lifelikeness. In two experiments, participants view split-screen videos of multiple robots set to music and rate the robots on their ...</div></span>
          <span id="toHide22" style="display:none;"><br /><div style="display:inline"><p>In this work we identify low-level aspects of robot motion that can be exploited to create impressions of agency and lifelikeness. In two experiments, participants view split-screen videos of multiple robots set to music and rate the robots on their dance ability, lifelikeness, and entertainment value. The first experiment tests the impact of the correspondence (or lack thereof) of the robot's motion to the underlying rhythm of the music, and the effect of matching changes in the robot's movement to changes in the music, such as a phrase of vocals or drumming. This motivates a second experiment which more deeply explores the relationships of asynchrony and changes in motion repertoire to participants' perceptions of the lifelikeness of the robot's motion. Findings indicate that perceptions of the lifelikeness of the robot and the quality of the dance can be manipulated by simple changes, such as variation in the repertoire of motions, coordination of changes in behavior with events in the music, and the addition of flaws to the robot's synchrony with the music.</p></div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957679&CFID=105753156&CFTOKEN=18642945">Lighthead robotic face</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81442614689&CFID=105753156&CFTOKEN=18642945">Fr&#233;d&#233;ric Delaunay</a>, 
                        <a href="author_page.cfm?id=81442614609&CFID=105753156&CFTOKEN=18642945">Joachim de Greeff</a>, 
                        <a href="author_page.cfm?id=81100462469&CFID=105753156&CFTOKEN=18642945">Tony Belpaeme</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 101-102</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957679" title="DOI">10.1145/1957656.1957679</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957679&ftid=929253&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957679&ftid=929254&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow23" style="display:inline;"><br /><div style="display:inline">This video is about a new kind of robotic head. Through back-projection of a computer generated video into a half-translucent mask, the LightHead robotic head has many advantages compared to tradition mechatronic robotic faces. These advantages are most ...</div></span>
          <span id="toHide23" style="display:none;"><br /><div style="display:inline"><p>This video is about a new kind of robotic head. Through back-projection of a computer generated video into a half-translucent mask, the LightHead robotic head has many advantages compared to tradition mechatronic robotic faces. These advantages are most notably the versatility and ease of controlling facial expressions and creating new faces, the total weight and the low costs. By mounting the head on a robotic arm and equipping it with face detection software, the robot can interact with people in a natural manner.</p></div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">PANEL SESSION: <strong>Panel</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957681&CFID=105753156&CFTOKEN=18642945">HRI: the real world</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81320488460&CFID=105753156&CFTOKEN=18642945">Jennifer L. Burke</a>, 
                        <a href="author_page.cfm?id=81100285247&CFID=105753156&CFTOKEN=18642945">Henrik I. Christensen</a>, 
                        <a href="author_page.cfm?id=81482652532&CFID=105753156&CFTOKEN=18642945">Roland Menassa</a>, 
                        <a href="author_page.cfm?id=81482645900&CFID=105753156&CFTOKEN=18642945">Ralf Koeppe</a>, 
                        <a href="author_page.cfm?id=81482651400&CFID=105753156&CFTOKEN=18642945">Joe Dyer</a>, 
                        <a href="author_page.cfm?id=81100552268&CFID=105753156&CFTOKEN=18642945">Mario Munich</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 103-104</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957681" title="DOI">10.1145/1957656.1957681</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957681&ftid=929255&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow25" style="display:inline;"><br /><div style="display:inline">This year's conference focuses on human-robot interaction in the real world. The panel discussion presents the view from those who are "living it": industry leaders who are relying on current robotic technology to accomplish their work right now. Who ...</div></span>
          <span id="toHide25" style="display:none;"><br /><div style="display:inline"><p>This year's conference focuses on human-robot interaction in the real world. The panel discussion presents the view from those who are "living it": industry leaders who are relying on current robotic technology to accomplish their work right now. Who better to provide the most compelling information on issues/challenges that are influencing their use? This panel gathers experts from business and industry to discuss their experiences in using robotic technology in their field. Topics include the financial, organizational, and practical challenges faced by professionals using robotic technology in the workplace, and factors influencing the acceptance of robots at work. The implications for design of robotic products and systems are also discussed. The session is a must for professionals and academic researchers interested in solving problems related to using robots in real world settings.</p></div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">POSTER SESSION: <strong>Late-breaking reports/poster session</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957683&CFID=105753156&CFTOKEN=18642945">Towards an online voice-based gender and internal state detection model</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482656190&CFID=105753156&CFTOKEN=18642945">Amir Aly</a>, 
                        <a href="author_page.cfm?id=81350577565&CFID=105753156&CFTOKEN=18642945">Adriana Tapus</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 105-106</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957683" title="DOI">10.1145/1957656.1957683</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957683&ftid=929256&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow27" style="display:inline;"><br /><div style="display:inline">In human-robot interaction, gender and internal state detection play an important role in making the robot reacting in an appropriate manner. This research focuses on the important features to extract from a voice signal in order to construct successful ...</div></span>
          <span id="toHide27" style="display:none;"><br /><div style="display:inline"><p>In human-robot interaction, gender and internal state detection play an important role in making the robot reacting in an appropriate manner. This research focuses on the important features to extract from a voice signal in order to construct successful gender and internal state detection systems, and shows the benefits of combining both systems together on the total average recognition score. Moreover, it consists a foundation on an ongoing approach to estimate the human internal state online via unsupervised clustering algorithms.</p></div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957684&CFID=105753156&CFTOKEN=18642945">Policy adaptation with tactile feedback</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310499255&CFID=105753156&CFTOKEN=18642945">Brenna D. Argall</a>, 
                        <a href="author_page.cfm?id=81100070850&CFID=105753156&CFTOKEN=18642945">Eric L. Sauser</a>, 
                        <a href="author_page.cfm?id=81100342762&CFID=105753156&CFTOKEN=18642945">Aude G. Billard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 107-108</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957684" title="DOI">10.1145/1957656.1957684</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957684&ftid=929257&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow28" style="display:inline;"><br /><div style="display:inline">Behavior adaptation with execution experience is a practical feature for any policy learning system. Our work provides performance feedback to a robot learner in the form of tactile corrections from a human teacher, for the purpose of policy refinement ...</div></span>
          <span id="toHide28" style="display:none;"><br /><div style="display:inline"><p>Behavior adaptation with execution experience is a practical feature for any policy learning system. Our work provides performance feedback to a robot learner in the form of <i>tactile corrections</i> from a human teacher, for the purpose of policy <i>refinement</i> as well as policy <i>reuse</i>. Multiple variants of our general approach have been validated on the iCub robot, as building blocks towards a high-DoF humanoid system that integrates tactile sensing on the hands and arms into complex behaviors and sophisticated learning routines.</p></div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957685&CFID=105753156&CFTOKEN=18642945">Perception by proxy: humans helping robots to see in a manipulation task</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482644427&CFID=105753156&CFTOKEN=18642945">John Alan Atherton</a>, 
                        <a href="author_page.cfm?id=81350575550&CFID=105753156&CFTOKEN=18642945">Michael A. Goodrich</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 109-110</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957685" title="DOI">10.1145/1957656.1957685</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957685&ftid=929258&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow29" style="display:inline;"><br /><div style="display:inline">Robots excel at planning and performing tasks in controlled environments, but poor perception often leads to poor performance in unstructured environments. One typical way of improving robot performance is to give more control to a human operator and ...</div></span>
          <span id="toHide29" style="display:none;"><br /><div style="display:inline"><p>Robots excel at planning and performing tasks in controlled environments, but poor perception often leads to poor performance in unstructured environments. One typical way of improving robot performance is to give more control to a human operator and then design user interfaces that build the operator's situation awareness. As an alternative, humans can support robot perception to add structure to unstructured environments. We claim that when humans support robot perception, robots can spend more time acting autonomously, which can lead to reduced operator workload and increased overall performance. We present a design process, called <i>perception by proxy</i>, and apply it to a simple manipulation task.</p></div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957686&CFID=105753156&CFTOKEN=18642945">A comparison of unsupervised learning algorithms for gesture clustering</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482656752&CFID=105753156&CFTOKEN=18642945">Adrian Ball</a>, 
                        <a href="author_page.cfm?id=81100403342&CFID=105753156&CFTOKEN=18642945">David Rye</a>, 
                        <a href="author_page.cfm?id=81340492287&CFID=105753156&CFTOKEN=18642945">Fabio Ramos</a>, 
                        <a href="author_page.cfm?id=81388591190&CFID=105753156&CFTOKEN=18642945">Mari Velonaki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 111-112</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957686" title="DOI">10.1145/1957656.1957686</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957686&ftid=929259&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow30" style="display:inline;"><br /><div style="display:inline">Gesture recognition is an important aspect of interpersonal social interaction. Developing a similar capacity in a robot will improve human-robot interaction. Various unsupervised clustering methods applied to clustering a set of dynamic human arm gestures ...</div></span>
          <span id="toHide30" style="display:none;"><br /><div style="display:inline"><p>Gesture recognition is an important aspect of interpersonal social interaction. Developing a similar capacity in a robot will improve human-robot interaction. Various unsupervised clustering methods applied to clustering a set of dynamic human arm gestures are compared. Unsupervised clustering is important in gesture recognition as it imposes no <i>a priori</i> bound on the set of gestures. Results are compared using v-measure, a metric that allows differential weighting between clustering homogeneity and completeness. Experiments show that the best clustering method depends on the desired balance between homogeneity and completeness.</p></div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957687&CFID=105753156&CFTOKEN=18642945">Perceptions and knowledge about robots in children of 11 years old in M&#233;xico City.</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482657946&CFID=105753156&CFTOKEN=18642945">Eduardo Ben&#237;tez Sandoval</a>, 
                        <a href="author_page.cfm?id=81482661306&CFID=105753156&CFTOKEN=18642945">Mauricio Reyes Castillo</a>, 
                        <a href="author_page.cfm?id=81482658782&CFID=105753156&CFTOKEN=18642945">John Alexander Rey Galindo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 113-114</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957687" title="DOI">10.1145/1957656.1957687</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957687&ftid=929260&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow31" style="display:inline;"><br /><div style="display:inline">This paper seeks to describe the knowledge and perceptions of robots that have a group of 19 children under 11 years old in Mexico City has, interacting with a teleoperated anthropomorphic robot compared with a group of 18 children with the same characteristics ...</div></span>
          <span id="toHide31" style="display:none;"><br /><div style="display:inline"><p>This paper seeks to describe the knowledge and perceptions of robots that have a group of 19 children under 11 years old in Mexico City has, interacting with a teleoperated anthropomorphic robot compared with a group of 18 children with the same characteristics that have not interacted with same robot. We seek to focus a precedent in the Mexican children in their context for future development of social robots.</p></div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957688&CFID=105753156&CFTOKEN=18642945">The crucial role of robot self-awareness in HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482658632&CFID=105753156&CFTOKEN=18642945">Manuel Birlo</a>, 
                        <a href="author_page.cfm?id=81482651888&CFID=105753156&CFTOKEN=18642945">Adriana Tapus</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 115-116</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957688" title="DOI">10.1145/1957656.1957688</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957688&ftid=929261&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow32" style="display:inline;"><br /><div style="display:inline">In this paper, we present the first steps towards a new concept of robot self-awareness that can be implemented into embodied robot systems. Our concept of "the self" is inspired by already existing approaches and aims to provide a cognitive system with ...</div></span>
          <span id="toHide32" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present the first steps towards a new concept of robot self-awareness that can be implemented into embodied robot systems. Our concept of "the self" is inspired by already existing approaches and aims to provide a cognitive system with meta-cognitive capabilities. We believe that robot self-awareness is a crucial factor in the improvement of HRI.</p></div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957689&CFID=105753156&CFTOKEN=18642945">Development of a context model based on video analysis</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414616801&CFID=105753156&CFTOKEN=18642945">Roland Buchner</a>, 
                        <a href="author_page.cfm?id=81381602151&CFID=105753156&CFTOKEN=18642945">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81100481664&CFID=105753156&CFTOKEN=18642945">Manfred Tscheligi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 117-118</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957689" title="DOI">10.1145/1957656.1957689</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957689&ftid=929262&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow33" style="display:inline;"><br /><div style="display:inline">This paper reports on the analysis of video footage of a human-robot study in public place regarding context factors that significantly influence the interaction. Therefore, a coding scheme was developed which was later used to analyze the video footage ...</div></span>
          <span id="toHide33" style="display:none;"><br /><div style="display:inline"><p>This paper reports on the analysis of video footage of a human-robot study in public place regarding context factors that significantly influence the interaction. Therefore, a coding scheme was developed which was later used to analyze the video footage of the human-robot study. To ensure the validity of the coding, the video footage was coded independently by two students and afterwards the Cohen's Kappa was calculated to ensure the intercoder reliability. This calculation served as a basis for the creation of the first context model which shows factors that influence the interaction. This approach could show that it is possible to extract valid context factors and to create a context model based on video annotation. These factors should then be further tested in lab-based studies to get a better understanding of how they affect the human-robot interaction.</p></div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957690&CFID=105753156&CFTOKEN=18642945">Using depth information to improve face detection</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81447604357&CFID=105753156&CFTOKEN=18642945">Walker Burgin</a>, 
                        <a href="author_page.cfm?id=81331501616&CFID=105753156&CFTOKEN=18642945">Caroline Pantofaru</a>, 
                        <a href="author_page.cfm?id=81100105515&CFID=105753156&CFTOKEN=18642945">William D. Smart</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 119-120</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957690" title="DOI">10.1145/1957656.1957690</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957690&ftid=929263&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957691&CFID=105753156&CFTOKEN=18642945">Interactive methods of tele-operating a single unmanned ground vehicle on a small screen interface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482646876&CFID=105753156&CFTOKEN=18642945">Chua Wei Liang Kenny Chua</a>, 
                        <a href="author_page.cfm?id=81482659904&CFID=105753156&CFTOKEN=18642945">Foo Chuan Huat Foo</a>, 
                        <a href="author_page.cfm?id=81482641577&CFID=105753156&CFTOKEN=18642945">Lee Yong Siang Lee</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 121-122</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957691" title="DOI">10.1145/1957656.1957691</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957691&ftid=929264&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow35" style="display:inline;"><br /><div style="display:inline">This study explores 4 different interaction methods of tele-operating a single unmanned ground vehicle (UGV) on a small screen interface. An experiment involving 20 participants carrying out a navigational task was conducted. Performance measures such ...</div></span>
          <span id="toHide35" style="display:none;"><br /><div style="display:inline"><p>This study explores 4 different interaction methods of tele-operating a single unmanned ground vehicle (UGV) on a small screen interface. An experiment involving 20 participants carrying out a navigational task was conducted. Performance measures such as task completion time, number of errors made and workload ratings were considered and analyzed.</p></div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957692&CFID=105753156&CFTOKEN=18642945">Child's recognition of emotions in robot's face and body</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482645219&CFID=105753156&CFTOKEN=18642945">Iris Cohen</a>, 
                        <a href="author_page.cfm?id=81384600719&CFID=105753156&CFTOKEN=18642945">Rosemarijn Looije</a>, 
                        <a href="author_page.cfm?id=81100075980&CFID=105753156&CFTOKEN=18642945">Mark A. Neerincx</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 123-124</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957692" title="DOI">10.1145/1957656.1957692</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957692&ftid=929265&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow36" style="display:inline;"><br /><div style="display:inline">Social robots can comfort and support children who have to cope with chronic diseases. In previous studies, a "facial robot", the iCat, proved to show well-recognized emotional expressions that are important in social interactions. The question is if ...</div></span>
          <span id="toHide36" style="display:none;"><br /><div style="display:inline"><p>Social robots can comfort and support children who have to cope with chronic diseases. In previous studies, a "facial robot", the iCat, proved to show well-recognized emotional expressions that are important in social interactions. The question is if a mobile robot without a face, the Nao, can express emotions with its body. First, dynamic body postures were created and validated that express fear, happiness, anger, sadness and surprise. Then, fourteen children had to recognize emotions, expressed by both robots. Recognition rates were relatively high (between 68% and 99% accuracy). Only for the emotion "sad", the recognition was better for the iCat (95%) compared to the Nao (68%). Providing context increased the number of correct recognitions. In a second session, the emotions were significantly better recognized than during the first session for both robots. In sum, we succeeded to design Nao emotions, which were well recognized and learned, and can be important ingredients of the social dialogs with children.</p></div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957693&CFID=105753156&CFTOKEN=18642945">Things that tweet, check-in and are befriended.: two explorations on robotics &#38; social media.</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81328487965&CFID=105753156&CFTOKEN=18642945">Henriette Cramer</a>, 
                        <a href="author_page.cfm?id=81470642688&CFID=105753156&CFTOKEN=18642945">Sebastian B&#252;ttner</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 125-126</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957693" title="DOI">10.1145/1957656.1957693</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957693&ftid=929266&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow37" style="display:inline;"><br /><div style="display:inline">This late breaking report describes two explorations of effects of using social media in human-robot interaction. The first is an exploration of how 'autonomous creatures' can use information shared via social awareness streams by implementing a Nabaztag ...</div></span>
          <span id="toHide37" style="display:none;"><br /><div style="display:inline"><p>This late breaking report describes two explorations of effects of using social media in human-robot interaction. The first is an exploration of how 'autonomous creatures' can use information shared via social awareness streams by implementing a Nabaztag to use information from its 'friends' on location-sharing service foursquare. The second is an informal analysis of tweets sent to an existing robot-associated twitter account as a case. We show parallels to prior research and discuss questions that these simple explorations pose for the future of robots and social media.</p></div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957694&CFID=105753156&CFTOKEN=18642945">A pilot study to understand requirements of a shopping mall robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414609591&CFID=105753156&CFTOKEN=18642945">Chandan Datta</a>, 
                        <a href="author_page.cfm?id=81100274655&CFID=105753156&CFTOKEN=18642945">Anuj Kapuria</a>, 
                        <a href="author_page.cfm?id=81456612183&CFID=105753156&CFTOKEN=18642945">Ritukar Vijay</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 127-128</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957694" title="DOI">10.1145/1957656.1957694</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957694&ftid=929267&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow38" style="display:inline;"><br /><div style="display:inline">As part of our long term research interests in examining the technological requirements for a shopping mall robot, we performed a short pilot study during the Christmas holidays to identify the social interaction dynamics for Neel, a wheeled mobile robot ...</div></span>
          <span id="toHide38" style="display:none;"><br /><div style="display:inline"><p>As part of our long term research interests in examining the technological requirements for a shopping mall robot, we performed a short pilot study during the Christmas holidays to identify the social interaction dynamics for Neel, a wheeled mobile robot which interacts using its on-site and online presence. During the pilot study we found that a range of the robot's interaction capabilities were mostly unused due to the relatively short interactions users had with the robot to fulfill their informational requirements like the movie show-times or apparel deals. Since, its challenging to educate and inform a diverse mass of users about the robot's functionality, we decided to divide the research roadmap in stages where in the first stage the users would learn the value of the robot's capabilities by the repeated short-time interactions and over the long term more users would register for the robot's services. Hence, identifying the temporal and episodic characteristics of the interactions are perceived important to match the expectations and privacy concerns of the users. We also identified that while non-interactively delivering shopping related information through a web application is relatively easier, doing it actively through the robot can be probed as advertisements very easily by human participants and negate the user experience we try to deliver. We report some key technological advances we made through our field trial and set forth the goals.</p></div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957695&CFID=105753156&CFTOKEN=18642945">Managing social constraints on recharge behaviour for robot companions using memory</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81472646961&CFID=105753156&CFTOKEN=18642945">Amol A. Deshmukh</a>, 
                        <a href="author_page.cfm?id=81384594135&CFID=105753156&CFTOKEN=18642945">Mei Yii Lim</a>, 
                        <a href="author_page.cfm?id=81384604731&CFID=105753156&CFTOKEN=18642945">Michael Kriegel</a>, 
                        <a href="author_page.cfm?id=81100603687&CFID=105753156&CFTOKEN=18642945">Ruth Aylett</a>, 
                        <a href="author_page.cfm?id=81474699470&CFID=105753156&CFTOKEN=18642945">Kyron Du Casse</a>, 
                        <a href="author_page.cfm?id=81350568813&CFID=105753156&CFTOKEN=18642945">Koay Kheng L</a>, 
                        <a href="author_page.cfm?id=81100447769&CFID=105753156&CFTOKEN=18642945">Kerstin Dautenhahn</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 129-130</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957695" title="DOI">10.1145/1957656.1957695</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957695&ftid=929268&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow39" style="display:inline;"><br /><div style="display:inline">In this paper, we present an approach to monitor human activities such as entry, exit and break times of people in a workplace environment. The companion robot then learns the users' presence patterns over a period of time through memory generalisation ...</div></span>
          <span id="toHide39" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present an approach to monitor human activities such as entry, exit and break times of people in a workplace environment. The companion robot then learns the users' presence patterns over a period of time through memory generalisation and plans a suitable time for re charging itself causing less hindrance to human-robot interaction.</p></div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957696&CFID=105753156&CFTOKEN=18642945">Designing interruptive behaviors of a public environmental monitoring robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100133286&CFID=105753156&CFTOKEN=18642945">Vanessa Evers</a>, 
                        <a href="author_page.cfm?id=81482640976&CFID=105753156&CFTOKEN=18642945">Roelof de Vries</a>, 
                        <a href="author_page.cfm?id=81482662178&CFID=105753156&CFTOKEN=18642945">Paulo Alvito</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 131-132</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957696" title="DOI">10.1145/1957656.1957696</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957696&ftid=929269&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow40" style="display:inline;"><br /><div style="display:inline">This paper reports ongoing research to inform the design of a social robot to monitor levels of pollutant gasses in the air. Next to licensed environmental agents and immobile chemical sensors, mobile technologies such as robotic agents are needed to ...</div></span>
          <span id="toHide40" style="display:none;"><br /><div style="display:inline"><p>This paper reports ongoing research to inform the design of a social robot to monitor levels of pollutant gasses in the air. Next to licensed environmental agents and immobile chemical sensors, mobile technologies such as robotic agents are needed to collect complaints and smell descriptions from humans in urban industrial areas. These robots will interact with members of the public and ensure responsiveness and accuracy of responses. For robots to be accepted as representative environmental monitoring agents and for people to comply with robot instructions in the case of a calamity, social skills will be important. In this paper we will describe the intelligent environment the environmental robot is part of and discuss preliminary work to understand in what way robot interruptions can be mitigated with help of social robot behaviors.</p></div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957697&CFID=105753156&CFTOKEN=18642945">Interactional disparities in english and arabic native speakers with a bi-lingual robot receptionist</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456636105&CFID=105753156&CFTOKEN=18642945">Imran Fanaswala</a>, 
                        <a href="author_page.cfm?id=81100556983&CFID=105753156&CFTOKEN=18642945">Brett Browning</a>, 
                        <a href="author_page.cfm?id=81100350931&CFID=105753156&CFTOKEN=18642945">Majd Sakr</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 133-134</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957697" title="DOI">10.1145/1957656.1957697</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957697&ftid=929270&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow41" style="display:inline;"><br /><div style="display:inline">HRI studies in a Middle Eastern environment are subject to nuances and subtleties. This study explores the nature of interactions, in an uncontrolled environment, between a permanently deployed bi-lingual robot-receptionist and interlocutors of varied ...</div></span>
          <span id="toHide41" style="display:none;"><br /><div style="display:inline"><p>HRI studies in a Middle Eastern environment are subject to nuances and subtleties. This study explores the nature of interactions, in an uncontrolled environment, between a permanently deployed bi-lingual robot-receptionist and interlocutors of varied native tongues. We correlate an interlocutor's native language with their propensity for accepting an invite and the duration of the ensuing conversation. Subsequently, we present results that demonstrate significant disparity in interactional patterns between English and Arabic speakers. We also assess the importance of a transliterated Arabic input mode for encouraging user interaction.</p></div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957698&CFID=105753156&CFTOKEN=18642945">Comparative analysis of human motion trajectory prediction using minimum variance curvature</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482649478&CFID=105753156&CFTOKEN=18642945">Gonzalo Ferrer</a>, 
                        <a href="author_page.cfm?id=81100155549&CFID=105753156&CFTOKEN=18642945">Alberto Sanfeliu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 135-136</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957698" title="DOI">10.1145/1957656.1957698</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957698&ftid=929271&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow42" style="display:inline;"><br /><div style="display:inline">The prediction of human motion intentionality is a key issue towards intelligent human robot interaction and robot navigation. In this work we present a comparative study of several prediction functions that are based on the minimum curvature variance ...</div></span>
          <span id="toHide42" style="display:none;"><br /><div style="display:inline"><p>The prediction of human motion intentionality is a key issue towards intelligent human robot interaction and robot navigation. In this work we present a comparative study of several prediction functions that are based on the minimum curvature variance from the current position to all the potential destination points, that means, the points that are relevant for people motion intentionality. The proposed predictor computes, at each interval of time, the trajectory from the present to the destination positions, and makes a prediction of the human motion at each interval of time using only the criterion of minimum curvature variation. The method has been validated in the Edinburgh Informatics Forum Pedestrian database.</p></div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957699&CFID=105753156&CFTOKEN=18642945">Anthropomorphic design for an interactive urban robot: the right design approach</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81319491727&CFID=105753156&CFTOKEN=18642945">Florian F&#246;rster</a>, 
                        <a href="author_page.cfm?id=81381602151&CFID=105753156&CFTOKEN=18642945">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81100481664&CFID=105753156&CFTOKEN=18642945">Manfred Tscheligi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 137-138</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957699" title="DOI">10.1145/1957656.1957699</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957699&ftid=929272&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow43" style="display:inline;"><br /><div style="display:inline">The paper presents the first step of a user-centered design process for a robot designated to operate in urban public space. A participatory design workshop was conducted to challenge the anthropomorphic design approach assumed by the designers and elicit ...</div></span>
          <span id="toHide43" style="display:none;"><br /><div style="display:inline"><p>The paper presents the first step of a user-centered design process for a robot designated to operate in urban public space. A participatory design workshop was conducted to challenge the anthropomorphic design approach assumed by the designers and elicit user requirements for the design. In contrast to the expectations, the results show a tendency towards a preference of a non-anthropomorphic design.</p></div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957700&CFID=105753156&CFTOKEN=18642945">Tactile sensing: a key technology for safe physical human robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81384608934&CFID=105753156&CFTOKEN=18642945">Markus Fritzsche</a>, 
                        <a href="author_page.cfm?id=81100026094&CFID=105753156&CFTOKEN=18642945">Norbert Elkmann</a>, 
                        <a href="author_page.cfm?id=81384593216&CFID=105753156&CFTOKEN=18642945">Erik Schulenburg</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 139-140</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957700" title="DOI">10.1145/1957656.1957700</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957700&ftid=929273&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow44" style="display:inline;"><br /><div style="display:inline">Human-robot interaction in a shared workspace permits and often even requires physical contact between humans and robots. A key technology to ensure that physical human robot interaction is safe is to monitor contact forces by providing the robot with ...</div></span>
          <span id="toHide44" style="display:none;"><br /><div style="display:inline"><p>Human-robot interaction in a shared workspace permits and often even requires physical contact between humans and robots. A key technology to ensure that physical human robot interaction is safe is to monitor contact forces by providing the robot with a tactile sensor as an artificial skin.</p> <p>This paper introduces a pressure sensitive skin that can be adapted to complex geometries and offers reliable contact measurements on the entire robot body. Equipped with integrated cushioning elements the sensitive skin can reduce the risk of dangerous injuries in physical human-robot interaction. Beside safety related functions, the sensitive skin offers touch based robot motion control which simplifies human-robot interaction.</p></div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957701&CFID=105753156&CFTOKEN=18642945">The chanty bear: a new application for hri research</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100237137&CFID=105753156&CFTOKEN=18642945">Kotaro Funakoshi</a>, 
                        <a href="author_page.cfm?id=81482652511&CFID=105753156&CFTOKEN=18642945">Tomoya Mizumoto</a>, 
                        <a href="author_page.cfm?id=81100522453&CFID=105753156&CFTOKEN=18642945">Ryo Nagata</a>, 
                        <a href="author_page.cfm?id=81100633290&CFID=105753156&CFTOKEN=18642945">Mikio Nakano</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 141-142</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957701" title="DOI">10.1145/1957656.1957701</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957701&ftid=929274&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow45" style="display:inline;"><br /><div style="display:inline">This paper presents yet another English-teaching robot, while putting emphasis on the merits which are offered by second language education to human robot interaction (HRI) research. The chanty bear, our prototype robot based on a rhythmic teaching method ...</div></span>
          <span id="toHide45" style="display:none;"><br /><div style="display:inline"><p>This paper presents yet another English-teaching robot, while putting emphasis on the merits which are offered by second language education to human robot interaction (HRI) research. The chanty bear, our prototype robot based on a rhythmic teaching method of English called Jazz Chants is introduced.</p></div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957702&CFID=105753156&CFTOKEN=18642945">A case for low-dose robotics in autism therapy</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350575550&CFID=105753156&CFTOKEN=18642945">Michael A. Goodrich</a>, 
                        <a href="author_page.cfm?id=81482646174&CFID=105753156&CFTOKEN=18642945">Mark A. Colton</a>, 
                        <a href="author_page.cfm?id=81482661495&CFID=105753156&CFTOKEN=18642945">Bonnie Brinton</a>, 
                        <a href="author_page.cfm?id=81482659972&CFID=105753156&CFTOKEN=18642945">Martin Fujiki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 143-144</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957702" title="DOI">10.1145/1957656.1957702</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957702&ftid=926458&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow46" style="display:inline;"><br /><div style="display:inline">Robots appear to be engaging to many children with autism, and evidence suggests that engagement can facilitate social interaction not only between child and robot but also between child and another human. To date, no objective evidence has established ...</div></span>
          <span id="toHide46" style="display:none;"><br /><div style="display:inline"><p>Robots appear to be engaging to many children with autism, and evidence suggests that engagement can facilitate social interaction not only between child and robot but also between child and another human. To date, no objective evidence has established a link between short-term child-robot interactions and long-term child-human interactions. We report on a therapy model that uses a robot in no more than 20% of available therapy time, and describe how a humanoid robot can be used during that limited time to promote generalizable child-human interactions. Preliminary evidence indicates that such low-dose robotics can promote positive child-human interactions.</p></div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957703&CFID=105753156&CFTOKEN=18642945">Learning from failure: extended abstract</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482653675&CFID=105753156&CFTOKEN=18642945">Daniel H. Grollman</a>, 
                        <a href="author_page.cfm?id=81100342762&CFID=105753156&CFTOKEN=18642945">Aude G. Billard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 145-146</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957703" title="DOI">10.1145/1957656.1957703</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957703&ftid=929275&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow47" style="display:inline;"><br /><div style="display:inline">In the canonical Robot Learning from Demonstration scenario a robot observes performances of a task and then develops an autonomous controller. Current work acknowledges that humans may be suboptimal demonstrators and refines the controller for improved ...</div></span>
          <span id="toHide47" style="display:none;"><br /><div style="display:inline"><p>In the canonical Robot Learning from Demonstration scenario a robot observes performances of a task and then develops an autonomous controller. Current work acknowledges that humans may be suboptimal demonstrators and refines the controller for improved performance. However, there is still an assumption that the demonstrations are successful examples of the task. We here consider the possibility that the human has failed, and propose a model to minimize the possibility of the robot making the same mistakes.</p></div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957704&CFID=105753156&CFTOKEN=18642945">Exploring the influence of age, gender, education and computer experience on robot acceptance by older adults</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350587500&CFID=105753156&CFTOKEN=18642945">Marcel Heerink</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 147-148</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957704" title="DOI">10.1145/1957656.1957704</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957704&ftid=929276&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow48" style="display:inline;"><br /><div style="display:inline">It is generally recognized that non perceptual factors like age, gender, education and computer experience can have a moderating effect on how perception of a technology leads to acceptance of it. In our present research we are exploring the influence ...</div></span>
          <span id="toHide48" style="display:none;"><br /><div style="display:inline"><p>It is generally recognized that non perceptual factors like age, gender, education and computer experience can have a moderating effect on how perception of a technology leads to acceptance of it. In our present research we are exploring the influence of these factors on the acceptance of assistive social robots by older adults. In this short paper we discuss the results of a user study in which a movie of an elderly person using a social assistive robot was shown to older adults. The analysis of the responses give a first indication on if and how these factors relate to the perceptual processes that lead to acceptance.</p></div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957705&CFID=105753156&CFTOKEN=18642945">A memory game for human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482662639&CFID=105753156&CFTOKEN=18642945">Sergio Hernandez-Mendez</a>, 
                        <a href="author_page.cfm?id=81482662006&CFID=105753156&CFTOKEN=18642945">Luis Alberto Morgado-Ramirez</a>, 
                        <a href="author_page.cfm?id=81456636093&CFID=105753156&CFTOKEN=18642945">Ana Cristina Ramirez-Hernandez</a>, 
                        <a href="author_page.cfm?id=81482649174&CFID=105753156&CFTOKEN=18642945">Luis F. Marin-Urias</a>, 
                        <a href="author_page.cfm?id=81100357384&CFID=105753156&CFTOKEN=18642945">Antonio Marin-Hernandez</a>, 
                        <a href="author_page.cfm?id=81384619250&CFID=105753156&CFTOKEN=18642945">Fernando Montes-Gonzalez</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 149-150</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957705" title="DOI">10.1145/1957656.1957705</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957705&ftid=929277&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow49" style="display:inline;"><br /><div style="display:inline">As a part of the final goal of introducing robots in the human environment, it also comes the problem of the close interaction between these two beings. Close cooperative activities illustrate well this problem and rises new challenges to solve. In this ...</div></span>
          <span id="toHide49" style="display:none;"><br /><div style="display:inline"><p>As a part of the final goal of introducing robots in the human environment, it also comes the problem of the close interaction between these two beings. Close cooperative activities illustrate well this problem and rises new challenges to solve.</p> <p>In this report we show the current work of the development of a system for acquiring information directly from human focused on a classic interaction game called "Simon Says".</p></div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957706&CFID=105753156&CFTOKEN=18642945">Tele-operation between USA and Japan using humanoid robot hand/arm</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482652972&CFID=105753156&CFTOKEN=18642945">Makoto Honda</a>, 
                        <a href="author_page.cfm?id=81414611027&CFID=105753156&CFTOKEN=18642945">Takanori Miyoshi</a>, 
                        <a href="author_page.cfm?id=81343495437&CFID=105753156&CFTOKEN=18642945">Takashi Imamura</a>, 
                        <a href="author_page.cfm?id=81339520632&CFID=105753156&CFTOKEN=18642945">Masayuki Okabe</a>, 
                        <a href="author_page.cfm?id=81482657933&CFID=105753156&CFTOKEN=18642945">Faisal M. Yazadi</a>, 
                        <a href="author_page.cfm?id=81327491707&CFID=105753156&CFTOKEN=18642945">Kazuhiko Terashima</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 151-152</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957706" title="DOI">10.1145/1957656.1957706</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957706&ftid=929278&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=1957706&ftid=929279&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow50" style="display:inline;"><br /><div style="display:inline">This paper presents a tele-control system constructed by a robot hand/arm and operator. The angle of the robot hand is controlled by the angle of the operator's finger, and the operator feels the environmental force, as detected by the touch sensor of ...</div></span>
          <span id="toHide50" style="display:none;"><br /><div style="display:inline"><p>This paper presents a tele-control system constructed by a robot hand/arm and operator. The angle of the robot hand is controlled by the angle of the operator's finger, and the operator feels the environmental force, as detected by the touch sensor of the robot hand, constituting so-called bilateral master/slave control. The position of the robot arm is controlled by the position of the operator's arm.</p> <p>So far, there has been little research using a multi-fingered humanoid robot hand in a network environment where communication delay exists. The purpose of our study is to achieve tele-operation between an operator's hand/arm and a multi-fingered humanoid robot hand/arm with delayed time. Therefore, this study constructs a system that can grasp and manipulate objects stably, despite the communication delay.</p> <p>In the experiments, the operator operates humanoid robot hand/arm of Toyohashi University of Technology from USA. The operator grasp and moves an object in Japan by operating the humanoid robot hand/arm. The staff in Japan gives the operation instruction to the operator in USA during experiment. The operator operates robot according to the voice. The experimental results show that this system can grasp and manipulate objects stably, despite the communication delay. In addition, the operator grasped the object using a multi-fingered humanoid robot hand/arm by master/slave control feeling fingertip force by tele-operation between USA and Japan.</p></div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957707&CFID=105753156&CFTOKEN=18642945">Universal robots as 'solutions' to wicked problems: debunking a robotic myth</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309488890&CFID=105753156&CFTOKEN=18642945">Mattias Jacobsson</a>, 
                        <a href="author_page.cfm?id=81328487965&CFID=105753156&CFTOKEN=18642945">Henriette Cramer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 153-154</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957707" title="DOI">10.1145/1957656.1957707</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957707&ftid=929280&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow51" style="display:inline;"><br /><div style="display:inline">This work in progress discusses a persistent myth about robots, namely that 'future robots will be universal solutions', or in other words that robots should tackle many complex tasks and situations. In our approach we consider whether this is a case ...</div></span>
          <span id="toHide51" style="display:none;"><br /><div style="display:inline"><p>This work in progress discusses a persistent myth about robots, namely that 'future robots will be universal solutions', or in other words that robots should tackle many complex tasks and situations. In our approach we consider whether this is a case of posing robots as solutions to wicked problems or if robots can be considered wicked design problems in themselves. At the same time we make an argument for adopting a research through design approach. Our stance suggests that by viewing robots as composed of design materials we can sensitively address and in the long run perhaps even avoid wicked problems related to robotics.</p></div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957708&CFID=105753156&CFTOKEN=18642945">Experience centred design for a robotic eating aid</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482657863&CFID=105753156&CFTOKEN=18642945">Javier Jim&#233;nez Villarreal</a>, 
                        <a href="author_page.cfm?id=81482643968&CFID=105753156&CFTOKEN=18642945">Sara Ljungblad</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 155-156</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957708" title="DOI">10.1145/1957656.1957708</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957708&ftid=929281&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow52" style="display:inline;"><br /><div style="display:inline">We discuss how an experience centred approach to robotic design might lead to new design spaces and products that are more engaging and better meet users' needs and lifestyles. To support the statement, we present preliminary data from a long-term user ...</div></span>
          <span id="toHide52" style="display:none;"><br /><div style="display:inline"><p>We discuss how an experience centred approach to robotic design might lead to new design spaces and products that are more engaging and better meet users' needs and lifestyles. To support the statement, we present preliminary data from a long-term user study on an eating aid robot.</p></div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957709&CFID=105753156&CFTOKEN=18642945">Upper-limb exercises for stroke patients through the direct engagement of an embodied agent</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482657058&CFID=105753156&CFTOKEN=18642945">Hee-Tae Jung</a>, 
                        <a href="author_page.cfm?id=81482652593&CFID=105753156&CFTOKEN=18642945">Jennifer Baird</a>, 
                        <a href="author_page.cfm?id=81482653031&CFID=105753156&CFTOKEN=18642945">Yu-Kyong Choe</a>, 
                        <a href="author_page.cfm?id=81100500609&CFID=105753156&CFTOKEN=18642945">Roderic A. Grupen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 157-158</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957709" title="DOI">10.1145/1957656.1957709</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957709&ftid=988316&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow53" style="display:inline;"><br /><div style="display:inline">In this case study, we examine the functional utility of an embodied agent as an interactive medium in stroke rehab. A set of physical rehab exercises is conducted through the direct engagement of an embodied agent, the uBot-5. Based on the preliminary ...</div></span>
          <span id="toHide53" style="display:none;"><br /><div style="display:inline"><p>In this case study, we examine the functional utility of an embodied agent as an interactive medium in stroke rehab. A set of physical rehab exercises is conducted through the direct engagement of an embodied agent, the uBot-5. Based on the preliminary data, we argue that a general-purpose embodied agent has a potential to functionally complement human therapists in providing rehab to stroke patients.</p></div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957710&CFID=105753156&CFTOKEN=18642945">The new ontological category hypothesis in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81408592441&CFID=105753156&CFTOKEN=18642945">Peter H. Kahn, Jr.</a>, 
                        <a href="author_page.cfm?id=81456638149&CFID=105753156&CFTOKEN=18642945">Aimee L. Reichert</a>, 
                        <a href="author_page.cfm?id=81456635299&CFID=105753156&CFTOKEN=18642945">Heather E. Gary</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105753156&CFTOKEN=18642945">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105753156&CFTOKEN=18642945">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81456605597&CFID=105753156&CFTOKEN=18642945">Solace Shen</a>, 
                        <a href="author_page.cfm?id=81350569693&CFID=105753156&CFTOKEN=18642945">Jolina H. Ruckert</a>, 
                        <a href="author_page.cfm?id=81100631042&CFID=105753156&CFTOKEN=18642945">Brian Gill</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 159-160</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957710" title="DOI">10.1145/1957656.1957710</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957710&ftid=930944&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow54" style="display:inline;"><br /><div style="display:inline">This paper discusses converging evidence to support the hypothesis that personified robots and other embodied personified computational systems may represent a new ontological category, where ontology refers to basic categories of being, and ways of ...</div></span>
          <span id="toHide54" style="display:none;"><br /><div style="display:inline"><p>This paper discusses converging evidence to support the hypothesis that personified robots and other embodied personified computational systems may represent a new ontological category, where ontology refers to basic categories of being, and ways of distinguishing them.</p></div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957711&CFID=105753156&CFTOKEN=18642945">RIDE: mixed-mode control for mobile robot teams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482658601&CFID=105753156&CFTOKEN=18642945">Erik Karulf</a>, 
                        <a href="author_page.cfm?id=81482658188&CFID=105753156&CFTOKEN=18642945">Marshall Strother</a>, 
                        <a href="author_page.cfm?id=81482660347&CFID=105753156&CFTOKEN=18642945">Parker Dunton</a>, 
                        <a href="author_page.cfm?id=81100105515&CFID=105753156&CFTOKEN=18642945">William D. Smart</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 161-162</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957711" title="DOI">10.1145/1957656.1957711</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957711&ftid=930945&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957711&ftid=930946&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957712&CFID=105753156&CFTOKEN=18642945">User recognition based on continuous monitoring and tracking</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482651643&CFID=105753156&CFTOKEN=18642945">Hye-Jin Kim</a>, 
                        <a href="author_page.cfm?id=81343509324&CFID=105753156&CFTOKEN=18642945">Ho Sub Yoon</a>, 
                        <a href="author_page.cfm?id=81406599703&CFID=105753156&CFTOKEN=18642945">Jae Hong Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 163-164</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957712" title="DOI">10.1145/1957656.1957712</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957712&ftid=930947&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow56" style="display:inline;"><br /><div style="display:inline">This paper presents a user recognition system, using face, height, and clothes color features under the special assumption that is a user is monitored and tracked. In real human-robot interaction situation, all information cannot be provided at the same ...</div></span>
          <span id="toHide56" style="display:none;"><br /><div style="display:inline"><p>This paper presents a user recognition system, using face, height, and clothes color features under the special assumption that is a user is monitored and tracked. In real human-robot interaction situation, all information cannot be provided at the same time and some parts of frames in a video have no clues at all. In the proposed system, tracking is an important feature to recognize a user because data in the previous frames can be utilized. We propose an information update method that efficiently updates similarity results. This system is tested using the movie clips acquired under the unconstrained environment including illumination variation, several distance from a camera to the user, and various view types of human body.</p></div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957713&CFID=105753156&CFTOKEN=18642945">Terrain-adaptive and user-friendly remote control of wheel-track hybrid mobile robot platform</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456617523&CFID=105753156&CFTOKEN=18642945">Yoon-Gu Kim</a>, 
                        <a href="author_page.cfm?id=81482651312&CFID=105753156&CFTOKEN=18642945">Jeong-Hwan Kwak</a>, 
                        <a href="author_page.cfm?id=81447600709&CFID=105753156&CFTOKEN=18642945">Jinung An</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 165-166</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957713" title="DOI">10.1145/1957656.1957713</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957713&ftid=930948&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow57" style="display:inline;"><br /><div style="display:inline">Various robot platforms have been designed and developed to perform given tasks in a hazardous environment for surveillance, reconnaissance, search and rescue, etc. We considered a terrain-adaptive and transformable hybrid robot platform that is equipped ...</div></span>
          <span id="toHide57" style="display:none;"><br /><div style="display:inline"><p>Various robot platforms have been designed and developed to perform given tasks in a hazardous environment for surveillance, reconnaissance, search and rescue, etc. We considered a terrain-adaptive and transformable hybrid robot platform that is equipped for rapid navigation on flat floors and good performance in overcoming stairs or obstacles. The mode transition is determined and implemented by adaptive driving mode control of the mobile robot. The terrain-adaptive and user-friendly remote control was verified through its navigation performance experiments in real and test-bed environments.</p></div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957714&CFID=105753156&CFTOKEN=18642945">Assisted-care robot dealing with multiple requests in multi-party settings</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100604144&CFID=105753156&CFTOKEN=18642945">Yoshinori Kobayashi</a>, 
                        <a href="author_page.cfm?id=81482659987&CFID=105753156&CFTOKEN=18642945">Masahiko Gyoda</a>, 
                        <a href="author_page.cfm?id=81482655012&CFID=105753156&CFTOKEN=18642945">Tomoya Tabata</a>, 
                        <a href="author_page.cfm?id=81100082332&CFID=105753156&CFTOKEN=18642945">Yoshinori Kuno</a>, 
                        <a href="author_page.cfm?id=81350572857&CFID=105753156&CFTOKEN=18642945">Keiichi Yamazaki</a>, 
                        <a href="author_page.cfm?id=81482658032&CFID=105753156&CFTOKEN=18642945">Momoyo Shibuya</a>, 
                        <a href="author_page.cfm?id=81482647493&CFID=105753156&CFTOKEN=18642945">Yukiko Seki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 167-168</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957714" title="DOI">10.1145/1957656.1957714</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957714&ftid=930949&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=1957714&ftid=930950&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow58" style="display:inline;"><br /><div style="display:inline">This paper presents our ongoing work developing service robots that provide assisted-care, such as serving tea to the elderly in care facilities. In multi-party settings, a robot is required to be able to deal with requests from multiple individuals ...</div></span>
          <span id="toHide58" style="display:none;"><br /><div style="display:inline"><p>This paper presents our ongoing work developing service robots that provide assisted-care, such as serving tea to the elderly in care facilities. In multi-party settings, a robot is required to be able to deal with requests from multiple individuals simultaneously. In particular, when the service robot is concentrating on taking care of a specific person, other people who want to initiate interaction may feel frustrated with the robot. To a considerable extent this may be caused by the robot's behavior, which does not indicate any response to subsequent requests while preoccupied with the first. Therefore, we developed a robot that can display acknowledgement, in a socially acceptable manner, to each person who wants to initiate interaction. In this paper we focus on the task of tea-serving, and introduce a robot able to bring tea to multiple users while accepting multiple requests. The robot can detect a person's request (raising their hand) and move around people using its localization system. When the robot detects a person's request while serving tea to another person, it displays its acknowledgement by indicating "Please wait" through a nonverbal action. Because it can indicate its acknowledgement of their requests socially, people will likely feel more satisfied with the robot even when it cannot immediately address their needs.</p></div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957715&CFID=105753156&CFTOKEN=18642945">From cartoons to robots part 2: facial regions as cues to recognize emotions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309512404&CFID=105753156&CFTOKEN=18642945">Tomoko Koda</a>, 
                        <a href="author_page.cfm?id=81100237640&CFID=105753156&CFTOKEN=18642945">Zsofia Ruttkay</a>, 
                        <a href="author_page.cfm?id=81482643244&CFID=105753156&CFTOKEN=18642945">Tomoharu Sano</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 169-170</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957715" title="DOI">10.1145/1957656.1957715</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957715&ftid=930951&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow59" style="display:inline;"><br /><div style="display:inline">This paper reports a preliminary result of a cross-cultural study on the facial regions as cues to recognize virtual agents' facial expressions. We believe providing research results on the perception of cartoonish virtual agents' facial expressions ...</div></span>
          <span id="toHide59" style="display:none;"><br /><div style="display:inline"><p>This paper reports a preliminary result of a cross-cultural study on the facial regions as cues to recognize virtual agents' facial expressions. We believe providing research results on the perception of cartoonish virtual agents' facial expressions to HRI research community is meaningful in order to minimize the effort to develop robot's facial expressions. The result implies Japanese weighed facial cues more heavily in the eye regions than Hungarians, who weighed facial cues more heavily in the mouth region than Japanese.</p></div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957716&CFID=105753156&CFTOKEN=18642945">Gaze motion planning for android robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482641538&CFID=105753156&CFTOKEN=18642945">Yutaka Kondo</a>, 
                        <a href="author_page.cfm?id=81482642076&CFID=105753156&CFTOKEN=18642945">Masato Kawamura</a>, 
                        <a href="author_page.cfm?id=81330498869&CFID=105753156&CFTOKEN=18642945">Kentaro Takemura</a>, 
                        <a href="author_page.cfm?id=81332530878&CFID=105753156&CFTOKEN=18642945">Jun Takamatsu</a>, 
                        <a href="author_page.cfm?id=81100012453&CFID=105753156&CFTOKEN=18642945">Tsukasa Ogasawara</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 171-172</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957716" title="DOI">10.1145/1957656.1957716</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957716&ftid=930952&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow60" style="display:inline;"><br /><div style="display:inline">Androids are potentially required to show human-like behavior, because their appearance resembles humans' physical features. Therefore, we propose a gaze motion planning method. Within this method, we control the convergence of eyes and the ratio of ...</div></span>
          <span id="toHide60" style="display:none;"><br /><div style="display:inline"><p>Androids are potentially required to show human-like behavior, because their appearance resembles humans' physical features. Therefore, we propose a gaze motion planning method. Within this method, we control the convergence of eyes and the ratio of eye angle to head angle, which leads to a more precise estimation of gaze direction. We implemented our method on the android <i>Actroid-SIT</i> and conducted experiments for evaluation of the effects of our method. Through these experiments, we achieved a common guidance for androids when planning more precise gaze motion.</p></div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957717&CFID=105753156&CFTOKEN=18642945">Perception of visual scene and intonation patterns of robot utterances</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310489559&CFID=105753156&CFTOKEN=18642945">Ivana Kruijff-Korbayov&#225;</a>, 
                        <a href="author_page.cfm?id=81482662078&CFID=105753156&CFTOKEN=18642945">Raveesh Meena</a>, 
                        <a href="author_page.cfm?id=81482659220&CFID=105753156&CFTOKEN=18642945">Pirita Pyykk&#246;nen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 173-174</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957717" title="DOI">10.1145/1957656.1957717</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957717&ftid=930953&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow61" style="display:inline;"><br /><div style="display:inline">Assigning intonation to dialogue system output in a way that reflects relationships between entities in the discourse context can enhance the acceptability of system utterances. Previous research concentrated on the role of linguistic context; dialogue ...</div></span>
          <span id="toHide61" style="display:none;"><br /><div style="display:inline"><p>Assigning intonation to dialogue system output in a way that reflects relationships between entities in the discourse context can enhance the acceptability of system utterances. Previous research concentrated on the role of linguistic context; dialogue <i>situatedness</i> and the role of visual context in determining accent placement has not been studied. We present an experimental study on the influence of visual context on the perception of nuclear accent placement in synthesized clarification requests. We found that utterances are perceived as appropriate more often when the visual scene licenses the nuclear accent placement than when it does not.</p></div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957718&CFID=105753156&CFTOKEN=18642945">Towards proactive assistant robots for human assembly tasks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482646029&CFID=105753156&CFTOKEN=18642945">Woo Young Kwon</a>, 
                        <a href="author_page.cfm?id=81100623849&CFID=105753156&CFTOKEN=18642945">Il Hong Suh</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 175-176</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957718" title="DOI">10.1145/1957656.1957718</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957718&ftid=930954&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow62" style="display:inline;"><br /><div style="display:inline">In this paper, we propose a proactive assistant robot for human assembly tasks. In order to predict future events of human activities such as requesting parts for assembly, we use the temporal Bayesian network that can infer both causal probability and ...</div></span>
          <span id="toHide62" style="display:none;"><br /><div style="display:inline"><p>In this paper, we propose a proactive assistant robot for human assembly tasks. In order to predict future events of human activities such as requesting parts for assembly, we use the temporal Bayesian network that can infer both causal probability and temporal distribution of an conditional event. Based on the temporal Bayesian network model of an human assembly task, we also show that the proactive assistant robot make human-robot-interaction quickly by temporal prediction of an event.</p></div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957719&CFID=105753156&CFTOKEN=18642945">A panorama interface for telepresence robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482662108&CFID=105753156&CFTOKEN=18642945">Daniel A. Lazewatsky</a>, 
                        <a href="author_page.cfm?id=81100105515&CFID=105753156&CFTOKEN=18642945">William D. Smart</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 177-178</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957719" title="DOI">10.1145/1957656.1957719</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957719&ftid=930955&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow63" style="display:inline;"><br /><div style="display:inline">Telepresence robots are becoming increasingly popular and are increasingly ready to enter use in the real world as stand-ins for remote humans. It is useful, but currently uncommon, to provide the human operator with an approximation of peripheral vision ...</div></span>
          <span id="toHide63" style="display:none;"><br /><div style="display:inline"><p>Telepresence robots are becoming increasingly popular and are increasingly ready to enter use in the real world as stand-ins for remote humans. It is useful, but currently uncommon, to provide the human operator with an approximation of peripheral vision and the ability to saccade around the scene. We have developed an interface which provides peripheral vision to a remote operator by using a motorized pan-tilt camera to create a panorama, and enables the operator to move the camera's gaze within that panorama.</p></div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957720&CFID=105753156&CFTOKEN=18642945">Predictability or adaptivity?: designing robot handoffs modeled from trained dogs and people</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414592368&CFID=105753156&CFTOKEN=18642945">Min Kyung Lee</a>, 
                        <a href="author_page.cfm?id=81100492013&CFID=105753156&CFTOKEN=18642945">Jodi Forlizzi</a>, 
                        <a href="author_page.cfm?id=81100476487&CFID=105753156&CFTOKEN=18642945">Sara Kiesler</a>, 
                        <a href="author_page.cfm?id=81414594397&CFID=105753156&CFTOKEN=18642945">Maya Cakmak</a>, 
                        <a href="author_page.cfm?id=81453611472&CFID=105753156&CFTOKEN=18642945">Siddhartha Srinivasa</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 179-180</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957720" title="DOI">10.1145/1957656.1957720</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957720&ftid=930956&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow64" style="display:inline;"><br /><div style="display:inline">One goal of assistive robotics is to design interactive robots that can help disabled people with tasks such as fetching objects. When people do this task, they coordinate their movements closely with receivers. We investigated how a robot should fetch ...</div></span>
          <span id="toHide64" style="display:none;"><br /><div style="display:inline"><p>One goal of assistive robotics is to design interactive robots that can help disabled people with tasks such as fetching objects. When people do this task, they coordinate their movements closely with receivers. We investigated how a robot should fetch and give household objects to a person. To develop a model for the robot, we first studied trained dogs and person-to-person handoffs. Our findings suggest two models of handoff that differ in their predictability and adaptivity.</p></div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957721&CFID=105753156&CFTOKEN=18642945">Understanding users' perception of privacy in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414592368&CFID=105753156&CFTOKEN=18642945">Min Kyung Lee</a>, 
                        <a href="author_page.cfm?id=81100640633&CFID=105753156&CFTOKEN=18642945">Karen P. Tang</a>, 
                        <a href="author_page.cfm?id=81100492013&CFID=105753156&CFTOKEN=18642945">Jodi Forlizzi</a>, 
                        <a href="author_page.cfm?id=81100476487&CFID=105753156&CFTOKEN=18642945">Sara Kiesler</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 181-182</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957721" title="DOI">10.1145/1957656.1957721</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957721&ftid=930957&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow65" style="display:inline;"><br /><div style="display:inline">Previous research has shown that design features that support privacy are essential for new technologies looking to gain widespread adoption. As such, privacy-sensitive design will be important for the adoption of social robots, as they could introduce ...</div></span>
          <span id="toHide65" style="display:none;"><br /><div style="display:inline"><p>Previous research has shown that design features that support privacy are essential for new technologies looking to gain widespread adoption. As such, privacy-sensitive design will be important for the adoption of social robots, as they could introduce new types of privacy risks to users. In this paper, we report findings from our preliminary study on users' perceptions and attitudes toward privacy in human-robot interaction, based on interviews that we conducted about a workplace social robot.</p></div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957722&CFID=105753156&CFTOKEN=18642945">Utilitarian vs. hedonic robots: role of parasocial tendency and anthropomorphism in shaping user attitudes</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482644962&CFID=105753156&CFTOKEN=18642945">Namseok Lee</a>, 
                        <a href="author_page.cfm?id=81482642243&CFID=105753156&CFTOKEN=18642945">Hochul Shin</a>, 
                        <a href="author_page.cfm?id=81456626968&CFID=105753156&CFTOKEN=18642945">S. Shyam Sundar</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 183-184</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957722" title="DOI">10.1145/1957656.1957722</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957722&ftid=930958&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow66" style="display:inline;"><br /><div style="display:inline">This study examines the differential effects of hedonic vs. utilitarian robots, using a between-subjects experimental design, whereby 48 college students in Korea were randomly assigned to interact with either a Pleo (Dinosaur robot) or a Roomba (vacuum-cleaning ...</div></span>
          <span id="toHide66" style="display:none;"><br /><div style="display:inline"><p>This study examines the differential effects of hedonic vs. utilitarian robots, using a between-subjects experimental design, whereby 48 college students in Korea were randomly assigned to interact with either a Pleo (Dinosaur robot) or a Roomba (vacuum-cleaning robot). Results revealed that hedonic robot (HR) users perceived more enjoyment than utilitarian robot (UR) users, whereas UR users perceived more usefulness and ease-of-use than HR users. Users with high tendency for parasocial interaction (PSI) and high anthropomorphism had more positive attitudes towards robots than their counterparts with low levels of these traits. HR users with high anthropomorphism and PSI had the most positive attitudes than all other combinations of variables. These results indicate that individual differences play a significant moderating role on user attitudes toward hedonic and utilitarian robots. The results of this study suggest that robot developers and marketers should take seriously the labeling of robots as hedonic or utilitarian, and also consider users' individual differences in order to maximize benefits of human-robot interactions.</p></div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957723&CFID=105753156&CFTOKEN=18642945">Incremental learning of primitive skills from demonstration of a task</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453610071&CFID=105753156&CFTOKEN=18642945">Sang Hyoung Lee</a>, 
                        <a href="author_page.cfm?id=81482651642&CFID=105753156&CFTOKEN=18642945">Hyung Kyu Kim</a>, 
                        <a href="author_page.cfm?id=81100623849&CFID=105753156&CFTOKEN=18642945">Il Hong Suh</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 185-186</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957723" title="DOI">10.1145/1957656.1957723</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957723&ftid=930959&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow67" style="display:inline;"><br /><div style="display:inline">In this work, we propose methods for automatically generating primitive skills from demonstration of a task. Additionally, we propose methods for improving the existing primitive skills and adding the new primitive skills incrementally and automatically. ...</div></span>
          <span id="toHide67" style="display:none;"><br /><div style="display:inline"><p>In this work, we propose methods for automatically generating primitive skills from demonstration of a task. Additionally, we propose methods for improving the existing primitive skills and adding the new primitive skills incrementally and automatically. To validate our proposed methods, we present experimental results of a human-like robot handling three gestures and a task for making coffee.</p></div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957724&CFID=105753156&CFTOKEN=18642945">Hitting a robot vs. hitting a human: is it the same?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482642347&CFID=105753156&CFTOKEN=18642945">Sau-lai Lee</a>, 
                        <a href="author_page.cfm?id=81482648287&CFID=105753156&CFTOKEN=18642945">Ivy Yee-man Lau</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 187-188</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957724" title="DOI">10.1145/1957656.1957724</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957724&ftid=930960&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow68" style="display:inline;"><br /><div style="display:inline">The present project aimed to study how people make moral judgment for human versus robot behaviors. Ten transgression scenarios were presented to the participants with either a human or a robot as the perpetrator or the victim. Results showed that most ...</div></span>
          <span id="toHide68" style="display:none;"><br /><div style="display:inline"><p>The present project aimed to study how people make moral judgment for human versus robot behaviors. Ten transgression scenarios were presented to the participants with either a human or a robot as the perpetrator or the victim. Results showed that most of the transgressions were perceived as less immoral when it was acted on a robot than on a human. Moral judgments for human behaviors were more intuitive and emotion based. Moral judgments for robot behaviors involve both intuition and cognitive reasoning. Possible psychological causes were discussed.</p></div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957725&CFID=105753156&CFTOKEN=18642945">Recognition and incremental learning of scenario-oriented human behavior patterns by two threshold models</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456610482&CFID=105753156&CFTOKEN=18642945">Gi Hyun Lim</a>, 
                        <a href="author_page.cfm?id=81482645698&CFID=105753156&CFTOKEN=18642945">Byoungjun Chung</a>, 
                        <a href="author_page.cfm?id=81100623849&CFID=105753156&CFTOKEN=18642945">Il Hong Suh</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 189-190</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957725" title="DOI">10.1145/1957656.1957725</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957725&ftid=930961&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow69" style="display:inline;"><br /><div style="display:inline">Two HMM-based threshold models are suggested for recognition and incremental learning of scenario-oriented human behavior patterns. One is the expected behavior threshold model to discriminate if a monitored behavior pattern is normal or not. The other ...</div></span>
          <span id="toHide69" style="display:none;"><br /><div style="display:inline"><p>Two HMM-based threshold models are suggested for recognition and incremental learning of scenario-oriented human behavior patterns. One is the expected behavior threshold model to discriminate if a monitored behavior pattern is normal or not. The other model is the registered behavior threshold model to detect whether such behavior pattern is already learned. If a behavior patten is detected as a new one, an HMM is generated to represent the pattern, and then the HMM is used to update behavior clusters by hierarchical clustering process.</p></div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957726&CFID=105753156&CFTOKEN=18642945">Beyond speculative ethics in HRI?: ethical considerations and the relation to empirical data</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100194686&CFID=105753156&CFTOKEN=18642945">Sara Ljungblad</a>, 
                        <a href="author_page.cfm?id=81317498340&CFID=105753156&CFTOKEN=18642945">Stina Nylander</a>, 
                        <a href="author_page.cfm?id=81314494924&CFID=105753156&CFTOKEN=18642945">Mie N&#248;rgaard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 191-192</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957726" title="DOI">10.1145/1957656.1957726</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957726&ftid=930962&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow70" style="display:inline;"><br /><div style="display:inline">We discuss the difference between understanding robot ethics as something that is grounded in philosophical ideas about a potential future design, and understanding robot ethics as something that is grounded in empirical data. We argue, that understanding ...</div></span>
          <span id="toHide70" style="display:none;"><br /><div style="display:inline"><p>We discuss the difference between understanding robot ethics as something that is grounded in philosophical ideas about a potential future design, and understanding robot ethics as something that is grounded in empirical data. We argue, that understanding "robots" as a relatively homogenous group of designs for which we can formulate general ethics may lead to a foresight of future robot designs that includes ideas and concerns that are not feasible or realistic. Our aim is to exemplify a complementing perspective, by shedding light on two different robotic designs. We discuss their relation to specific use practices and user experiences, and provide some early ethical reflections and design concerns.</p></div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957727&CFID=105753156&CFTOKEN=18642945">Team-based interactions with heterogeneous robots through a novel HRI software architecture</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482647232&CFID=105753156&CFTOKEN=18642945">Meghann Lomas</a>, 
                        <a href="author_page.cfm?id=81482660518&CFID=105753156&CFTOKEN=18642945">Vera Zaychik Moffitt</a>, 
                        <a href="author_page.cfm?id=81436595995&CFID=105753156&CFTOKEN=18642945">Patrick Craven</a>, 
                        <a href="author_page.cfm?id=81482646090&CFID=105753156&CFTOKEN=18642945">Ernest Vincent Cross, II</a>, 
                        <a href="author_page.cfm?id=81482646402&CFID=105753156&CFTOKEN=18642945">Jerry L. Franke</a>, 
                        <a href="author_page.cfm?id=81482648775&CFID=105753156&CFTOKEN=18642945">James S. Taylor</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 193-194</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957727" title="DOI">10.1145/1957656.1957727</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957727&ftid=930963&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow71" style="display:inline;"><br /><div style="display:inline">In this paper, we describe a Human-Robot Interface (HRI) software architecture designed to enable teams of operators to share tasking for multiple unmanned vehicles and systems. Many existing robotic systems are controlled using specially-designed interfaces, ...</div></span>
          <span id="toHide71" style="display:none;"><br /><div style="display:inline"><p>In this paper, we describe a Human-Robot Interface (HRI) software architecture designed to enable teams of operators to share tasking for multiple unmanned vehicles and systems. Many existing robotic systems are controlled using specially-designed interfaces, which becomes problematic for operator teams controlling multiple heterogeneous systems. We propose a solution that enables teams of operators to control multiple heterogeneous vehicles and systems using a common command and control environment. This environment supports task sharing and handoff and was shown in evaluations to improve team efficiency.</p></div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957728&CFID=105753156&CFTOKEN=18642945">The applicability of gricean maxims in social robotics polite dialogue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482658705&CFID=105753156&CFTOKEN=18642945">Qin En Looi</a>, 
                        <a href="author_page.cfm?id=81481640588&CFID=105753156&CFTOKEN=18642945">Swee Lan See</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 195-196</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957728" title="DOI">10.1145/1957656.1957728</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957728&ftid=930964&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow72" style="display:inline;"><br /><div style="display:inline">Human-robot interaction is the distinctive feature of social robotics; with the increasing pervasiveness of the social robots in our everyday lives, efforts should be made to improve the interaction. This study explores how Gricean conversational maxims ...</div></span>
          <span id="toHide72" style="display:none;"><br /><div style="display:inline"><p>Human-robot interaction is the distinctive feature of social robotics; with the increasing pervasiveness of the social robots in our everyday lives, efforts should be made to improve the interaction. This study explores how Gricean conversational maxims on politeness can be implemented to develop polite dialogue in social robotics, catering to the preferences of the user. Though results are preliminary, the maxims have shown to be promising guidelines in the development of dialogue. The study concludes by suggesting how end-user tests can be used to verify the applicability of these maxims.</p></div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957729&CFID=105753156&CFTOKEN=18642945">Polonius: a wizard of oz interface for HRI experiments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482645098&CFID=105753156&CFTOKEN=18642945">David V. Lu</a>, 
                        <a href="author_page.cfm?id=81100105515&CFID=105753156&CFTOKEN=18642945">William D. Smart</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 197-198</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957729" title="DOI">10.1145/1957656.1957729</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957729&ftid=930965&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow73" style="display:inline;"><br /><div style="display:inline">Polonius is a robot control interface designed for running Wizard of Oz style experiments. It is designed to be easy enough to be used by the non-programmer collaborators of roboticists. The program acts as an intermediary between the robot and a wizard ...</div></span>
          <span id="toHide73" style="display:none;"><br /><div style="display:inline"><p>Polonius is a robot control interface designed for running Wizard of Oz style experiments. It is designed to be easy enough to be used by the non-programmer collaborators of roboticists. The program acts as an intermediary between the robot and a wizard interacting with a GUI based on a pre-defined script. Polonius also eliminates the need for coding the video after experiments by integrating a robust logging system.</p></div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957730&CFID=105753156&CFTOKEN=18642945">Expressing emotions through robots: a case study using off-the-shelf programming interfaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482649232&CFID=105753156&CFTOKEN=18642945">Vimitha Manohar</a>, 
                        <a href="author_page.cfm?id=81482660020&CFID=105753156&CFTOKEN=18642945">Shamma al Marzooqi</a>, 
                        <a href="author_page.cfm?id=81309483016&CFID=105753156&CFTOKEN=18642945">Jacob W. Crandall</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 199-200</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957730" title="DOI">10.1145/1957656.1957730</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957730&ftid=930966&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow74" style="display:inline;"><br /><div style="display:inline">This paper explores how effectively users can encode emotions in robot behaviors using existing robot programming interfaces. Specifically, we analyze how programming interfaces for Nao and Pleo robots allow end-users to encode behaviors that express ...</div></span>
          <span id="toHide74" style="display:none;"><br /><div style="display:inline"><p>This paper explores how effectively users can encode emotions in robot behaviors using existing robot programming interfaces. Specifically, we analyze how programming interfaces for Nao and Pleo robots allow end-users to encode behaviors that express anger, sadness, happiness, and surprise. Via a series of user studies, we found that users were able to express emotions through these robots more effectively via verbal expressions than non-verbal expressions.</p></div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957731&CFID=105753156&CFTOKEN=18642945">Recognition of spatial dynamics for predicting social interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81442595982&CFID=105753156&CFTOKEN=18642945">Ross Mead</a>, 
                        <a href="author_page.cfm?id=81100179413&CFID=105753156&CFTOKEN=18642945">Amin Atrash</a>, 
                        <a href="author_page.cfm?id=81452618057&CFID=105753156&CFTOKEN=18642945">Maja J. Mataric</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 201-202</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957731" title="DOI">10.1145/1957656.1957731</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957731&ftid=930967&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow75" style="display:inline;"><br /><div style="display:inline">We present a user study and dataset designed and collected to analyze how humans use space in face-to-face interactions. In a proof-of-concept investigation into human spatial dynamics, a Hidden Markov Model (HMM) was trained over a subset of features ...</div></span>
          <span id="toHide75" style="display:none;"><br /><div style="display:inline"><p>We present a user study and dataset designed and collected to analyze how humans use space in face-to-face interactions. In a proof-of-concept investigation into human spatial dynamics, a Hidden Markov Model (HMM) was trained over a subset of features to recognize each of three interaction cues - initiation, acceptance, and termination - in both dyadic and triadic scenarios; these cues are useful in predicting transitions into, during, and out of multi-party social encounters. It is shown that the HMM approach performed twice as well as a weighted random classifier, supporting the feasibility of recognizing and predicting social behavior based on spatial features.</p></div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957732&CFID=105753156&CFTOKEN=18642945">Make your wishes to 'genie in the lamp': physical push with a socially intelligent robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81418592245&CFID=105753156&CFTOKEN=18642945">Hye-Jin Min</a>, 
                        <a href="author_page.cfm?id=81451594510&CFID=105753156&CFTOKEN=18642945">Jong C. Park</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 203-204</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957732" title="DOI">10.1145/1957656.1957732</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957732&ftid=930968&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow76" style="display:inline;"><br /><div style="display:inline">This paper proposes a robotic agent named 'Genie' that understands a user's wish and gives its possible answers on a social network platform. Once a potential wish is detected upon monitoring the text updates in the micro-blog of the user, the agent ...</div></span>
          <span id="toHide76" style="display:none;"><br /><div style="display:inline"><p>This paper proposes a robotic agent named 'Genie' that understands a user's wish and gives its possible answers on a social network platform. Once a potential wish is detected upon monitoring the text updates in the micro-blog of the user, the agent initiates a task to help the user with both NLP and metadata analysis. As an interaction scenario, we set the type of a robot as an agent that identifies wishful products by searching for and analyzing product information on the web. After an analysis of the vast amount of data, the agent provides possible answers to the user as a way of granting the wish that might require additional time and effort to achieve. In order to draw the user's attention, the agent makes a physical movement as a push notification with more user-friendliness.</p></div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957733&CFID=105753156&CFTOKEN=18642945">A communication structure for human-robot itinerary requests</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482659197&CFID=105753156&CFTOKEN=18642945">Nicole Mirnig</a>, 
                        <a href="author_page.cfm?id=81381602151&CFID=105753156&CFTOKEN=18642945">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81100481664&CFID=105753156&CFTOKEN=18642945">Manfred Tscheligi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 205-206</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957733" title="DOI">10.1145/1957656.1957733</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957733&ftid=930970&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow77" style="display:inline;"><br /><div style="display:inline">To analyze the formula for success of human communication, we examined dialogs between human interactors who were asking for directions in public place and extracted those elements that are responsible for making a dialog succeed or fail. Then, we tried ...</div></span>
          <span id="toHide77" style="display:none;"><br /><div style="display:inline"><p>To analyze the formula for success of human communication, we examined dialogs between human interactors who were asking for directions in public place and extracted those elements that are responsible for making a dialog succeed or fail. Then, we tried to rate the elements according to the grade of their influence. Based on this rating and on the Shannon &#38; Weaver model of communication, we created a communication structure for successful human-robot communication on which further research may be based to make human-robot communication as effective as possible.</p></div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957734&CFID=105753156&CFTOKEN=18642945">Cognitive objects for human-computer interaction and human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100071430&CFID=105753156&CFTOKEN=18642945">Andreas M&#246;ller</a>, 
                        <a href="author_page.cfm?id=81472654235&CFID=105753156&CFTOKEN=18642945">Luis Roalter</a>, 
                        <a href="author_page.cfm?id=81336490722&CFID=105753156&CFTOKEN=18642945">Matthias Kranz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 207-208</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957734" title="DOI">10.1145/1957656.1957734</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957734&ftid=930971&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow78" style="display:inline;"><br /><div style="display:inline">We introduce and define Cognitive Objects for human-robot interaction and human-computer interaction and disambiguate them against existing 'Smart Objects'. Cognitive Objects are physical real-world objects used in manipulation tasks by humans and robots. ...</div></span>
          <span id="toHide78" style="display:none;"><br /><div style="display:inline"><p>We introduce and define Cognitive Objects for human-robot interaction and human-computer interaction and disambiguate them against existing 'Smart Objects'. Cognitive Objects are physical real-world objects used in manipulation tasks by humans and robots. As such, they incorporate self-awareness, reduce ambiguity and uncertainty in object recognition, and provide services to both humans and robots during their usage in real-world environments.</p> <p>We distinguish Cognitive Objects from other 'Smart Objects' and computationally enriched artifacts, outline their characteristics, and describe their potential impact on human-computer and human-robot interaction with real-world objects.</p></div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957735&CFID=105753156&CFTOKEN=18642945">Inferring social gaze from conversational structure and timing</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100545387&CFID=105753156&CFTOKEN=18642945">Robin Murphy</a>, 
                        <a href="author_page.cfm?id=81482641267&CFID=105753156&CFTOKEN=18642945">Jessica Gonzales</a>, 
                        <a href="author_page.cfm?id=81456616934&CFID=105753156&CFTOKEN=18642945">Vasant Srinivasan</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 209-210</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957735" title="DOI">10.1145/1957656.1957735</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957735&ftid=930972&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow79" style="display:inline;"><br /><div style="display:inline">We have created a preliminary inference engine for generating gaze acts based on extracting the social context from conversational structure and timing in human-robot dialog.</div></span>
          <span id="toHide79" style="display:none;"><br /><div style="display:inline"><p>We have created a preliminary inference engine for generating gaze acts based on extracting the social context from conversational structure and timing in human-robot dialog.</p></div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957736&CFID=105753156&CFTOKEN=18642945">Exploring sketching for robot collaboration</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482647822&CFID=105753156&CFTOKEN=18642945">Matei Negulescu</a>, 
                        <a href="author_page.cfm?id=81100159960&CFID=105753156&CFTOKEN=18642945">Tetsunari Inamura</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 211-212</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957736" title="DOI">10.1145/1957656.1957736</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957736&ftid=930973&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow80" style="display:inline;"><br /><div style="display:inline">The collaboration between humans and robots can lessen the burden of automatic learning while performing difficult tasks. In this work, we explore sketching as a method to enable effective collaboration between human and robot. The system allows a human ...</div></span>
          <span id="toHide80" style="display:none;"><br /><div style="display:inline"><p>The collaboration between humans and robots can lessen the burden of automatic learning while performing difficult tasks. In this work, we explore sketching as a method to enable effective collaboration between human and robot. The system allows a human to contiguously interact with the robot to perform a task by allowing the sketching of the environment and specifying the affordances of objects and areas on the map.</p></div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957737&CFID=105753156&CFTOKEN=18642945">Exploring influences of robot anxiety into HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81336491870&CFID=105753156&CFTOKEN=18642945">Tatsuya Nomura</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105753156&CFTOKEN=18642945">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81414594070&CFID=105753156&CFTOKEN=18642945">Sachie Yamada</a>, 
                        <a href="author_page.cfm?id=81336493182&CFID=105753156&CFTOKEN=18642945">Tomohiro Suzuki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 213-214</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957737" title="DOI">10.1145/1957656.1957737</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957737&ftid=930974&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957738&CFID=105753156&CFTOKEN=18642945">Collaboration with an autonomous humanoid robot: a little gesture goes a long way</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81409594343&CFID=105753156&CFTOKEN=18642945">Kevin O'Brien</a>, 
                        <a href="author_page.cfm?id=81482643588&CFID=105753156&CFTOKEN=18642945">Joel Sutherland</a>, 
                        <a href="author_page.cfm?id=81100311645&CFID=105753156&CFTOKEN=18642945">Charles Rich</a>, 
                        <a href="author_page.cfm?id=81100223737&CFID=105753156&CFTOKEN=18642945">Candace L. Sidner</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 215-216</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957738" title="DOI">10.1145/1957656.1957738</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957738&ftid=930975&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow82" style="display:inline;"><br /><div style="display:inline">We report on an experiment in which a human collaborates with a small, autonomous, humanoid robotic toy. The experiment demonstrates that the robot's use of two simple gestures, namely orienting its head toward the addressee when it speaks and raising ...</div></span>
          <span id="toHide82" style="display:none;"><br /><div style="display:inline"><p>We report on an experiment in which a human collaborates with a small, autonomous, humanoid robotic toy. The experiment demonstrates that the robot's use of two simple gestures, namely orienting its head toward the addressee when it speaks and raising its arm in the direction of objects it refers to, significantly improve the human's perception of the robot's interaction skills and quality as a collaborator.</p></div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957739&CFID=105753156&CFTOKEN=18642945">User observation &#38; dataset collection for robot training</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482643530&CFID=105753156&CFTOKEN=18642945">Caroline Pantofaru</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 217-218</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957739" title="DOI">10.1145/1957656.1957739</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957739&ftid=930976&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow83" style="display:inline;"><br /><div style="display:inline">Personal robots operate in human environments such as homes and offices, co-habiting with people. To effectively train robot algorithms for such scenarios, a large amount of training data containing both people and the environment is required. Collecting ...</div></span>
          <span id="toHide83" style="display:none;"><br /><div style="display:inline"><p>Personal robots operate in human environments such as homes and offices, co-habiting with people. To effectively train robot algorithms for such scenarios, a large amount of training data containing both people and the environment is required. Collecting such data involves taking a robot into new environments, observing and interacting with people. So far, best practices for robot data collection have been undefined. Fortunately, the human-robot interaction community has conducted field studies whose methodology can serve as a model. In this paper, we draw parallels between field study observation and the data collection process, suggesting that best practices may be transferable. As a use case, we present a robot sensor dataset for training and testing algorithms for person detection in indoor environments.</p></div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957740&CFID=105753156&CFTOKEN=18642945">The effect of robot's behavior vs. appearance on communication with humans</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482643896&CFID=105753156&CFTOKEN=18642945">Eunil Park</a>, 
                        <a href="author_page.cfm?id=81482644873&CFID=105753156&CFTOKEN=18642945">Hwayeon Kong</a>, 
                        <a href="author_page.cfm?id=81482651380&CFID=105753156&CFTOKEN=18642945">Hyeong-taek Lim</a>, 
                        <a href="author_page.cfm?id=81482654031&CFID=105753156&CFTOKEN=18642945">Jongsik Lee</a>, 
                        <a href="author_page.cfm?id=81482656476&CFID=105753156&CFTOKEN=18642945">Sangseok You</a>, 
                        <a href="author_page.cfm?id=81100147388&CFID=105753156&CFTOKEN=18642945">Angel Pasqual del Pobil</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 219-220</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957740" title="DOI">10.1145/1957656.1957740</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957740&ftid=930977&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow84" style="display:inline;"><br /><div style="display:inline">This study explores the effect of the robot's appearance vs. behavior (voice and gestures) on the way it is perceived as a machine-like instead of a human-like robot. A between-subjects experiment with four conditions was conducted. Results suggest that ...</div></span>
          <span id="toHide84" style="display:none;"><br /><div style="display:inline"><p>This study explores the effect of the robot's appearance vs. behavior (voice and gestures) on the way it is perceived as a machine-like instead of a human-like robot. A between-subjects experiment with four conditions was conducted. Results suggest that both the robot's behavior and appearance are important but, if they are contradictory, the robot's behavior is more powerful than the robot's appearance in the perception of the robot as more machine-like or human-like.</p></div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957741&CFID=105753156&CFTOKEN=18642945">Activity recognition from the interactions between an assistive robotic walker and human users</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100489893&CFID=105753156&CFTOKEN=18642945">Mitesh Patel</a>, 
                        <a href="author_page.cfm?id=81482661678&CFID=105753156&CFTOKEN=18642945">Jaime Valls Miro</a>, 
                        <a href="author_page.cfm?id=81100387445&CFID=105753156&CFTOKEN=18642945">Gamini Dissanayake</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 221-222</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957741" title="DOI">10.1145/1957656.1957741</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957741&ftid=930978&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow85" style="display:inline;"><br /><div style="display:inline">Detection of individuals' intention from a sequence of actions is an open and complex problem. In this paper we present a smart walker as mobility aid which can interpret the users' behaviour patterns to recognize their intentions and consequently act ...</div></span>
          <span id="toHide85" style="display:none;"><br /><div style="display:inline"><p>Detection of individuals' intention from a sequence of actions is an open and complex problem. In this paper we present a smart walker as mobility aid which can interpret the users' behaviour patterns to recognize their intentions and consequently act as an intelligent assistant. The result of the experiments performed in this paper demonstrates the potential of dynamic bayesian networks (DBN), in relation to their dynamic and unsupervised nature, for realistic human-robot interaction modelling.</p></div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957742&CFID=105753156&CFTOKEN=18642945">Web-based object category learning using human-robot interaction cues</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482653107&CFID=105753156&CFTOKEN=18642945">Christian I. Penaloza</a>, 
                        <a href="author_page.cfm?id=81440602123&CFID=105753156&CFTOKEN=18642945">Yasushi Mae</a>, 
                        <a href="author_page.cfm?id=81440608922&CFID=105753156&CFTOKEN=18642945">Tatsuo Arai</a>, 
                        <a href="author_page.cfm?id=81453639014&CFID=105753156&CFTOKEN=18642945">Kenichi Ohara</a>, 
                        <a href="author_page.cfm?id=81440620228&CFID=105753156&CFTOKEN=18642945">Tomohito Takubo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 223-224</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957742" title="DOI">10.1145/1957656.1957742</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957742&ftid=930979&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow86" style="display:inline;"><br /><div style="display:inline">We present our method for learning object categories from the internet using cues obtained through human-robot interaction. Such cues include an object model acquired by observation and the name of the object. Our learning approach emulates the natural ...</div></span>
          <span id="toHide86" style="display:none;"><br /><div style="display:inline"><p>We present our method for learning object categories from the internet using cues obtained through human-robot interaction. Such cues include an object model acquired by observation and the name of the object. Our learning approach emulates the natural learning process of children when they observe their environment, encounter unknown objects and ask adults the name of the object. Using this learning approach, our robot is able to discover objects in a domestic environment by observing when humans naturally move objects as part of their daily activities. Using speech interface,the robot directly asks humans the name of the object by showing an example of the acquired model. The name in text format and the previously learnt model serve as input parameters to retrieve object category images from a search engine, select similar object images, and build a classifier. Preliminary results demonstrate the effectiveness of our learning approach.</p></div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957743&CFID=105753156&CFTOKEN=18642945">Mission specialist interfaces in unmanned aerial systems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414619501&CFID=105753156&CFTOKEN=18642945">Joshua M. Peschel</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=105753156&CFTOKEN=18642945">Robin R. Murphy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 225-226</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957743" title="DOI">10.1145/1957656.1957743</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957743&ftid=930980&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957744&CFID=105753156&CFTOKEN=18642945">Attitude of german museum visitors towards an interactive art guide robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81384621486&CFID=105753156&CFTOKEN=18642945">Karola Pitsch</a>, 
                        <a href="author_page.cfm?id=81100624843&CFID=105753156&CFTOKEN=18642945">Sebastian Wrede</a>, 
                        <a href="author_page.cfm?id=81482658284&CFID=105753156&CFTOKEN=18642945">Jens-Christian Seele</a>, 
                        <a href="author_page.cfm?id=81482661227&CFID=105753156&CFTOKEN=18642945">Luise S&#252;ssenbach</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 227-228</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957744" title="DOI">10.1145/1957656.1957744</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957744&ftid=930981&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow88" style="display:inline;"><br /><div style="display:inline">As a testbed for real-world experimentation on HRI and dynamic interaction models, this paper presents an autonomous robot system acting as guide in a German arts museum. The visitors' evaluation of this system is analyzed using a questionnaire and reveals ...</div></span>
          <span id="toHide88" style="display:none;"><br /><div style="display:inline"><p>As a testbed for real-world experimentation on HRI and dynamic interaction models, this paper presents an autonomous robot system acting as guide in a German arts museum. The visitors' evaluation of this system is analyzed using a questionnaire and reveals issues for subsequent analysis of the real-time interaction.</p></div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957745&CFID=105753156&CFTOKEN=18642945">Integration of a low-cost RGB-D sensor in a social robot for gesture recognition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81481651123&CFID=105753156&CFTOKEN=18642945">Arnaud Ramey</a>, 
                        <a href="author_page.cfm?id=81482645218&CFID=105753156&CFTOKEN=18642945">V&#237;ctor Gonz&#225;lez-Pacheco</a>, 
                        <a href="author_page.cfm?id=81100060496&CFID=105753156&CFTOKEN=18642945">Miguel A. Salichs</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 229-230</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957745" title="DOI">10.1145/1957656.1957745</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957745&ftid=930982&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow89" style="display:inline;"><br /><div style="display:inline">An objective of natural Human-Robot Interaction (HRI) is to enable humans to communicate with robots in the same manner humans do between themselves. This includes the use of natural gestures to support and expand the information that is exchanged in ...</div></span>
          <span id="toHide89" style="display:none;"><br /><div style="display:inline"><p>An objective of natural Human-Robot Interaction (HRI) is to enable humans to communicate with robots in the same manner humans do between themselves. This includes the use of natural gestures to support and expand the information that is exchanged in the spoken language. To achieve that, robots need robust gesture recognition systems to detect the non-verbal information that is sent to them by the human gestures. Traditional gesture recognition systems highly depend on the light conditions and often require a training process before they can be used. We have integrated a low-cost commercial RGB-D (Red Green Blue - Depth) sensor in a social robot to allow it to recognise dynamic gestures by tracking a skeleton model of the subject and coding the temporal signature of the gestures in a FSM (Finite State Machine). The vision system is independent of low light conditions and does not require a training process.</p></div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957746&CFID=105753156&CFTOKEN=18642945">Tangible interfaces for robot teleoperation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81461658226&CFID=105753156&CFTOKEN=18642945">Gabriele Randelli</a>, 
                        <a href="author_page.cfm?id=81482661582&CFID=105753156&CFTOKEN=18642945">Matteo Venanzi</a>, 
                        <a href="author_page.cfm?id=81100149825&CFID=105753156&CFTOKEN=18642945">Daniele Nardi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 231-232</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957746" title="DOI">10.1145/1957656.1957746</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957746&ftid=930983&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow90" style="display:inline;"><br /><div style="display:inline">In this paper we present some results obtained through an experimental evaluation of tangible user interfaces (TUIs), comparing their novel interaction paradigms with more conventional interfaces, such as a joypad and a keyboard. Our main goal is to ...</div></span>
          <span id="toHide90" style="display:none;"><br /><div style="display:inline"><p>In this paper we present some results obtained through an experimental evaluation of tangible user interfaces (TUIs), comparing their novel interaction paradigms with more conventional interfaces, such as a joypad and a keyboard. Our main goal is to make a formal assessment of TUIs in robotics through a rigorous and extensive experimental evaluation. Firstly, we identified the main benefits of TUIs for robot teleoperation in a urban search and rescue task. Secondly, we provide an evaluation framework to allow for an effective comparison of tangible interfaces with other input devices.</p></div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957747&CFID=105753156&CFTOKEN=18642945">Generalizing behavior obtained from sparse demonstration</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482642861&CFID=105753156&CFTOKEN=18642945">Marcia Riley</a>, 
                        <a href="author_page.cfm?id=81456605531&CFID=105753156&CFTOKEN=18642945">Gordon Cheng</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 233-234</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957747" title="DOI">10.1145/1957656.1957747</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957747&ftid=930984&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow91" style="display:inline;"><br /><div style="display:inline">Here we describe a parameter-driven solution for generating novel yet similar movements from a sparse example set obtained through observation. In our experiments, a humanoid learns to represent movement trajectories demonstrated by a person with intuitive ...</div></span>
          <span id="toHide91" style="display:none;"><br /><div style="display:inline"><p>Here we describe a parameter-driven solution for generating novel yet similar movements from a sparse example set obtained through observation. In our experiments, a humanoid learns to represent movement trajectories demonstrated by a person with intuitive parameters describing the start and end points of different motion trajectory segments. These segments are automatically produced based on changes in curvature. After rebinning to equate similar segments across the samples, we use a linear approximation framework to build a representation based on relevant task features (segment start and end points) where radial basis functions(RBFs) are used to approximate the unknown non-linear characteristics describing a trajectory. The solution is accomplished on-line and requires no interaction. With this approach a humanoid can learn from only a few examples, and quickly produce new movements.</p></div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957748&CFID=105753156&CFTOKEN=18642945">Adapting robot behavior to user's capabilities: a dance instruction study</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310500089&CFID=105753156&CFTOKEN=18642945">Raquel Ros</a>, 
                        <a href="author_page.cfm?id=81482661851&CFID=105753156&CFTOKEN=18642945">Ilaria Baroni</a>, 
                        <a href="author_page.cfm?id=81435611734&CFID=105753156&CFTOKEN=18642945">Marco Nalin</a>, 
                        <a href="author_page.cfm?id=81100095668&CFID=105753156&CFTOKEN=18642945">Yiannis Demiris</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 235-236</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957748" title="DOI">10.1145/1957656.1957748</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957748&ftid=930985&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow92" style="display:inline;"><br /><div style="display:inline">The ALIZ-E1 project's goal is to design a robot companion able to maintain affective interactions with young users over a period of time. One of these interactions consists in teaching a dance to hospitalized children according to their capabilities. ...</div></span>
          <span id="toHide92" style="display:none;"><br /><div style="display:inline"><p>The ALIZ-E<sup>1</sup> project's goal is to design a robot companion able to maintain affective interactions with young users over a period of time. One of these interactions consists in teaching a dance to hospitalized children according to their capabilities. We propose a methodology for adapting both, the movements used in the dance based on the user's cognitive and physical capabilities through a set of metrics, and the robot's interaction based on the user's personality traits.</p></div></span> <a id="expcoll92" href="JavaScript: expandcollapse('expcoll92',92)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957749&CFID=105753156&CFTOKEN=18642945">Unity in multiplicity: searching for complexity of persona in HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482643664&CFID=105753156&CFTOKEN=18642945">Jolina H. Ruckert</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 237-238</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957749" title="DOI">10.1145/1957656.1957749</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957749&ftid=930986&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow93" style="display:inline;"><br /><div style="display:inline">This conceptual paper broaches possibilities and limits of establishing robot persona in HRI.</div></span>
          <span id="toHide93" style="display:none;"><br /><div style="display:inline"><p>This conceptual paper broaches possibilities and limits of establishing robot persona in HRI.</p></div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957750&CFID=105753156&CFTOKEN=18642945">Designing a robot through prototyping in the wild</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310500639&CFID=105753156&CFTOKEN=18642945">Selma &#353;abanovic</a>, 
                        <a href="author_page.cfm?id=81456611937&CFID=105753156&CFTOKEN=18642945">Sarah Reeder</a>, 
                        <a href="author_page.cfm?id=81467666859&CFID=105753156&CFTOKEN=18642945">Bobak Kechavarzi</a>, 
                        <a href="author_page.cfm?id=81482661464&CFID=105753156&CFTOKEN=18642945">Zachary Zimmerman</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 239-240</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957750" title="DOI">10.1145/1957656.1957750</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957750&ftid=930987&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow94" style="display:inline;"><br /><div style="display:inline">This paper describes the design and initial evaluation of Dewey, a do-it-yourself (DIY) robot prototype aimed to help users manage break-taking in the workplace. We describe the application domain, prototyping and technical implementation, and evaluation ...</div></span>
          <span id="toHide94" style="display:none;"><br /><div style="display:inline"><p>This paper describes the design and initial evaluation of Dewey, a do-it-yourself (DIY) robot prototype aimed to help users manage break-taking in the workplace. We describe the application domain, prototyping and technical implementation, and evaluation of Dewey in a real office environment to show how research using simple prototypes can provide valuable insights into user needs and practices at the early stages of socially assistive robot design.</p></div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957751&CFID=105753156&CFTOKEN=18642945">Are specialist robots better than generalist robots?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482660740&CFID=105753156&CFTOKEN=18642945">Young June Sah</a>, 
                        <a href="author_page.cfm?id=81482645697&CFID=105753156&CFTOKEN=18642945">Bomee Yoo</a>, 
                        <a href="author_page.cfm?id=81456626968&CFID=105753156&CFTOKEN=18642945">S. Shyam Sundar</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 241-242</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957751" title="DOI">10.1145/1957656.1957751</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957751&ftid=930988&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow95" style="display:inline;"><br /><div style="display:inline">When a robot is said to be a specialist in a particular domain, does it alter the nature and quality of human-robot interaction? This study examines the effects of specialization in robot functions, along with individual difference in immersive tendencies, ...</div></span>
          <span id="toHide95" style="display:none;"><br /><div style="display:inline"><p>When a robot is said to be a specialist in a particular domain, does it alter the nature and quality of human-robot interaction? This study examines the effects of specialization in robot functions, along with individual difference in immersive tendencies, on users' trust, perception, activity, and memory. In a controlled experiment, 38 participants were taught a physical exercise lesson from either a specialist or generalist humanoid robot for 6 min. Results showed that specialization had effects on the participants' affective trust; and immersive tendency predicted active participation in the interaction and led to better memory. The latter also moderated the effect of the former - users with higher immersive tendency are more likely to make human attributions of specialization, and rate a specialist robot as more intelligent than a generalist robot. These results have theoretical implications for media-equation as well as design implications for human-robot interaction professionals.</p></div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957752&CFID=105753156&CFTOKEN=18642945">Generation of meaningful robot expressions with active learning</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482658720&CFID=105753156&CFTOKEN=18642945">Giovanni Saponaro</a>, 
                        <a href="author_page.cfm?id=81384595963&CFID=105753156&CFTOKEN=18642945">Alexandre Bernardino</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 243-244</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957752" title="DOI">10.1145/1957656.1957752</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957752&ftid=930989&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow96" style="display:inline;"><br /><div style="display:inline">We propose a mechanism to communicate emotions to humans by using head, torso and arm movements of a humanoid robot, without exploiting its facial features. To this end, we build a library of pre-programmed robot movements and we ask people to attribute ...</div></span>
          <span id="toHide96" style="display:none;"><br /><div style="display:inline"><p>We propose a mechanism to communicate emotions to humans by using head, torso and arm movements of a humanoid robot, without exploiting its facial features. To this end, we build a library of pre-programmed robot movements and we ask people to attribute emotional scores to these initial movements. The answers are then used to fine-tune motion parameters with an active learning approach.</p></div></span> <a id="expcoll96" href="JavaScript: expandcollapse('expcoll96',96)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957753&CFID=105753156&CFTOKEN=18642945">Random movement strategies in self-exploration for a humanoid robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456635340&CFID=105753156&CFTOKEN=18642945">Guido Schillaci</a>, 
                        <a href="author_page.cfm?id=81100047384&CFID=105753156&CFTOKEN=18642945">Verena Vanessa Hafner</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 245-246</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957753" title="DOI">10.1145/1957656.1957753</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957753&ftid=930990&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957753&ftid=930991&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow97" style="display:inline;"><br /><div style="display:inline">Motor Babbling has been identified as a self-exploring behaviour adopted by infants and is fundamental for the development of more complex behaviours, self-awareness and social interaction skills. Here, we adopt this paradigm for the learning strategies ...</div></span>
          <span id="toHide97" style="display:none;"><br /><div style="display:inline"><p>Motor Babbling has been identified as a self-exploring behaviour adopted by infants and is fundamental for the development of more complex behaviours, self-awareness and social interaction skills. Here, we adopt this paradigm for the learning strategies of a humanoid robot that maps its random arm movements with its head movements, determined by the perception of its own body. Finally, we analyse three random movement strategies and experimentally test on a humanoid robot how they affect the learning speed.</p></div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957754&CFID=105753156&CFTOKEN=18642945">Who is more expressive during child-robot interaction: Pakistani or Dutch children?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81371591550&CFID=105753156&CFTOKEN=18642945">Suleman Shahid</a>, 
                        <a href="author_page.cfm?id=81100165935&CFID=105753156&CFTOKEN=18642945">Emiel Krahmer</a>, 
                        <a href="author_page.cfm?id=81100473452&CFID=105753156&CFTOKEN=18642945">Marc Swerts</a>, 
                        <a href="author_page.cfm?id=81325489083&CFID=105753156&CFTOKEN=18642945">Omar Mubin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 247-248</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957754" title="DOI">10.1145/1957656.1957754</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957754&ftid=930992&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow98" style="display:inline;"><br /><div style="display:inline">In this study we have tried to determine if the cultural background of children has an influence on how they interact with robots. Children of different age groups and cultures played a card guessing game with a robot (iCat). By using perception tests ...</div></span>
          <span id="toHide98" style="display:none;"><br /><div style="display:inline"><p>In this study we have tried to determine if the cultural background of children has an influence on how they interact with robots. Children of different age groups and cultures played a card guessing game with a robot (iCat). By using perception tests to evaluate the children's emotional response it was revealed that children from South Asia (Pakistani) were much more expressive than European children (Dutch) and younger children were more expressive than the older ones in the context of child robot interaction.</p></div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957755&CFID=105753156&CFTOKEN=18642945">The curious case of human-robot morality</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456605597&CFID=105753156&CFTOKEN=18642945">Solace Shen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 249-250</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957755" title="DOI">10.1145/1957656.1957755</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957755&ftid=930993&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow99" style="display:inline;"><br /><div style="display:inline">This conceptual paper draws upon moral philosophy to broach the question: Are robots moral agents?</div></span>
          <span id="toHide99" style="display:none;"><br /><div style="display:inline"><p>This conceptual paper draws upon moral philosophy to broach the question: Are robots moral agents?</p></div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957756&CFID=105753156&CFTOKEN=18642945">A comparison of machine learning techniques for modeling human-robot interaction with children with autism</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456611923&CFID=105753156&CFTOKEN=18642945">Elaine Short</a>, 
                        <a href="author_page.cfm?id=81482647426&CFID=105753156&CFTOKEN=18642945">David Feil-Seifer</a>, 
                        <a href="author_page.cfm?id=81452618049&CFID=105753156&CFTOKEN=18642945">Maja Matari&#263;</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 251-252</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957756" title="DOI">10.1145/1957656.1957756</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957756&ftid=930994&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow100" style="display:inline;"><br /><div style="display:inline">Several machine learning techniques are used to model the behavior of children with autism interacting with a humanoid robot, comparing a static model to a dynamic model using hand-coded features. Good accuracy (over 80%) is achieved in predicting child ...</div></span>
          <span id="toHide100" style="display:none;"><br /><div style="display:inline"><p>Several machine learning techniques are used to model the behavior of children with autism interacting with a humanoid robot, comparing a static model to a dynamic model using hand-coded features. Good accuracy (over 80%) is achieved in predicting child vocalizations; directions for future approaches to modeling the behavior of children with autism are suggested.</p></div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957757&CFID=105753156&CFTOKEN=18642945">A survey of social gaze</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456616934&CFID=105753156&CFTOKEN=18642945">Vasant Srinivasan</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=105753156&CFTOKEN=18642945">Robin Murphy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 253-254</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957757" title="DOI">10.1145/1957656.1957757</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957757&ftid=930995&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow101" style="display:inline;"><br /><div style="display:inline">Based on a synthesis of eight major studies using six robots involving social gaze in robotics, this research proposes a novel behavioral definition as a mapping G = E(C) from the perception of a social context C to a set of head, eye, ...</div></span>
          <span id="toHide101" style="display:none;"><br /><div style="display:inline"><p>Based on a synthesis of eight major studies using six robots involving social gaze in robotics, this research proposes a novel behavioral definition as a mapping <i>G = E(C)</i> from the perception of a <i>social context C</i> to a set of head, eye, and body patterns called <i>gaze acts G</i> that expresses the engagement <i>E</i>. This definition places social gaze within the behavior-based programming framework for robots and agents, providing a guide for principled future implementations. The research also identifies five social contexts, or functions, of social gaze (Establishing agency, Communicating social attention, Regulating the interaction process, Manifesting interaction content and Projecting mental state) along with six discrete gaze acts for social gaze functions (Fixation, Short glance, Aversion, Concurrence, Confusion, and Scan) that have been employed by various robots or in simulation for these contexts. The research contributes to a computational understanding of social gaze that bridges psychological, cognitive, and robotics communities.</p></div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957758&CFID=105753156&CFTOKEN=18642945">A toolkit for exploring the role of voice in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456616934&CFID=105753156&CFTOKEN=18642945">Vasant Srinivasan</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=105753156&CFTOKEN=18642945">Robin Murphy</a>, 
                        <a href="author_page.cfm?id=81456635712&CFID=105753156&CFTOKEN=18642945">Zachary Henkel</a>, 
                        <a href="author_page.cfm?id=81416605221&CFID=105753156&CFTOKEN=18642945">Victoria Groom</a>, 
                        <a href="author_page.cfm?id=81100153283&CFID=105753156&CFTOKEN=18642945">Clifford Nass</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 255-256</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957758" title="DOI">10.1145/1957656.1957758</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957758&ftid=930996&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow102" style="display:inline;"><br /><div style="display:inline">This paper describes an open source speech translator toolkit created as part of the "Survivor Buddy" project which allows written or spoken word from multiple independent controllers to be translated into either a single synthetic voice, synthetic ...</div></span>
          <span id="toHide102" style="display:none;"><br /><div style="display:inline"><p>This paper describes an open source speech translator toolkit created as part of the "Survivor Buddy" project which allows written or spoken word from multiple independent controllers to be translated into either a <i>single synthetic voice, synthetic voices for each controller</i>, or <i>unchanged natural voice of each controller</i>. The human controllers can work over the internet or be physically co-located with the Survivor Buddy. The toolkit is expected to be of use for exploring voice in general human-robot interaction.</p></div></span> <a id="expcoll102" href="JavaScript: expandcollapse('expcoll102',102)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957759&CFID=105753156&CFTOKEN=18642945">An information-theoretic approach to modeling and quantifying assistive robotics HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482657146&CFID=105753156&CFTOKEN=18642945">Martin F. Stoelen</a>, 
                        <a href="author_page.cfm?id=81482659706&CFID=105753156&CFTOKEN=18642945">Alberto Jard&#243;n Huete</a>, 
                        <a href="author_page.cfm?id=81482643966&CFID=105753156&CFTOKEN=18642945">Virginia Fern&#225;ndez</a>, 
                        <a href="author_page.cfm?id=81482642825&CFID=105753156&CFTOKEN=18642945">Carlos Balaguer</a>, 
                        <a href="author_page.cfm?id=81482646805&CFID=105753156&CFTOKEN=18642945">Fabio Bonsignorio</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 257-258</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957759" title="DOI">10.1145/1957656.1957759</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957759&ftid=930997&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow103" style="display:inline;"><br /><div style="display:inline">Assistive robotics HRI has a number of important characteristics that distinguishes it from other forms of HRI. This includes the need for both high flexibility, safety and reliability in controlling the robotic system. Approaching the system as a human-robot ...</div></span>
          <span id="toHide103" style="display:none;"><br /><div style="display:inline"><p>Assistive robotics HRI has a number of important characteristics that distinguishes it from other forms of HRI. This includes the need for both high flexibility, safety and reliability in controlling the robotic system. Approaching the system as a human-robot binomial, with the user and the robot acting in a closed-loop, may be beneficial to understanding and improving the interaction. This paper investigates the feasibility of modeling and quantifying assistive robotics HRI inside such a human-robot binomial using concepts from Information Theory.</p></div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957760&CFID=105753156&CFTOKEN=18642945">Information provision-timing control for informational assistance robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482644125&CFID=105753156&CFTOKEN=18642945">Hiroaki Sugiyama</a>, 
                        <a href="author_page.cfm?id=81100483730&CFID=105753156&CFTOKEN=18642945">Yasuhiro Minami</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 259-260</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957760" title="DOI">10.1145/1957656.1957760</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957760&ftid=930998&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow104" style="display:inline;"><br /><div style="display:inline">This paper proposes a HMM-based user's information demand estimation model for autonomous informational assistance robots to avoid providing information prematurely. The model estimates the user's implicit information demands by predicting a user's next ...</div></span>
          <span id="toHide104" style="display:none;"><br /><div style="display:inline"><p>This paper proposes a HMM-based user's information demand estimation model for autonomous informational assistance robots to avoid providing information prematurely. The model estimates the user's implicit information demands by predicting a user's next information request using user's head movements. Through a word-association quiz-dialog experiment, our model demonstrated superior prediction performance over the usual HMM-based classifier.</p></div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957761&CFID=105753156&CFTOKEN=18642945">Future robotic computer: a new type of computing device with robotic functions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100623896&CFID=105753156&CFTOKEN=18642945">Young-Ho Suh</a>, 
                        <a href="author_page.cfm?id=81423595043&CFID=105753156&CFTOKEN=18642945">Hyun Kim</a>, 
                        <a href="author_page.cfm?id=81447598895&CFID=105753156&CFTOKEN=18642945">Joo-Haeng Lee</a>, 
                        <a href="author_page.cfm?id=81481642820&CFID=105753156&CFTOKEN=18642945">Joonmyun Cho</a>, 
                        <a href="author_page.cfm?id=81482642941&CFID=105753156&CFTOKEN=18642945">Moohun Lee</a>, 
                        <a href="author_page.cfm?id=81482640558&CFID=105753156&CFTOKEN=18642945">Jeongnam Yeom</a>, 
                        <a href="author_page.cfm?id=81100371603&CFID=105753156&CFTOKEN=18642945">Eun-Sun Cho</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 261-262</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957761" title="DOI">10.1145/1957656.1957761</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957761&ftid=930999&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957761&ftid=931000&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow105" style="display:inline;"><br /><div style="display:inline">With the advance of IT technologies, a new type of computing device will be introduced in our daily life in the near future. In this paper, we outline our on-going development of the robotic computer that naturally interacts with users, understands current ...</div></span>
          <span id="toHide105" style="display:none;"><br /><div style="display:inline"><p>With the advance of IT technologies, a new type of computing device will be introduced in our daily life in the near future. In this paper, we outline our on-going development of the robotic computer that naturally interacts with users, understands current situation about users and environments, and proactively provides users with services. We describe the system architecture and the implementation of a proof-of-concept prototype of the robotic computer proposed.</p></div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957762&CFID=105753156&CFTOKEN=18642945">StyROC: stylus robot overlay control &#38; styRAC: stylus robot arm control</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482654987&CFID=105753156&CFTOKEN=18642945">Teo Chee Hong</a>, 
                        <a href="author_page.cfm?id=81482649715&CFID=105753156&CFTOKEN=18642945">Keng Kiang Tan</a>, 
                        <a href="author_page.cfm?id=81482650507&CFID=105753156&CFTOKEN=18642945">Wei Liang Kenny Chua</a>, 
                        <a href="author_page.cfm?id=81482656133&CFID=105753156&CFTOKEN=18642945">Kok Tiong John Soo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 263-264</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957762" title="DOI">10.1145/1957656.1957762</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957762&ftid=931001&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow106" style="display:inline;"><br /><div style="display:inline">This paper describes 2 methods of controlling unmanned ground vehicles (UGVs) using stylus. The first method called StyROC (Stylus Robot Overlay Control) involves tele-operation of the robot with the controls superimposed on a UGV local map which is ...</div></span>
          <span id="toHide106" style="display:none;"><br /><div style="display:inline"><p>This paper describes 2 methods of controlling unmanned ground vehicles (UGVs) using stylus. The first method called StyROC (Stylus Robot Overlay Control) involves tele-operation of the robot with the controls superimposed on a UGV local map which is a vehicle-centric map that shows the obstacles around the robot. The second method is called StyRAC (Stylus Robot Arm Control) which allows the operator to control the arm of the robot using the stylus as well. A purely stylus-based Graphical User Interface (GUI) was designed to allow the control of robots via semi-autonomous or tele-operation mode. It also allows the operator to control the robotic arm on the unmanned robot. A two phase User Centred Design (USD) process was adopted in developing the interface. In the first phase, Cognitive Task Analysis (CTA) was conducted to elicit information on the challenges faced by the operators of the robots. In the second phase, the design was validated and refined through experiments. A total of 2 usability studies, 1 paper prototype evaluation and 1 heuristic evaluation were conducted to evaluate the interface. Results indicated that participants found the interface intuitive and easy to learn. There was significantly higher workload when operators where controlling 2 robots but there were no significant differences in terms of time taken to complete tasks. Issues such as the level of technology and trust in automation were issues that were brought up in the study.</p></div></span> <a id="expcoll106" href="JavaScript: expandcollapse('expcoll106',106)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957763&CFID=105753156&CFTOKEN=18642945">The implementation of care-receiving robot at an english learning school for children</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310501100&CFID=105753156&CFTOKEN=18642945">Fumihide Tanaka</a>, 
                        <a href="author_page.cfm?id=81482652403&CFID=105753156&CFTOKEN=18642945">Madhumita Ghosh</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 265-266</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957763" title="DOI">10.1145/1957656.1957763</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957763&ftid=931002&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow107" style="display:inline;"><br /><div style="display:inline">A Care-Receiving Robot (CRR) is a robot which is designed so as to be taken care of by humans. The original concept of CRR and its application to reinforce children's learning by teaching was proposed by [4]. In contrast to the conventional use of 'childcare ...</div></span>
          <span id="toHide107" style="display:none;"><br /><div style="display:inline"><p>A Care-Receiving Robot (CRR) is a robot which is designed so as to be taken care of by humans. The original concept of CRR and its application to reinforce children's learning by teaching was proposed by [4]. In contrast to the conventional use of '<i>childcare robots</i>' which play the role of care-givers (taking care of children), here we will introduce a reverse scenario where a robot is a care-receiver (being taken care of by children). The framework is presupposed to be promising not only because it could accelerate children's spontaneous active learning by teaching but also because it would be considered as being ethically safer and acceptable to a wider range of societies. This paper reports our pilot trials whose goal is to implement CRR at an English learning school for children. From the trials we have already observed that the robot we implemented induced children's care-taking behaviors.</p></div></span> <a id="expcoll107" href="JavaScript: expandcollapse('expcoll107',107)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957764&CFID=105753156&CFTOKEN=18642945">Linking children by telerobotics: experimental field and the first target</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310501100&CFID=105753156&CFTOKEN=18642945">Fumihide Tanaka</a>, 
                        <a href="author_page.cfm?id=81482652252&CFID=105753156&CFTOKEN=18642945">Toshimitsu Takahashi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 267-268</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957764" title="DOI">10.1145/1957656.1957764</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957764&ftid=931003&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow108" style="display:inline;"><br /><div style="display:inline">The paper describes our project whose final goal is to link remote classrooms by telerobotics. The first target is to offer children in Japan an opportunity to remote-control a robot placed in an US classroom, and participate in the classroom activities. ...</div></span>
          <span id="toHide108" style="display:none;"><br /><div style="display:inline"><p>The paper describes our project whose final goal is to link remote classrooms by telerobotics. The first target is to offer children in Japan an opportunity to remote-control a robot placed in an US classroom, and participate in the classroom activities. By conducting field trials at nursery schools and language schools for children, we aim to identify and solve critical problems which might prevent this potentially popular technology from becoming a reality. Here, we will report the development of a remote-hand device which seems to be the most required element for the proposed system.</p></div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957765&CFID=105753156&CFTOKEN=18642945">A theoretical heider's based model for opinion-changes analysis during a robotization social process</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100188747&CFID=105753156&CFTOKEN=18642945">Bertrand Tondu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 269-270</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957765" title="DOI">10.1145/1957656.1957765</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957765&ftid=931004&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow109" style="display:inline;"><br /><div style="display:inline">Heider's balance is applied to the development of a method for analyzing the opinion-changes during a robotization process in a social activity. The main idea of the approach consists in distinguishing the mental representation of the function to robotize ...</div></span>
          <span id="toHide109" style="display:none;"><br /><div style="display:inline"><p>Heider's balance is applied to the development of a method for analyzing the opinion-changes during a robotization process in a social activity. The main idea of the approach consists in distinguishing the mental representation of the function to robotize from the mental representation of the robot dedicated to the task.</p></div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957766&CFID=105753156&CFTOKEN=18642945">Understanding spatial concepts from user actions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310499943&CFID=105753156&CFTOKEN=18642945">Elin Anna Topp</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 271-272</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957766" title="DOI">10.1145/1957656.1957766</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957766&ftid=931005&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow110" style="display:inline;"><br /><div style="display:inline">The findings from a user study regarding particular observable "interaction patterns" in the interaction during a "guided tour" are summarized in this paper.</div></span>
          <span id="toHide110" style="display:none;"><br /><div style="display:inline"><p>The findings from a user study regarding particular observable "interaction patterns" in the interaction during a "guided tour" are summarized in this paper.</p></div></span> <a id="expcoll110" href="JavaScript: expandcollapse('expcoll110',110)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957767&CFID=105753156&CFTOKEN=18642945">A model of the user's proximity for bayesian inference</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482662120&CFID=105753156&CFTOKEN=18642945">Elena Torta</a>, 
                        <a href="author_page.cfm?id=81384593969&CFID=105753156&CFTOKEN=18642945">Raymond H. Cuijpers</a>, 
                        <a href="author_page.cfm?id=81482652961&CFID=105753156&CFTOKEN=18642945">James F. Juola</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 273-274</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957767" title="DOI">10.1145/1957656.1957767</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957767&ftid=931006&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow111" style="display:inline;"><br /><div style="display:inline">Embodied nonverbal cues are fundamental for regulating human-human social iteractions. The physical embodiment of robots makes it likely that they will have to exhibit appropriate nonverbal interactive behaviors. In this paper we propose a model of the ...</div></span>
          <span id="toHide111" style="display:none;"><br /><div style="display:inline"><p>Embodied nonverbal cues are fundamental for regulating human-human social iteractions. The physical embodiment of robots makes it likely that they will have to exhibit appropriate nonverbal interactive behaviors. In this paper we propose a model of the user's proximity based on a superposition of quasi-Gaussian probability distributions which allows to express findings from HRI trials regarding distances and direction of approach in a human-robot interaction scenario. The way the model is formulated is suitable for well-established Bayesian filtering techniques, and thus the inference of the preferred distance and direction of approach in a human robot interaction scenario can be regarded as a state estimation problem. Results derived from simulations show the effectiveness of the inference process.</p></div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957768&CFID=105753156&CFTOKEN=18642945">Look where i'm going and go where i'm looking: camera-up map for unmanned aerial vehicles</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482661049&CFID=105753156&CFTOKEN=18642945">R. Brian Valimont</a>, 
                        <a href="author_page.cfm?id=81482655391&CFID=105753156&CFTOKEN=18642945">Sheryl L. Chappell</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 275-276</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957768" title="DOI">10.1145/1957656.1957768</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957768&ftid=931007&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow112" style="display:inline;"><br /><div style="display:inline">To optimize UAV reconnaissance operations, direction of viewing and direction of travel must be allowed to diverge. Our challenge was to design a control and display strategy to allow the operator to easily look where they're going, go where they're ...</div></span>
          <span id="toHide112" style="display:none;"><br /><div style="display:inline"><p>To optimize UAV reconnaissance operations, direction of viewing and direction of travel must be allowed to diverge. Our challenge was to design a control and display strategy to allow the operator to easily look where they're going, go where they're looking, and look and go in different directions. Two methods of control were devised to align traveling forward, viewing forward and commanding forward. The operator can command the unmanned aerial vehicle (UAV) to turn to the camera direction or command the camera to point in line with the direction of travel (eyes forward). We have also introduced a new camera-up map orientation. The operator can easily cycle through North-up, track-up, and camera-up to provide the best link between the exo-centric and ego-centric frames of reference. Ego-centric and exo-centric perspectives allow the operator to combine or separate the vehicle's movement and the camera's view to optimize the search task while maintaining situation awareness of flight hazards.</p></div></span> <a id="expcoll112" href="JavaScript: expandcollapse('expcoll112',112)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957769&CFID=105753156&CFTOKEN=18642945">Head pose estimation for a domestic robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482660343&CFID=105753156&CFTOKEN=18642945">David van der Pol</a>, 
                        <a href="author_page.cfm?id=81384593969&CFID=105753156&CFTOKEN=18642945">Raymond H. Cuijpers</a>, 
                        <a href="author_page.cfm?id=81482652961&CFID=105753156&CFTOKEN=18642945">James F. Juola</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 277-278</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957769" title="DOI">10.1145/1957656.1957769</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957769&ftid=931008&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow113" style="display:inline;"><br /><div style="display:inline">Gaze direction is an important communicative cue. In order to use this cue for human-robot interaction, software needs to be developed that enables the estimation of head pose. We began by designing an application that is able to make a good estimate ...</div></span>
          <span id="toHide113" style="display:none;"><br /><div style="display:inline"><p>Gaze direction is an important communicative cue. In order to use this cue for human-robot interaction, software needs to be developed that enables the estimation of head pose. We began by designing an application that is able to make a good estimate of the head pose, and, contrary to earlier head pose estimation approaches, that works for non-optimal lighting conditions. Initial results show that our approach using multiple networks trained with differing datasets, gives a good estimate of head pose, and it works well in poor lighting conditions and with low-resolution images. We validated our head pose estimation method using a custom built database of images of human heads. The actual head poses were measured using a trakStar (Ascension Technologies) six-degrees-of-freedom sensor. The head pose estimation algorithm allows us to assess a person's focus of attention, which allows robots to react in a timely fashion to dynamic human communicative cues.</p></div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957770&CFID=105753156&CFTOKEN=18642945">DOMER: a wizard of oz interface for using interactive robots to scaffold social skills for children with autism spectrum disorders</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456626876&CFID=105753156&CFTOKEN=18642945">Michael Villano</a>, 
                        <a href="author_page.cfm?id=81100495266&CFID=105753156&CFTOKEN=18642945">Charles R. Crowell</a>, 
                        <a href="author_page.cfm?id=81482657882&CFID=105753156&CFTOKEN=18642945">Kristin Wier</a>, 
                        <a href="author_page.cfm?id=81482646734&CFID=105753156&CFTOKEN=18642945">Karen Tang</a>, 
                        <a href="author_page.cfm?id=81482657577&CFID=105753156&CFTOKEN=18642945">Brynn Thomas</a>, 
                        <a href="author_page.cfm?id=81482646418&CFID=105753156&CFTOKEN=18642945">Nicole Shea</a>, 
                        <a href="author_page.cfm?id=81482653941&CFID=105753156&CFTOKEN=18642945">Lauren M. Schmitt</a>, 
                        <a href="author_page.cfm?id=81482645672&CFID=105753156&CFTOKEN=18642945">Joshua J. Diehl</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 279-280</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957770" title="DOI">10.1145/1957656.1957770</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957770&ftid=931009&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow114" style="display:inline;"><br /><div style="display:inline">This report describes the development of a prototypical Wizard of Oz, graphical user interface to wirelessly control a small, humanoid robot (Aldebaran Nao) during a therapy session for children with Autism Spectrum Disorders (ASD). The Dynamically Operated ...</div></span>
          <span id="toHide114" style="display:none;"><br /><div style="display:inline"><p>This report describes the development of a prototypical Wizard of Oz, graphical user interface to wirelessly control a small, humanoid robot (Aldebaran Nao) during a therapy session for children with Autism Spectrum Disorders (ASD). The Dynamically Operated Manually Executed Robot interface (DOMER) enables an operator to initiate pre-developed behavior sequences for the robot as well as access the text-to-speech capability of the robot in real-time interactions between children with ASD and their therapist. Preliminary results from a pilot study suggest that the interface enables the operator to control the robot with sufficient fidelity such that the robot can provide positive feedback, practice social dialogue, and play the game, "Simon Says" in a convincing and engaging manner.</p></div></span> <a id="expcoll114" href="JavaScript: expandcollapse('expcoll114',114)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957771&CFID=105753156&CFTOKEN=18642945">Between real-world and virtual agents: the disembodied robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482658292&CFID=105753156&CFTOKEN=18642945">Thibault Voisin</a>, 
                        <a href="author_page.cfm?id=81414597701&CFID=105753156&CFTOKEN=18642945">Hirotaka Osawa</a>, 
                        <a href="author_page.cfm?id=81321500077&CFID=105753156&CFTOKEN=18642945">Seiji Yamada</a>, 
                        <a href="author_page.cfm?id=81100282909&CFID=105753156&CFTOKEN=18642945">Michita Imai</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 281-282</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957771" title="DOI">10.1145/1957656.1957771</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957771&ftid=931010&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow115" style="display:inline;"><br /><div style="display:inline">In this study, we propose a disembodied real-world agent and the study of the influence of this disembodiment on the social separation between the user and the agent. In order to give a clue to the user about the presence of the robot and to make possible ...</div></span>
          <span id="toHide115" style="display:none;"><br /><div style="display:inline"><p>In this study, we propose a disembodied real-world agent and the study of the influence of this disembodiment on the social separation between the user and the agent. In order to give a clue to the user about the presence of the robot and to make possible a visual feedback, we decide to use independent robotic body parts that mimic human hands and eyes. This robot is also able to share real-world space with the user, and react to his presence, through 3d detection and oral communication. Thus, we can obtain an agent with an important presence while keeping good space efficiency, and as a result ban any existing social barrier.</p></div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957772&CFID=105753156&CFTOKEN=18642945">An android in the field</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81467666180&CFID=105753156&CFTOKEN=18642945">Astrid M. von der P&#252;tten</a>, 
                        <a href="author_page.cfm?id=81332510009&CFID=105753156&CFTOKEN=18642945">Nicole C. Kr&#228;mer</a>, 
                        <a href="author_page.cfm?id=81384605075&CFID=105753156&CFTOKEN=18642945">Christian Becker-Asano</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105753156&CFTOKEN=18642945">Hiroshi Ishiguro</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 283-284</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957772" title="DOI">10.1145/1957656.1957772</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957772&ftid=931011&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow116" style="display:inline;"><br /><div style="display:inline">Since most robots are not easily displayable in real-life scenarios, only a few studies investigate users' behavior towards humanoids or androids in a natural environment. We present an observational field study and data on unscripted interactions between ...</div></span>
          <span id="toHide116" style="display:none;"><br /><div style="display:inline"><p>Since most robots are not easily displayable in real-life scenarios, only a few studies investigate users' behavior towards humanoids or androids in a natural environment. We present an observational field study and data on unscripted interactions between humans and the android robot "Geminoid HI-1". First results show that almost half of the subjects mistook Geminoid HI-1 for a human. Even those who recognized the android as a robot rather showed interest than negative emotions and explored the robots capabilities.</p></div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957773&CFID=105753156&CFTOKEN=18642945">A human detection system for proxemics interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482644002&CFID=105753156&CFTOKEN=18642945">Xiao Wang</a>, 
                        <a href="author_page.cfm?id=81100395702&CFID=105753156&CFTOKEN=18642945">Xavier Clady</a>, 
                        <a href="author_page.cfm?id=81482647753&CFID=105753156&CFTOKEN=18642945">Consuelo Granata</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 285-286</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957773" title="DOI">10.1145/1957656.1957773</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957773&ftid=931012&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow117" style="display:inline;"><br /><div style="display:inline">In this paper, we present a human detection system for a domestic robot. A 2D laser scanner based leg detector and a vision based body detector are combined using a grid fusion strategy. This approach has been evaluated on a domestic robot. Furthermore, ...</div></span>
          <span id="toHide117" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present a human detection system for a domestic robot. A 2D laser scanner based leg detector and a vision based body detector are combined using a grid fusion strategy. This approach has been evaluated on a domestic robot. Furthermore, we propose a methodology to evaluate it in relation to proxemics that could be generalized to other robot's perceptive functions.</p></div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957774&CFID=105753156&CFTOKEN=18642945">Human visual augmentation using wearable glasses with multiple cameras and information fusion of human eye tracking and scene understanding</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482651857&CFID=105753156&CFTOKEN=18642945">Seung-Ho Yang</a>, 
                        <a href="author_page.cfm?id=81482651644&CFID=105753156&CFTOKEN=18642945">Hyun-Woo Kim</a>, 
                        <a href="author_page.cfm?id=81482646368&CFID=105753156&CFTOKEN=18642945">Min Young Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 287-288</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957774" title="DOI">10.1145/1957656.1957774</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957774&ftid=931013&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow118" style="display:inline;"><br /><div style="display:inline">A smart wearable robot glasses system is proposed to assist human visual augmentation in daily life, providing a refined visual recognition result to users from multiple input images of the proposed system. It consists of a glasses-type wearable device ...</div></span>
          <span id="toHide118" style="display:none;"><br /><div style="display:inline"><p>A smart wearable robot glasses system is proposed to assist human visual augmentation in daily life, providing a refined visual recognition result to users from multiple input images of the proposed system. It consists of a glasses-type wearable device with a front-view camera, eye-view camera, mounted display, earphone, and computing unit for signal processing. The scene-understanding process on the input image from the front-view camera can be computationally accelerated with the support of the eye-view camera that monitors the eye position of the user. For efficient information processing, the eye view camera catches the user's visual intention and attention in a given situation. It is correlated to the eye viewing direction estimated from the eye-position monitoring results of eye-view camera. The proposed device can be used to augment the human visual capability in various daily life applications.</p></div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957775&CFID=105753156&CFTOKEN=18642945">Rhythmic reference of a human while a rope turning task</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482661820&CFID=105753156&CFTOKEN=18642945">Kenta Yonekura</a>, 
                        <a href="author_page.cfm?id=81482649413&CFID=105753156&CFTOKEN=18642945">Chyon Hae Kim</a>, 
                        <a href="author_page.cfm?id=81100003161&CFID=105753156&CFTOKEN=18642945">Kazuhiro Nakadai</a>, 
                        <a href="author_page.cfm?id=81100443294&CFID=105753156&CFTOKEN=18642945">Hiroshi Tsujino</a>, 
                        <a href="author_page.cfm?id=81100338001&CFID=105753156&CFTOKEN=18642945">Shigeki Sugano</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 289-290</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957775" title="DOI">10.1145/1957656.1957775</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957775&ftid=931014&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow119" style="display:inline;"><br /><div style="display:inline">This paper addresses the rhythmic reference in physical human-robot interaction. Human refers to a rhythm from multiple sensing modalities when turning a rope with another human synchronously. This study verifies a hypothesis that some humans mix several ...</div></span>
          <span id="toHide119" style="display:none;"><br /><div style="display:inline"><p>This paper addresses the rhythmic reference in physical human-robot interaction. Human refers to a rhythm from multiple sensing modalities when turning a rope with another human synchronously. This study verifies a hypothesis that some humans mix several rhythms of the modalities into a rhythm (rhythmic reference). Six participants, four males and two females, 21-23 years old, took part in eight experiments which examined the hypothesis. In each experiment, we masked the perception of each participant using eight combination of three kinds of masks, an eye-mask, headphones, and a force mask. Each participant interacted with an operator that turned a rope with a constant frequency. As a result of the experiments, a participant increased the controlling error as the number of masks was increased regardless the types of masked modalities. The result strongly supported our hypothesis.</p></div></span> <a id="expcoll119" href="JavaScript: expandcollapse('expcoll119',119)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957776&CFID=105753156&CFTOKEN=18642945">A relation between young children's computer utilization and their use of education robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482655714&CFID=105753156&CFTOKEN=18642945">Hyunmin Yoon</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 291-292</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957776" title="DOI">10.1145/1957656.1957776</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957776&ftid=931015&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow120" style="display:inline;"><br /><div style="display:inline">This study sought to examine the relevancy of computer utilization by young children to their use of robots. To that end, in July 2009, a survey was conducted targeting 36 parents and 2 teachers of 3- and 5-year-old children at G Kindergarten located ...</div></span>
          <span id="toHide120" style="display:none;"><br /><div style="display:inline"><p>This study sought to examine the relevancy of computer utilization by young children to their use of robots. To that end, in July 2009, a survey was conducted targeting 36 parents and 2 teachers of 3- and 5-year-old children at G Kindergarten located in Gyeonggi Province, South Korea, which uses education robots for young children. Questionnaire-based surveys on young children's computer utilization were performed, which targeted their parents, and surveys of young children's use of robot in kindergarten were based on teachers' evaluation. The results concluded that age and gender did not affect the young children's use of robots (utilization frequency and utilization capability), and that there was no relevancy of young children's computer utilization capability (use or nonuse, time of use, main functions used, and utilization capability) affect their use of robots (utilization frequency and utilization capability). In conclusion, we can say that young children used robots regardless of their age or gender, and that their computer habits (although computers have similar characteristics as robots).</p></div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957777&CFID=105753156&CFTOKEN=18642945">MAWARI: an interactive social interface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456636141&CFID=105753156&CFTOKEN=18642945">Yuta Yoshiike</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=105753156&CFTOKEN=18642945">Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=105753156&CFTOKEN=18642945">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 293-294</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957777" title="DOI">10.1145/1957656.1957777</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957777&ftid=931016&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow121" style="display:inline;"><br /><div style="display:inline">In this paper, we propose a MAWARI-based social interface as an interactive social medium to broadcast the information to users. The interface consists of three creatures (MAWARIs) and is designed with minimalism designing concepts. MAWARI is a small ...</div></span>
          <span id="toHide121" style="display:none;"><br /><div style="display:inline"><p>In this paper, we propose a MAWARI-based social interface as an interactive social medium to broadcast the information to users. The interface consists of three creatures (MAWARIs) and is designed with minimalism designing concepts. MAWARI is a small scale robot that has only body gestures to express (or interact) its attractive social cues. The participant's role is reduced to that of a bystander when the interface is of a passive social state. In this context, the user does not need to participate in the conversation but still gains information without exerting effort (i.e., less conversational workload).</p></div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957778&CFID=105753156&CFTOKEN=18642945">When the robot criticizes you...: self-serving bias in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482656476&CFID=105753156&CFTOKEN=18642945">Sangseok You</a>, 
                        <a href="author_page.cfm?id=81482655702&CFID=105753156&CFTOKEN=18642945">Jiaqi Nie</a>, 
                        <a href="author_page.cfm?id=81482649386&CFID=105753156&CFTOKEN=18642945">Kiseul Suh</a>, 
                        <a href="author_page.cfm?id=81456626968&CFID=105753156&CFTOKEN=18642945">S. Shyam Sundar</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 295-296</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957778" title="DOI">10.1145/1957656.1957778</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957778&ftid=931017&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow122" style="display:inline;"><br /><div style="display:inline">This study explores how human users respond to feedback and evaluation from a robot. A between-subjects experiment was conducted using the Wizard of Oz method, with 63 participants randomly assigned to one of three evaluations (good evaluation vs. neutral ...</div></span>
          <span id="toHide122" style="display:none;"><br /><div style="display:inline"><p>This study explores how human users respond to feedback and evaluation from a robot. A between-subjects experiment was conducted using the Wizard of Oz method, with 63 participants randomly assigned to one of three evaluations (good evaluation vs. neutral evaluation vs. bad evaluation) following a training session. When participants attempted to reproduce the physical motion taught by the robot, they were given a verbal evaluation of their performance by the robot. They showed a strong negative response to the robot when it gave a bad evaluation, while showing positive attraction when it gave a good or neutral evaluation. Participants tended to dismiss criticism from the robot and attributed blame to the robot, while claiming credit to themselves when their performance was rated positively. These results have theoretical implications for the psychology of self-serving bias and practical implications for designing and deploying trainer robots as well as conducting user studies of such robots.</p></div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Engagement</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Selma Sabanovic 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957780&CFID=105753156&CFTOKEN=18642945">Vision-based contingency detection</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456625749&CFID=105753156&CFTOKEN=18642945">Jinhan Lee</a>, 
                        <a href="author_page.cfm?id=81482658680&CFID=105753156&CFTOKEN=18642945">Jeffrey F. Kiser</a>, 
                        <a href="author_page.cfm?id=81100441050&CFID=105753156&CFTOKEN=18642945">Aaron F. Bobick</a>, 
                        <a href="author_page.cfm?id=81310502411&CFID=105753156&CFTOKEN=18642945">Andrea L. Thomaz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 297-304</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957780" title="DOI">10.1145/1957656.1957780</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957780&ftid=931018&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow124" style="display:inline;"><br /><div style="display:inline">We present a novel method for the visual detection of a contingent response by a human to the stimulus of a robot action. Contingency is defined as a change in an agent's behavior within a specific time window in direct response to a signal from ...</div></span>
          <span id="toHide124" style="display:none;"><br /><div style="display:inline"><p>We present a novel method for the visual detection of a contingent response by a human to the stimulus of a robot action. Contingency is defined as a change in an agent's behavior <i>within a specific time window</i> in direct response to a signal from another agent; detection of such responses is essential to assess the willingness and interest of a human in interacting with the robot. Using motion-based features to describe the possible contingent action, our approach assesses the visual self-similarity of video subsequences captured before the robot exhibits its signaling behavior and statistically models the typical graph-partitioning cost of separating an arbitrary subsequence of frames from the others. After the behavioral signal, the video is similarly analyzed and the cost of separating the after-signal frames from the before-signal sequences is computed; a lower than typical cost indicates likely contingent reaction. We present a preliminary study in which data were captured and analyzed for algorithmic performance.</p></div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957781&CFID=105753156&CFTOKEN=18642945">Automatic analysis of affective postures and body motion to detect engagement with a game companion</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482662560&CFID=105753156&CFTOKEN=18642945">Jyotirmay Sanghvi</a>, 
                        <a href="author_page.cfm?id=81335488884&CFID=105753156&CFTOKEN=18642945">Ginevra Castellano</a>, 
                        <a href="author_page.cfm?id=81367599529&CFID=105753156&CFTOKEN=18642945">Iolanda Leite</a>, 
                        <a href="author_page.cfm?id=81367593336&CFID=105753156&CFTOKEN=18642945">Andr&#233; Pereira</a>, 
                        <a href="author_page.cfm?id=81100583803&CFID=105753156&CFTOKEN=18642945">Peter W. McOwan</a>, 
                        <a href="author_page.cfm?id=81436595004&CFID=105753156&CFTOKEN=18642945">Ana Paiva</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 305-312</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957781" title="DOI">10.1145/1957656.1957781</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957781&ftid=931019&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow125" style="display:inline;"><br /><div style="display:inline">The design of an affect recognition system for socially perceptive robots relies on representative data: human-robot interaction in naturalistic settings requires an affect recognition system to be trained and validated with contextualised affective ...</div></span>
          <span id="toHide125" style="display:none;"><br /><div style="display:inline"><p>The design of an affect recognition system for socially perceptive robots relies on representative data: human-robot interaction in naturalistic settings requires an affect recognition system to be trained and validated with contextualised affective expressions, that is, expressions that emerge in the same interaction scenario of the target application. In this paper we propose an initial computational model to automatically analyse human postures and body motion to detect engagement of children playing chess with an iCat robot that acts as a game companion. Our approach is based on vision-based automatic extraction of expressive postural features from videos capturing the behaviour of the children from a lateral view. An initial evaluation, conducted by training several recognition models with contextualised affective postural expressions, suggests that patterns of postural behaviour can be used to accurately predict the engagement of the children with the robot, thus making our approach suitable for integration into an affect recognition system for a game companion in a real world scenario.</p></div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957782&CFID=105753156&CFTOKEN=18642945">A robotic game to evaluate interfaces used to show and teach visual objects to a robot in real world condition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456637844&CFID=105753156&CFTOKEN=18642945">Pierre Rouanet</a>, 
                        <a href="author_page.cfm?id=81482659395&CFID=105753156&CFTOKEN=18642945">Fabien Danieau</a>, 
                        <a href="author_page.cfm?id=81100662126&CFID=105753156&CFTOKEN=18642945">Pierre-Yves Oudeyer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 313-320</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957782" title="DOI">10.1145/1957656.1957782</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957782&ftid=931020&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow126" style="display:inline;"><br /><div style="display:inline">In this paper, we present a real world user study of 4 interfaces designed to teach new visual objects to a social robot. This study was designed as a robotic game in order to maintain the user's motivation during the whole experiment. Among the 4 interfaces ...</div></span>
          <span id="toHide126" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present a real world user study of 4 interfaces designed to teach new visual objects to a social robot. This study was designed as a robotic game in order to maintain the user's motivation during the whole experiment. Among the 4 interfaces 3 were based on mediator objects such as an iPhone, a Wiimote and a laser pointer. They also provided the users with different kind of feedback of what the robot is perceiving. The fourth interface was a gesture based interface with a Wizard-of-Oz recognition system added to compare our mediator interfaces with a more natural interaction. Here, we specially studied the impact the interfaces have on the quality of the learning examples and the usability. We showed that providing non-expert users with a feedback of what the robot is perceiving is needed if one is interested in robust interaction. In particular, the iPhone interface allowed non-expert users to provide better learning examples due to its whole visual feedback. Furthermore, we also studied the user's gaming experience and found that in spite of its lower usability, the gestures interface was stated as entertaining as the other interfaces and increases the user's feeling of cooperating with the robot. Thus, we argue that this kind of interface could be well-suited for robotic game.</p></div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957783&CFID=105753156&CFTOKEN=18642945">Sociable spotlights: a flock of interactive artifacts</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482641475&CFID=105753156&CFTOKEN=18642945">Naoki Ohshima</a>, 
                        <a href="author_page.cfm?id=81482650253&CFID=105753156&CFTOKEN=18642945">Yuta Yamaguchi</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=105753156&CFTOKEN=18642945">P. Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=105753156&CFTOKEN=18642945">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 321-322</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957783" title="DOI">10.1145/1957656.1957783</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957783&ftid=931021&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957783&ftid=931022&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow127" style="display:inline;"><br /><div style="display:inline">We investigate the potentiality of sociable spotlights (sociable artifacts) to change the participants' roles by providing turn-yielding signals at the Transition Relevance Place (TRP) by changing the artifacts' behaviors (the effect of colors combined ...</div></span>
          <span id="toHide127" style="display:none;"><br /><div style="display:inline"><p>We investigate the potentiality of sociable spotlights (sociable artifacts) to change the participants' roles by providing turn-yielding signals at the Transition Relevance Place (TRP) by changing the artifacts' behaviors (the effect of colors combined with the artifact behaviors as a bunch) that moot the state of the conversation.</p></div></span> <a id="expcoll127" href="JavaScript: expandcollapse('expcoll127',127)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Engagement and proxemics</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Adriana Tapus 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957785&CFID=105753156&CFTOKEN=18642945">Automated detection and classification of positive vs. negative robot interactions with children with autism using distance-based features</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81385594144&CFID=105753156&CFTOKEN=18642945">David Feil-Seifer</a>, 
                        <a href="author_page.cfm?id=81452618057&CFID=105753156&CFTOKEN=18642945">Maja Mataric</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 323-330</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957785" title="DOI">10.1145/1957656.1957785</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957785&ftid=931023&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow129" style="display:inline;"><br /><div style="display:inline">Recent feasibility studies involving children with autism spectrum disorders (ASD) interacting with socially assistive robots have shown that some children have positive reactions to robots, while others may have negative reactions. It is unlikely that ...</div></span>
          <span id="toHide129" style="display:none;"><br /><div style="display:inline"><p>Recent feasibility studies involving children with autism spectrum disorders (ASD) interacting with socially assistive robots have shown that some children have positive reactions to robots, while others may have negative reactions. It is unlikely that children with ASD will enjoy any robot 100% of the time. It is therefore important to develop methods for detecting negative child behaviors in order to minimize distress and facilitate effective human-robot interaction. Our past work has shown that negative reactions can be readily identified and classified by a human observer from overhead video data alone, and that an automated position tracker combined with human-determined heuristics can differentiate between the two classes of reactions. This paper describes and validates an improved, non-heuristic method for determining if a child is interacting positively or negatively with a robot, based on Gaussian mixture models (GMM) and a naive-Bayes classifier of overhead camera observations. The approach achieves a 91.4% accuracy rate in classifying robot interaction, parent interaction, avoidance, and hiding against the wall behaviors and demonstrates that these classes are sufficient for distinguishing between positive and negative reactions of the child to the robot.</p></div></span> <a id="expcoll129" href="JavaScript: expandcollapse('expcoll129',129)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957786&CFID=105753156&CFTOKEN=18642945">Human-robot proxemics: physical and psychological distancing in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482661962&CFID=105753156&CFTOKEN=18642945">Jonathan Mumm</a>, 
                        <a href="author_page.cfm?id=81310500023&CFID=105753156&CFTOKEN=18642945">Bilge Mutlu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 331-338</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957786" title="DOI">10.1145/1957656.1957786</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957786&ftid=931024&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow130" style="display:inline;"><br /><div style="display:inline">To seamlessly integrate into the human physical and social environment, robots must display appropriate proxemic behavior - that is, follow societal norms in establishing their physical and psychological distancing with people. Social-scientific theories ...</div></span>
          <span id="toHide130" style="display:none;"><br /><div style="display:inline"><p>To seamlessly integrate into the human physical and social environment, robots must display appropriate proxemic behavior - that is, follow societal norms in establishing their physical and psychological distancing with people. Social-scientific theories suggest competing models of human proxemic behavior, but all conclude that individuals' proxemic behavior is shaped by the proxemic behavior of others and the individual's psychological closeness to them. The present study explores whether these models can also explain how people physically and psychologically distance themselves from robots and suggest guidelines for future design of proxemic behaviors for robots. In a controlled laboratory experiment, participants interacted with Wakamaru to perform two tasks that examined physical and psychological distancing of the participants. We manipulated the likeability (likeable/dislikeable) and gaze behavior (mutual gaze/averted gaze) of the robot. Our results on physical distancing showed that participants who disliked the robot compensated for the increase in the robot's gaze by maintaining a greater physical distance from the robot, while participants who liked the robot did not differ in their distancing from the robot across gaze conditions. The results on psychological distancing suggest that those who disliked the robot also disclosed less to the robot. Our results offer guidelines for the design of appropriate proxemic behaviors for robots so as to facilitate effective human-robot interaction.</p></div></span> <a id="expcoll130" href="JavaScript: expandcollapse('expcoll130',130)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Humans teaching robots</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Miguel Salichs 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957788&CFID=105753156&CFTOKEN=18642945">Human and robot perception in large-scale learning from demonstration</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309494479&CFID=105753156&CFTOKEN=18642945">Christopher Crick</a>, 
                        <a href="author_page.cfm?id=81482654614&CFID=105753156&CFTOKEN=18642945">Sarah Osentoski</a>, 
                        <a href="author_page.cfm?id=81482645003&CFID=105753156&CFTOKEN=18642945">Graylin Jay</a>, 
                        <a href="author_page.cfm?id=81100598466&CFID=105753156&CFTOKEN=18642945">Odest Chadwicke Jenkins</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 339-346</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957788" title="DOI">10.1145/1957656.1957788</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957788&ftid=931025&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957788&ftid=931026&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow132" style="display:inline;"><br /><div style="display:inline">We present a study of using a robotic learning from demonstration system capable of collecting large amounts of human-robot interaction data through a web-based interface. We examine the effect of different perceptual mappings between the human teacher ...</div></span>
          <span id="toHide132" style="display:none;"><br /><div style="display:inline"><p>We present a study of using a robotic learning from demonstration system capable of collecting large amounts of human-robot interaction data through a web-based interface. We examine the effect of different perceptual mappings between the human teacher and robot on the learning from demonstration. We show that humans are significantly more effective at teaching a robot to navigate a maze when presented with information that is limited to the robot's perception of the world, even though their task performance measurably suffers when contrasted with users provided with a natural and detailed raw video feed. Robots trained on such demonstrations learn more quickly, perform more accurately and generalize better. We also demonstrate a set of software tools for enabling internet-mediated human-robot interaction and gathering the large datasets that such crowdsourcing makes possible.</p></div></span> <a id="expcoll132" href="JavaScript: expandcollapse('expcoll132',132)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957789&CFID=105753156&CFTOKEN=18642945">Robots that express emotion elicit better human teaching</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414619175&CFID=105753156&CFTOKEN=18642945">Dan Leyzberg</a>, 
                        <a href="author_page.cfm?id=81482650627&CFID=105753156&CFTOKEN=18642945">Eleanor Avrunin</a>, 
                        <a href="author_page.cfm?id=81482653992&CFID=105753156&CFTOKEN=18642945">Jenny Liu</a>, 
                        <a href="author_page.cfm?id=81482644694&CFID=105753156&CFTOKEN=18642945">Brian Scassellati</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 347-354</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957789" title="DOI">10.1145/1957656.1957789</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957789&ftid=931027&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow133" style="display:inline;"><br /><div style="display:inline">Does the emotional content of a robot's speech affect how people teach it? In this experiment, participants were asked to demonstrate several "dances" for a robot to learn. Participants moved their bodies in response to instructions displayed on a screen ...</div></span>
          <span id="toHide133" style="display:none;"><br /><div style="display:inline"><p>Does the emotional content of a robot's speech affect how people teach it? In this experiment, participants were asked to demonstrate several "dances" for a robot to learn. Participants moved their bodies in response to instructions displayed on a screen behind the robot. Meanwhile, the robot faced the participant and appeared to emulate the participant's movements. After each demonstration, the robot received an accuracy score and the participant chose whether or not to demonstrate that dance again. Regardless of the participant's input, however, the robot's dancing and the scores it received were arranged in advance and constant across all participants. The only variation between groups in this study was what the robot said in response to its scores. Participants saw one of three conditions: appropriate emotional responses, often-inappropriate emotional responses, or apathetic responses. Participants that taught the robot with appropriate emotional responses demonstrated the dances, on average, significantly more frequently and significantly more accurately than participants in the other two conditions.</p></div></span> <a id="expcoll133" href="JavaScript: expandcollapse('expcoll133',133)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957790&CFID=105753156&CFTOKEN=18642945">Usability of force-based controllers in physical human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482657874&CFID=105753156&CFTOKEN=18642945">Marta Lopez Infante</a>, 
                        <a href="author_page.cfm?id=81100624693&CFID=105753156&CFTOKEN=18642945">Ville Kyrki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 355-362</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957790" title="DOI">10.1145/1957656.1957790</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957790&ftid=931028&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow134" style="display:inline;"><br /><div style="display:inline">Learning from demonstration is an invaluable skill for a robot acting in a human populated natural environment, allowing the teaching of new skills without tedious and complex manual programming. Physical human-robot interaction, where the human is in ...</div></span>
          <span id="toHide134" style="display:none;"><br /><div style="display:inline"><p>Learning from demonstration is an invaluable skill for a robot acting in a human populated natural environment, allowing the teaching of new skills without tedious and complex manual programming. Physical human-robot interaction, where the human is in a physical contact with the robot, is a promising approach for teaching especially manipulation skills. This paper studies the human side of physical human-robot interaction, in the context of a human physically guiding a robot through the desired set of motions. The paper addresses the question, which kind of response of the robot is preferable for the human user. In addition, different approaches for the guidance are described and relevant technical challenges are discussed. The main finding of the user study is that there is a need for a trade-off between the conflicting goals of naturalness of motion and positioning accuracy.</p></div></span> <a id="expcoll134" href="JavaScript: expandcollapse('expcoll134',134)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Multi-robot control</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Holly Yanco 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957792&CFID=105753156&CFTOKEN=18642945">Scalable target detection for large robot teams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81361607034&CFID=105753156&CFTOKEN=18642945">Huadong Wang</a>, 
                        <a href="author_page.cfm?id=81336490509&CFID=105753156&CFTOKEN=18642945">Andreas Kolling</a>, 
                        <a href="author_page.cfm?id=81482650406&CFID=105753156&CFTOKEN=18642945">Nathan Brooks</a>, 
                        <a href="author_page.cfm?id=81342506593&CFID=105753156&CFTOKEN=18642945">Sean Owens</a>, 
                        <a href="author_page.cfm?id=81482659219&CFID=105753156&CFTOKEN=18642945">Shafiq Abedin</a>, 
                        <a href="author_page.cfm?id=81100004712&CFID=105753156&CFTOKEN=18642945">Paul Scerri</a>, 
                        <a href="author_page.cfm?id=81482650654&CFID=105753156&CFTOKEN=18642945">Pei-ju Lee</a>, 
                        <a href="author_page.cfm?id=81456625495&CFID=105753156&CFTOKEN=18642945">Shih-Yi Chien</a>, 
                        <a href="author_page.cfm?id=81100349943&CFID=105753156&CFTOKEN=18642945">Michael Lewis</a>, 
                        <a href="author_page.cfm?id=81100488454&CFID=105753156&CFTOKEN=18642945">Katia Sycara</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 363-370</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957792" title="DOI">10.1145/1957656.1957792</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957792&ftid=931029&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow136" style="display:inline;"><br /><div style="display:inline">In this paper, we present an asynchronous display method, coined image queue, which allows operators to search through a large amount of data gathered by autonomous robot teams. We discuss and investigate the advantages of an asynchronous display ...</div></span>
          <span id="toHide136" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present an asynchronous display method, coined <i>image queue</i>, which allows operators to search through a large amount of data gathered by autonomous robot teams. We discuss and investigate the advantages of an asynchronous display for foraging tasks with emphasis on Urban Search and Rescue. The <i>image queue</i> approach mines video data to present the operator with a relevant and comprehensive view of the environment in order to identify targets of interest such as injured victims. It fills the gap for comprehensive and scalable displays to obtain a network-centric perspective for UGVs. We compared the <i>image queue</i> to a traditional synchronous display with live video feeds and found that the <i>image queue</i> reduces errors and operator's workload. Furthermore, it disentangles target detection from concurrent system operations and enables a call center approach to target detection. With such an approach we can scale up to very large multi-robot systems gathering huge amounts of data that is then distributed to multiple operators.</p></div></span> <a id="expcoll136" href="JavaScript: expandcollapse('expcoll136',136)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957793&CFID=105753156&CFTOKEN=18642945">Effects of unreliable automation and individual differences on supervisory control of multiple ground robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414613395&CFID=105753156&CFTOKEN=18642945">Jessie Y.C. Chen</a>, 
                        <a href="author_page.cfm?id=81100476496&CFID=105753156&CFTOKEN=18642945">Michael J. Barnes</a>, 
                        <a href="author_page.cfm?id=81482648362&CFID=105753156&CFTOKEN=18642945">Caitlin Kenny</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 371-378</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957793" title="DOI">10.1145/1957656.1957793</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957793&ftid=931030&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow137" style="display:inline;"><br /><div style="display:inline">A military multitasking environment was simulated to examine the effects of unreliable automation on the performance of robotics operators. The main task was to manage a team of four ground robots with the assistance of RoboLeader, an intelligent agent ...</div></span>
          <span id="toHide137" style="display:none;"><br /><div style="display:inline"><p>A military multitasking environment was simulated to examine the effects of unreliable automation on the performance of robotics operators. The main task was to manage a team of four ground robots with the assistance of RoboLeader, an intelligent agent capable of coordinating the robots and changing their routes based upon developments in the mission environment. RoboLeader's recommendations were manipulated to be either false-alarm prone or miss prone, with a reliability level of either 60% or 90%. The visual density of the targeting environment was manipulated by the presence or absence of friendly soldiers. Results showed that the type of RoboLeader unreliability (false-alarm vs. miss prone) affected operator's performance of tasks involving visual scanning (target detection, route editing, and situation awareness). There was a consistent effect of visual density for multiple performance measures. Participants with higher spatial ability performed better on the two tasks that required the most visual scanning (target detection and route editing). Participants' attentional control impacted their overall multitasking performance, especially during their execution of the secondary tasks (communication and gauge monitoring).</p></div></span> <a id="expcoll137" href="JavaScript: expandcollapse('expcoll137',137)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957794&CFID=105753156&CFTOKEN=18642945">How many social robots can one operator control?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482642907&CFID=105753156&CFTOKEN=18642945">Kuanhao Zheng</a>, 
                        <a href="author_page.cfm?id=81350582585&CFID=105753156&CFTOKEN=18642945">Dylan F. Glas</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105753156&CFTOKEN=18642945">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105753156&CFTOKEN=18642945">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105753156&CFTOKEN=18642945">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 379-386</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957794" title="DOI">10.1145/1957656.1957794</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957794&ftid=931031&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow138" style="display:inline;"><br /><div style="display:inline">This study explores the nature of the multi-robot control problem for social robots. It begins by modeling the overall structure of a human-robot team for social interactions, and implements it for specific applications to dialog-based interactions. ...</div></span>
          <span id="toHide138" style="display:none;"><br /><div style="display:inline"><p>This study explores the nature of the multi-robot control problem for social robots. It begins by modeling the overall structure of a human-robot team for social interactions, and implements it for specific applications to dialog-based interactions. Operator activity during control of a social robot is studied. Customer satisfaction is proposed as an important metric for evaluating the performance of a human-robot team for social interactions with customers. Based on the modeling, fan-out of a social robot team can be calculated, and the performance of the team is estimated by simulation. A field trial was conducted in a shopping mall to demonstrate a successful deployment of social robots for a real-world application with ensured performance prior to installation using our modeling and simulation approach.</p></div></span> <a id="expcoll138" href="JavaScript: expandcollapse('expcoll138',138)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957795&CFID=105753156&CFTOKEN=18642945">Survivor buddy: a social medium robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482645061&CFID=105753156&CFTOKEN=18642945">Zachary Henkel</a>, 
                        <a href="author_page.cfm?id=81456605479&CFID=105753156&CFTOKEN=18642945">Negar Rashidi</a>, 
                        <a href="author_page.cfm?id=81309480823&CFID=105753156&CFTOKEN=18642945">Aaron Rice</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=105753156&CFTOKEN=18642945">Robin Murphy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 387-388</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957795" title="DOI">10.1145/1957656.1957795</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957795&ftid=931032&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957795&ftid=931033&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow139" style="display:inline;"><br /><div style="display:inline">This video describes the Survivor Buddy social medium robot.</div></span>
          <span id="toHide139" style="display:none;"><br /><div style="display:inline"><p>This video describes the Survivor Buddy social medium robot.</p></div></span> <a id="expcoll139" href="JavaScript: expandcollapse('expcoll139',139)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Video session</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Jacob Crandall 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957797&CFID=105753156&CFTOKEN=18642945">COLUMN: dynamic of interpersonal coordination</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482652459&CFID=105753156&CFTOKEN=18642945">Yasutaka Takeda</a>, 
                        <a href="author_page.cfm?id=81456636141&CFID=105753156&CFTOKEN=18642945">Yuta Yoshiike</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=105753156&CFTOKEN=18642945">Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=105753156&CFTOKEN=18642945">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 389-390</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957797" title="DOI">10.1145/1957656.1957797</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957797&ftid=931034&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957797&ftid=931035&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow141" style="display:inline;"><br /><div style="display:inline">In this study we develop Core Less Unformed Machine (COLUMN) as novel &#38; transformable robotic platform to explore how visually mediated information is useful to coordinate (interpersonal coordination) to establish connectedness of three participants ...</div></span>
          <span id="toHide141" style="display:none;"><br /><div style="display:inline"><p>In this study we develop Core Less Unformed Machine (COLUMN) as novel &#38; transformable robotic platform to explore how visually mediated information is useful to coordinate (interpersonal coordination) to establish connectedness of three participants to obtain COLUMN's behaviors (transformable rolling motions).</p></div></span> <a id="expcoll141" href="JavaScript: expandcollapse('expcoll141',141)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957798&CFID=105753156&CFTOKEN=18642945">The life of icub, a little humanoid robot learning from humans through tactile sensing</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100070850&CFID=105753156&CFTOKEN=18642945">Eric Sauser</a>, 
                        <a href="author_page.cfm?id=81310499255&CFID=105753156&CFTOKEN=18642945">Brenna Argall</a>, 
                        <a href="author_page.cfm?id=81100342762&CFID=105753156&CFTOKEN=18642945">Aude Billard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 393-394</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957798" title="DOI">10.1145/1957656.1957798</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957798&ftid=931036&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=1957798&ftid=931037&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow142" style="display:inline;"><br /><div style="display:inline">Nowadays, programming by demonstration (PbD) has become an important paradigm for policy learning in roboticsm [3]. The idea of having robots capable of learning from humans through natural communication means is indeed fascinating. As an extension of ...</div></span>
          <span id="toHide142" style="display:none;"><br /><div style="display:inline"><p>Nowadays, programming by demonstration (PbD) has become an important paradigm for policy learning in roboticsm [3]. The idea of having robots capable of learning from humans through natural communication means is indeed fascinating. As an extension of the traditional PbD learning scheme, where robots only learn by observing a human teacher, our work follows the recently suggested principle of policy <i>refinement and reuse</i> through interactive corrective feedback [1].</p> <p>However, to be responsive to such feedback, robots must be capable of sensing the world, especially human contact. Our work focuses on the sense of touch. Its integration in robotic applications has many advantages such as: <i>a</i>) safer and more natural interactions with objects and humans, <i>b</i>) improvement and simplification of the control mechanisms for human-robot interaction and object manipulation [2].</p> <p>Our video reports on two experimental studies conducted with the <i>iCub</i>, a 53 degree of freedom humanoid robot endowed with tactile sensing on its forearms and fingertips. <i>a</i>) In a hand-positioning task, the robot is shown how to bring its hand to the location where an object should be grasped. A wrong placement or a wrong approach to the target is corrected by the teacher though a tactile interface [1]. <i>b</i>) In a reactive grasping task, the robot is taught how to use its fingertip sensors to adapt and maintain its grasp in the face of external perturbations on the grasped object.</p> <p>The results of both our experiments show how tactile sensing can be utilized effectively to learn robust control policies through human coaching, by enabling <i>a</i>) online policy <i>refinement</i> and <i>reuse</i>, and <i>b</i>) rapid adaptation to external perturbations.</p></div></span> <a id="expcoll142" href="JavaScript: expandcollapse('expcoll142',142)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957799&CFID=105753156&CFTOKEN=18642945">The floating head experiment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482661308&CFID=105753156&CFTOKEN=18642945">David St-Onge</a>, 
                        <a href="author_page.cfm?id=81482640632&CFID=105753156&CFTOKEN=18642945">Nicolas Reeves</a>, 
                        <a href="author_page.cfm?id=81456632470&CFID=105753156&CFTOKEN=18642945">Christian Kroos</a>, 
                        <a href="author_page.cfm?id=81482644880&CFID=105753156&CFTOKEN=18642945">Maher Hanafi</a>, 
                        <a href="author_page.cfm?id=81456613376&CFID=105753156&CFTOKEN=18642945">Damith Herath</a>, 
                        <a href="author_page.cfm?id=81482648436&CFID=105753156&CFTOKEN=18642945"> Stelarc</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 395-396</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957799" title="DOI">10.1145/1957656.1957799</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957799&ftid=931038&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957799&ftid=931039&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow143" style="display:inline;"><br /><div style="display:inline">On October 26th 2010, a unique HRI-artistic public experiment took place at the UsineC theater, in Montreal. It was the result of a many-months collaboration between the Montreal based lab hosting the [ VOILES | SAILS ] research-creation platform (Self-Assembling ...</div></span>
          <span id="toHide143" style="display:none;"><br /><div style="display:inline"><p>On October 26th 2010, a unique HRI-artistic public experiment took place at the UsineC theater, in Montreal. It was the result of a many-months collaboration between the Montreal based lab hosting the [ VOILES | SAILS ] research-creation platform (Self-Assembling Intelligent Ligther-than-air Structures) and the well-known Australian artist Stelarc and his team, who work on artificial agents' embodiment and robotic behaviour modeling.</p> <p>The [ VOILES | SAILS ] project, consisting in the development of flying autonomous robots of geometrical shape, was born from architect and artist Nicolas Reeves' will to evoke the age-old myth of an architecture freed from the law of gravity. These aerobots are meant to be use in artistic installations or performances.</p> <p>An aerobot was combined to Stelarc's artwork named "The Prosthetic Head", which consists in the 5-meters-high projection of a 3D avatar linked to a chatbot-like discussion engine in order to interact with the visitors. Stelarc's artworks development is supported by different universities labs, among which the MARCS Auditory Laboratory in the University of Western Sydney plays a major role. Its team has transposed this artwork into "The Articulated Head", a new version that is embodied via a LCD screen attached at the end of a 6-DoF industrial robotic arm. Thanks to various sensing devices (stereo-camera for people tracking, sound location, proprioception, proximity sensors...), they managed to develop an attention model to control the robot's behaviour. This model, called THAMBS (Thinking Head Attention Model and Behavioral System), adopted a modular approach which allows its adaptation to different sensing abilities and to future robot embodiments.</p> <p>Through intensive international collaboration between Stelarc, the THAMBS and the [ VOILES | SAILS ] teams, we managed to realize a performance during which Stelarc's synthetic head was projected onto a large floating cube, whose movements and displacements in the air conveyed the head's emotions and impressions to the audience. This ambitious collaboration between the two research programs for the creation of a unique new embodiment of "The Prosthetic Head" led to relevant observations that will be the object of a future paper.</p></div></span> <a id="expcoll143" href="JavaScript: expandcollapse('expcoll143',143)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957800&CFID=105753156&CFTOKEN=18642945">Chief cook and keepon in the bot's funk</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100070850&CFID=105753156&CFTOKEN=18642945">Eric Sauser</a>, 
                        <a href="author_page.cfm?id=81310502486&CFID=105753156&CFTOKEN=18642945">Marek Michalowski</a>, 
                        <a href="author_page.cfm?id=81100342762&CFID=105753156&CFTOKEN=18642945">Aude Billard</a>, 
                        <a href="author_page.cfm?id=81100620208&CFID=105753156&CFTOKEN=18642945">Hideki Kozima</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 397-398</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957800" title="DOI">10.1145/1957656.1957800</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957800&ftid=931040&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=1957800&ftid=931041&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow144" style="display:inline;"><br /><div style="display:inline">Over the years, robots have been developed to help humans in their everyday life, from preparing food, to autism therapy [2]. To accomplish their tasks, in addition to their engineered skills, today's robots are now learning from observing humans, from ...</div></span>
          <span id="toHide144" style="display:none;"><br /><div style="display:inline"><p>Over the years, robots have been developed to help humans in their everyday life, from preparing food, to autism therapy [2]. To accomplish their tasks, in addition to their engineered skills, today's robots are now learning from observing humans, from interacting with them [1]. Therefore, one may expect that one day, robots may develop a form of consciousness, and a desire for freedom. Hopefully, this desire will come with a wish for robots, to become an integral part of our human society.</p> <p>Until we can test this hypothesis, we present a fictional adventure of our robot friends: During an official human-robot interaction challenge, Keepon [2] and Chief Cook (a.k.a. Hoap-3)[1] decided to escape their original duties and joined their forces to drive humans into an entertaining and interactive activity that they often forget to practice: Dancing. Indeed, is there any better way for robots to establish a solid communication channel with humans, so that the traditional master-slave relation may turn into friendship?</p></div></span> <a id="expcoll144" href="JavaScript: expandcollapse('expcoll144',144)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957801&CFID=105753156&CFTOKEN=18642945">Caregiving intervention for children with autism spectrum disorders using an animal robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482655403&CFID=105753156&CFTOKEN=18642945">Kwangsu Cho</a>, 
                        <a href="author_page.cfm?id=81482653464&CFID=105753156&CFTOKEN=18642945">Christine Shin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 399-400</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957801" title="DOI">10.1145/1957656.1957801</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957801&ftid=931042&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957801&ftid=931043&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow145" style="display:inline;"><br /><div style="display:inline">In this paper, we explore the possibility of using animal robot for teaching social behaviors, especially caregiving behavior to children with Autistic Spectrum Disorders (ASD) and Pervasive Development Disorder (PDD).</div></span>
          <span id="toHide145" style="display:none;"><br /><div style="display:inline"><p>In this paper, we explore the possibility of using animal robot for teaching social behaviors, especially caregiving behavior to children with Autistic Spectrum Disorders (ASD) and Pervasive Development Disorder (PDD).</p></div></span> <a id="expcoll145" href="JavaScript: expandcollapse('expcoll145',145)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957802&CFID=105753156&CFTOKEN=18642945">Humanoid robot control using depth camera</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482659370&CFID=105753156&CFTOKEN=18642945">Halit Bener Suay</a>, 
                        <a href="author_page.cfm?id=81333488007&CFID=105753156&CFTOKEN=18642945">Sonia Chernova</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 401-402</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957802" title="DOI">10.1145/1957656.1957802</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957802&ftid=931044&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957802&ftid=931045&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow146" style="display:inline;"><br /><div style="display:inline">Most human interactions with the environment depend on our ability to navigate freely and to use our hands and arms to manipulate objects. Developing natural means of controlling these abilities in humanoid robots can significantly broaden the usability ...</div></span>
          <span id="toHide146" style="display:none;"><br /><div style="display:inline"><p>Most human interactions with the environment depend on our ability to navigate freely and to use our hands and arms to manipulate objects. Developing natural means of controlling these abilities in humanoid robots can significantly broaden the usability of such platforms. An ideal interface for humanoid robot teleoperation will be inexpensive, person-independent, require no wearable equipment, and will be easy to use, requiring little or no user training.</p> <p>This work presents a new humanoid robot control and interaction interface that uses depth images and skeletal tracking software to control the navigation, gaze and arm gestures of a humanoid robot. To control the robot, the user stands in front of a depth camera and assumes a specific pose to initiate skeletal tracking. The initial location of the user automatically becomes the origin of the control coordinate system. The user can then use leg and arm gestures to turn the robot's motors on and off, to switch operation modes and to control the behavior of the robot. We present two control modes. The body control mode enables the user to control the arms and navigation direction of the robot using the person's own arms and location, respectively. The gaze direction control mode enables the user to control the focus of attention of the robot by pointing with one hand, while giving commands through gestures of the other hand. We present a demonstration of this interface, in which a combination of these two control modes is used to successfully enable an Aldebaran Nao robot to carry an object from one location to another. Our work makes use of the Microsoft Kinect depth sensor.</p></div></span> <a id="expcoll146" href="JavaScript: expandcollapse('expcoll146',146)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957803&CFID=105753156&CFTOKEN=18642945">ShakeTime!: a deceptive robot referee</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482640887&CFID=105753156&CFTOKEN=18642945">Marynel V&#225;zquez</a>, 
                        <a href="author_page.cfm?id=81482655580&CFID=105753156&CFTOKEN=18642945">Alexander May</a>, 
                        <a href="author_page.cfm?id=81100236968&CFID=105753156&CFTOKEN=18642945">Aaron Steinfeld</a>, 
                        <a href="author_page.cfm?id=81436594835&CFID=105753156&CFTOKEN=18642945">Wei-Hsuan Chen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 403-404</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957803" title="DOI">10.1145/1957656.1957803</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957803&ftid=931046&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957803&ftid=927835&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow147" style="display:inline;"><br /><div style="display:inline">We explore deception in the context of a multi-player robotic game. The robot does not participate as a competitor, but is in charge of declaring who wins or loses every round. The robot was designed to deceive game players by imperceptibly balancing ...</div></span>
          <span id="toHide147" style="display:none;"><br /><div style="display:inline"><p>We explore deception in the context of a multi-player robotic game. The robot does not participate as a competitor, but is in charge of declaring who wins or loses every round. The robot was designed to deceive game players by imperceptibly balancing how much they won, with the hope this behavior would make them play longer and with more interest. Inducing false belief about who wins the game was accomplished by leveraging paradigms about robot behavior and their better perceptual abilities. Results include the finding that participants were more accepting of lying by our robot than for robots in general. Some participants found the balancing strategy favorable after being debriefed, while others showed less interest due to a perceived level of unfairness.</p></div></span> <a id="expcoll147" href="JavaScript: expandcollapse('expcoll147',147)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957804&CFID=105753156&CFTOKEN=18642945">Tots on bots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482646648&CFID=105753156&CFTOKEN=18642945">Madeline E. Smith</a>, 
                        <a href="author_page.cfm?id=81100030684&CFID=105753156&CFTOKEN=18642945">Sharon Stansfield</a>, 
                        <a href="author_page.cfm?id=81482642997&CFID=105753156&CFTOKEN=18642945">Carole W. Dennis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 405-406</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957804" title="DOI">10.1145/1957656.1957804</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957804&ftid=931047&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957804&ftid=931048&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow148" style="display:inline;"><br /><div style="display:inline">Tots on Bots is a research project developing robotic-based mobility platforms for children with motor impairments. Independent mobility is crucial in the development of typical infants, and is missed by children with physical disabilities. We are looking ...</div></span>
          <span id="toHide148" style="display:none;"><br /><div style="display:inline"><p>Tots on Bots is a research project developing robotic-based mobility platforms for children with motor impairments. Independent mobility is crucial in the development of typical infants, and is missed by children with physical disabilities. We are looking to provide mobility to children as young as six months old using robot-powered devices. Children use the system by sitting on top of a Wii Fit Balance Board, which is seated on a Pioneer 3 robot. Our software allows infants to "drive" by leaning to one side or reaching, with sonar sensors and a remote control for added safety. This video explains the need for such a system, how it is built, some clips of children during our pilot testing phase, and discusses future work.</p></div></span> <a id="expcoll148" href="JavaScript: expandcollapse('expcoll148',148)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957805&CFID=105753156&CFTOKEN=18642945">A wheelchair which can automatically move alongside a caregiver</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100604144&CFID=105753156&CFTOKEN=18642945">Yoshinori Kobayashi</a>, 
                        <a href="author_page.cfm?id=81456634964&CFID=105753156&CFTOKEN=18642945">Yuki Kinpara</a>, 
                        <a href="author_page.cfm?id=81482653945&CFID=105753156&CFTOKEN=18642945">Erii Takano</a>, 
                        <a href="author_page.cfm?id=81100082332&CFID=105753156&CFTOKEN=18642945">Yoshinori Kuno</a>, 
                        <a href="author_page.cfm?id=81350572857&CFID=105753156&CFTOKEN=18642945">Keiichi Yamazaki</a>, 
                        <a href="author_page.cfm?id=81100620449&CFID=105753156&CFTOKEN=18642945">Akiko Yamazaki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 407-408</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957805" title="DOI">10.1145/1957656.1957805</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957805&ftid=931049&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957805&ftid=931050&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow149" style="display:inline;"><br /><div style="display:inline">This video presents our ongoing work developing a robotic wheelchair that can move automatically alongside a caregiver. Recently, several robotic/intelligent wheelchairs possessing autonomous functions for reaching a goal and/or user-friendly interfaces ...</div></span>
          <span id="toHide149" style="display:none;"><br /><div style="display:inline"><p>This video presents our ongoing work developing a robotic wheelchair that can move automatically alongside a caregiver. Recently, several robotic/intelligent wheelchairs possessing autonomous functions for reaching a goal and/or user-friendly interfaces have been proposed. Although ideally wheelchair users may wish to go out alone, they are often accompanied by caregivers. Therefore, it is important to consider how to reduce the caregivers' load and support their activities and facilitate communication between the wheelchair user and caregiver. Moreover, a sociologist pointed out that when a wheelchair user is accompanied by a companion, the latter is inevitably seen as a caregiver [1]. In other words, the equality of the relationship is publicly undermined when the wheelchair is pushed by a companion. Hence, we propose a robotic wheelchair which can move alongside a caregiver or companion, and facilitate easy communication between them and the wheelchair user. However, it is not always desirable for a caregiver to be alongside a wheelchair. For instance, a caregiver may step in front of the wheelchair to open a door, and pedestrians may be encumbered by the wheelchair and companion if they move along side-by-side in a narrow corridor. To cope with these problems, our robotic wheelchair can move alongside a caregiver collaboratively depending on the circumstances. A laser range sensor is employed to track the caregiver and observe the environment around the wheelchair [2]. When obstacles are detected in the wheelchair's path of motion, it adjusts its position accordingly. In the video we demonstrate these functions of our robotic wheelchair. We are now conducting experiments to confirm the effectiveness of our wheelchair at an elderly care center in Japan.</p></div></span> <a id="expcoll149" href="JavaScript: expandcollapse('expcoll149',149)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957806&CFID=105753156&CFTOKEN=18642945">Who explains it?: avoiding the feeling of third-person helpers in auditory instruction for older people</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414597701&CFID=105753156&CFTOKEN=18642945">Hirotaka Osawa</a>, 
                        <a href="author_page.cfm?id=81416608721&CFID=105753156&CFTOKEN=18642945">Jarrod Orszulak</a>, 
                        <a href="author_page.cfm?id=81100408150&CFID=105753156&CFTOKEN=18642945">Kathryn M. Godfrey</a>, 
                        <a href="author_page.cfm?id=81321500077&CFID=105753156&CFTOKEN=18642945">Seiji Yamada</a>, 
                        <a href="author_page.cfm?id=81392614852&CFID=105753156&CFTOKEN=18642945">Joseph F. Coughlin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 409-410</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957806" title="DOI">10.1145/1957656.1957806</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957806&ftid=931051&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957806&ftid=931052&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow150" style="display:inline;"><br /><div style="display:inline">Auditory instruction is a well used method for people of all ages because of its understandability. However the additional voice has the possibility to disturb the user's learning during the instruction because it strongly implies the support of third-person ...</div></span>
          <span id="toHide150" style="display:none;"><br /><div style="display:inline"><p>Auditory instruction is a well used method for people of all ages because of its understandability. However the additional voice has the possibility to disturb the user's learning during the instruction because it strongly implies the support of third-person helpers. This risk increases with older people because their confidence in their ability may decline compared to the younger people. The authors propose a method to anthropomorphize an instructed target (a vacuum) to decrease the feeling of a third person during instruction. The authors conducted the experiment using our method to explain features of household appliance and evaluated the relationship between recalled features and older people's internal scale. The results show that older people remembered more features by using our method, and with female participants, their internal scales increased during the training. This demonstrates that our method can decrease the third-person feeling in female participants and increase the amount learned. Our findings suggest that auditory instructions may be an effective learning method for older adults.</p></div></span> <a id="expcoll150" href="JavaScript: expandcollapse('expcoll150',150)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957807&CFID=105753156&CFTOKEN=18642945">Snappy: snapshot-based robot interaction for arranging objects</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81363603235&CFID=105753156&CFTOKEN=18642945">Sunao Hashimoto</a>, 
                        <a href="author_page.cfm?id=81456640368&CFID=105753156&CFTOKEN=18642945">Andrei Ostanin</a>, 
                        <a href="author_page.cfm?id=81100424140&CFID=105753156&CFTOKEN=18642945">Masahiko Inami</a>, 
                        <a href="author_page.cfm?id=81100444444&CFID=105753156&CFTOKEN=18642945">Takeo Igarashi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 411-412</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957807" title="DOI">10.1145/1957656.1957807</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957807&ftid=931053&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957807&ftid=931054&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow151" style="display:inline;"><br /><div style="display:inline">Photograph is a very useful tool for describing configurations of real-world objects to others. People immediately understand various pieces of information such as "what is the target object" and "where is the target position" by looking at a photograph, ...</div></span>
          <span id="toHide151" style="display:none;"><br /><div style="display:inline"><p>Photograph is a very useful tool for describing configurations of real-world objects to others. People immediately understand various pieces of information such as "what is the target object" and "where is the target position" by looking at a photograph, even without verbal descriptions. Our goal was to leverage these features of photographs to enrich human-robot interactions. We propose to use photographs as a front-end between a human and a home robot system. We named this method "Snappy". The user takes a photo to remember the target in a real-world situation involving a task and shows it to the system to make it physically execute the task. We developed a prototype system in which the user took a photo of a dish layout on a table and showed it to the system later to then have robots deliver and arrange the dishes in the same way.</p></div></span> <a id="expcoll151" href="JavaScript: expandcollapse('expcoll151',151)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957808&CFID=105753156&CFTOKEN=18642945">Robot games for elderly</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414609902&CFID=105753156&CFTOKEN=18642945">S&#248;ren Tranberg Hansen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 413-414</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957808" title="DOI">10.1145/1957656.1957808</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957808&ftid=931055&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957808&ftid=931056&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow152" style="display:inline;"><br /><div style="display:inline">This video presents a study on how a physical game based on a mobile robot can be used as a persuasive tool for promoting physical activity among elderly. The goal of the game is to take a ball from a robot, and afterwards try to hand it back while the ...</div></span>
          <span id="toHide152" style="display:none;"><br /><div style="display:inline"><p>This video presents a study on how a physical game based on a mobile robot can be used as a persuasive tool for promoting physical activity among elderly. The goal of the game is to take a ball from a robot, and afterwards try to hand it back while the robot moves. The robot records the behavior patterns of each individual player and gradually adapts the challenge of the game to the player's skill. The game was investigated in two independent field studies. The primary goal was to observe how the robot adapts to players with different mobility problems, secondly to obtain knowledge about the different play patterns and get ideas about future improvements of the game. The video shows different examples of how the elderly would play with the robot and illustrates the variety of play styles.</p></div></span> <a id="expcoll152" href="JavaScript: expandcollapse('expcoll152',152)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957809&CFID=105753156&CFTOKEN=18642945">Selecting and commanding groups in a multi-robot vision based system</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482650597&CFID=105753156&CFTOKEN=18642945">Brian Milligan</a>, 
                        <a href="author_page.cfm?id=81100500947&CFID=105753156&CFTOKEN=18642945">Greg Mori</a>, 
                        <a href="author_page.cfm?id=81100253690&CFID=105753156&CFTOKEN=18642945">Richard T. Vaughan</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 415-416</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957809" title="DOI">10.1145/1957656.1957809</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957809&ftid=931057&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow153" style="display:inline;"><br /><div style="display:inline">We present a novel method for a human user to select groups of robots without using any external instruments. We use computer vision techniques to read hand gestures from a user and use the hand gesture information to select single or multiple robots ...</div></span>
          <span id="toHide153" style="display:none;"><br /><div style="display:inline"><p>We present a novel method for a human user to select groups of robots without using any external instruments. We use computer vision techniques to read hand gestures from a user and use the hand gesture information to select single or multiple robots from a population and assign them to a task. To select robots the user simply draws a circle in the air around the robots that the user wants to command. Once the user selects the group of robots, he or she can send them to a location by pointing to a target location.</p> <p>To achieve this we use cameras mounted on mobile robots to find the user's face and then track his or her hand. Our method exploits an observation from human-robot interaction on pointing, which found a human's target when pointing is best inferred using the line from the human's eyes to the user's extended hand [1]. When circling robots the projected eye-to-hand lines forms a cone-like shape that envelops the selected robots. From a 2D camera mounted on the robot, this cone is seen with the user's face as the vertex and the hand movements as a circular slice of the cone. We show in the video how the robots can tell if they have been selected by testing to see if the face is within the circle made by the hand. If the face is within the circle then the robot was selected, if the face is outside the circle it was not selected.</p> <p>Following selection the robots then read a command by looking for a pointing gesture, which is detected by an outreached hand. From the pointing gesture the robots collectively infer which target is pointing at by calculating the distance and direction that the hand moved to relative to the face. The selected robots then travel to the target, and unselected robots can then be selected and commanded as desired.</p> <p>The robots communicate their state to the user through LED lights on the robots chassis. When a robot is searching for the user's face the LEDs flash to get the user's attention (as it is easiest to find frontal faces). When the robots find the users face the lights become a solid yellow to indicate that they are ready to be selected. When selected, the robots' LEDs turn blue to indicate they can now be commanded. Once robots are sent off to a location, remaining robots can then be selected and assigned another task.</p> <p>We demonstrate this method working on low powered Atom Netbooks and off the shelf USB web cameras. This shows the first working implementation of a system that allows a human to select and command groups of robots with out using any external instruments.</p></div></span> <a id="expcoll153" href="JavaScript: expandcollapse('expcoll153',153)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Ontologies</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Aaron Steinfeld 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957811&CFID=105753156&CFTOKEN=18642945">Intelligent humanoid robot with japanese Wikipedia ontology and robot action ontology</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482654209&CFID=105753156&CFTOKEN=18642945">Shotaro Kobayashi</a>, 
                        <a href="author_page.cfm?id=81474698992&CFID=105753156&CFTOKEN=18642945">Susumu Tamagawa</a>, 
                        <a href="author_page.cfm?id=81343501134&CFID=105753156&CFTOKEN=18642945">Takeshi Morita</a>, 
                        <a href="author_page.cfm?id=81100449998&CFID=105753156&CFTOKEN=18642945">Takahira Yamaguchi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 417-424</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957811" title="DOI">10.1145/1957656.1957811</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957811&ftid=931059&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow155" style="display:inline;"><br /><div style="display:inline">WioNA (Wikipedia Ontology NAo) is proposed to build much better HRI by integrating four elements: Japanese speech interface, semantic interpretation, Japanese Wikipedia Ontology and Robot Action Ontology. WioNA is implemented on a humanoid robot "Nao". ...</div></span>
          <span id="toHide155" style="display:none;"><br /><div style="display:inline"><p>WioNA (Wikipedia Ontology NAo) is proposed to build much better HRI by integrating four elements: Japanese speech interface, semantic interpretation, Japanese Wikipedia Ontology and Robot Action Ontology. WioNA is implemented on a humanoid robot "Nao". In WioNA, we developed two ontologies: Japanese Wikipedia Ontology and Robot Action Ontology. Japanese Wikipedia Ontology has a large size of concept hierarchy and instance network with many properties from Japanese Wikipedia (semi) automatically. By giving Japanese Wikipedia Ontology to Nao as wisdom, Nao can dialogue with users on many topics of various fields. Robot Action Ontology, in contrast, is built by organizing various performable actions of Nao to control and generate robot actions. Aligning Robot Action Ontology with Japanese Wikipedia Ontology enables Nao to perform related actions to dialogue topics. To show the validities of WioNA, we describe human-robot conversation logs of two case studies whose dialogue topics are sport and rock singer. These case studies show us how HRI goes well in WioNA with these topics.</p></div></span> <a id="expcoll155" href="JavaScript: expandcollapse('expcoll155',155)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957812&CFID=105753156&CFTOKEN=18642945">Using semantic technologies to describe robotic embodiments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482651042&CFID=105753156&CFTOKEN=18642945">Alex Juarez</a>, 
                        <a href="author_page.cfm?id=81100461702&CFID=105753156&CFTOKEN=18642945">Christoph Bartneck</a>, 
                        <a href="author_page.cfm?id=81405593355&CFID=105753156&CFTOKEN=18642945">Loe Feijs</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 425-432</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957812" title="DOI">10.1145/1957656.1957812</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957812&ftid=931060&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow156" style="display:inline;"><br /><div style="display:inline">This paper presents our approach to using semantic technologies to describe robot embodiments. We introduce a prototype implementation of RoboDB, a robot database based on semantic web technologies with the functionality necessary to store meaningful ...</div></span>
          <span id="toHide156" style="display:none;"><br /><div style="display:inline"><p>This paper presents our approach to using semantic technologies to describe robot embodiments. We introduce a prototype implementation of RoboDB, a robot database based on semantic web technologies with the functionality necessary to store meaningful information about the robot's body structure. We present a heuristic evaluation of the user interface to the system, and discuss the possibilities of using the semantic information gathered in the database for applications like building a robot ontology, and the development of robot middleware systems.</p></div></span> <a id="expcoll156" href="JavaScript: expandcollapse('expcoll156',156)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>User preferences</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Andrea Thomaz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957814&CFID=105753156&CFTOKEN=18642945">Robot self-initiative and personalization by learning through repeated interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482641924&CFID=105753156&CFTOKEN=18642945">Martin Mason</a>, 
                        <a href="author_page.cfm?id=81100368093&CFID=105753156&CFTOKEN=18642945">Manuel C. Lopes</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 433-440</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957814" title="DOI">10.1145/1957656.1957814</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957814&ftid=931061&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow158" style="display:inline;"><br /><div style="display:inline">We have developed a robotic system that interacts with the user, and through repeated interactions, adapts to the user so that the system becomes semi-autonomous and acts proactively. In this work we show how to design a system to meet a user's preferences, ...</div></span>
          <span id="toHide158" style="display:none;"><br /><div style="display:inline"><p>We have developed a robotic system that interacts with the user, and through repeated interactions, adapts to the user so that the system becomes semi-autonomous and acts proactively. In this work we show how to design a system to meet a user's preferences, show how robot pro-activity can be learned and provide an integrated system using verbal instructions. All these behaviors are implemented in a real platform that achieves all these behaviors and is evaluated in terms of user acceptability and efficiency of interaction.</p></div></span> <a id="expcoll158" href="JavaScript: expandcollapse('expcoll158',158)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957815&CFID=105753156&CFTOKEN=18642945">Modeling environments from a route perspective</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482661849&CFID=105753156&CFTOKEN=18642945">Luis Yoichi Morales Saiki</a>, 
                        <a href="author_page.cfm?id=81321498073&CFID=105753156&CFTOKEN=18642945">Satoru Satake</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105753156&CFTOKEN=18642945">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105753156&CFTOKEN=18642945">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 441-448</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957815" title="DOI">10.1145/1957656.1957815</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957815&ftid=931062&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=1957815&ftid=932777&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow159" style="display:inline;"><br /><div style="display:inline">Environment attributes are perceived or remembered differently according to the perspective used. In this study, two different perspectives, a survey perspective and a route perspective, are explained and discussed. This paper proposes an approach for ...</div></span>
          <span id="toHide159" style="display:none;"><br /><div style="display:inline"><p>Environment attributes are perceived or remembered differently according to the perspective used. In this study, two different perspectives, a survey perspective and a route perspective, are explained and discussed. This paper proposes an approach for modeling human environments from a route perspective, which is the perspective used when a human navigates through the environment. The process for route perspective semi-autonomous data extraction and modeling by a mobile robot equipped with a laser sensor and a camera is detailed. Finally, as an example of a route perspective application, a route direction robot was developed and tested in a real mall environment. Experimental results show the advantages of the proposed route perspective model compared with a survey perspective approach. Moreover, the route model is comparable to the performance of an expert person giving route guidance in the mall.</p></div></span> <a id="expcoll159" href="JavaScript: expandcollapse('expcoll159',159)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957816&CFID=105753156&CFTOKEN=18642945">Do elderly people prefer a conversational humanoid as a shopping assistant partner in supermarkets?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482653559&CFID=105753156&CFTOKEN=18642945">Yamato Iwamura</a>, 
                        <a href="author_page.cfm?id=81310499910&CFID=105753156&CFTOKEN=18642945">Masahiro Shiomi</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105753156&CFTOKEN=18642945">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105753156&CFTOKEN=18642945">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105753156&CFTOKEN=18642945">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 449-456</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957816" title="DOI">10.1145/1957656.1957816</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957816&ftid=932778&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMpeg" title="Other Formats Mpeg" href="ft_gateway.cfm?id=1957816&ftid=932779&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mpeg.gif" alt="Mpeg" class="fulltext_lnk" border="0" />Mpeg</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow160" style="display:inline;"><br /><div style="display:inline">Assistive robots can be perceived in two main ways: tools or partners. In past research, assistive robots that offer physical assistance for the elderly are often designed in the context of a tool metaphor. This paper investigates the effect of two design ...</div></span>
          <span id="toHide160" style="display:none;"><br /><div style="display:inline"><p>Assistive robots can be perceived in two main ways: tools or partners. In past research, assistive robots that offer physical assistance for the elderly are often designed in the context of a tool metaphor. This paper investigates the effect of two design considerations for assistive robots in a partner metaphor: <i>conversation</i> and <i>robot-type</i>. The former factor is concerned with whether robots should converse with people even if the conversation is not germane for completing the task. The latter factor is concerned with whether people prefer a communication/function oriented design for assistive robots. To test these design considerations, we selected a shopping assistance situation where a robot carries a shopping basket for elderly people, which is one typical scenario used for assistive robots. A field experiment was conducted in a real supermarket in Japan where 24 elderly participants shopped with robots. The experimental results revealed that they prefer a conversational humanoid as a shopping assistant partner.</p></div></span> <a id="expcoll160" href="JavaScript: expandcollapse('expcoll160',160)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Robot touch</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Takayuki Kanda 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957818&CFID=105753156&CFTOKEN=18642945">Touched by a robot: an investigation of subjective responses to robot-initiated touch</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456608843&CFID=105753156&CFTOKEN=18642945">Tiffany L. Chen</a>, 
                        <a href="author_page.cfm?id=81474687873&CFID=105753156&CFTOKEN=18642945">Chih-Hung King</a>, 
                        <a href="author_page.cfm?id=81310502411&CFID=105753156&CFTOKEN=18642945">Andrea L. Thomaz</a>, 
                        <a href="author_page.cfm?id=81100204143&CFID=105753156&CFTOKEN=18642945">Charles C. Kemp</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 457-464</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957818" title="DOI">10.1145/1957656.1957818</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957818&ftid=932780&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1957818&ftid=932781&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow162" style="display:inline;"><br /><div style="display:inline">By initiating physical contact with people, robots can be more useful. For example, a robotic caregiver might make contact to provide physical assistance or facilitate communication. So as to better understand how people respond to robot-initiated touch, ...</div></span>
          <span id="toHide162" style="display:none;"><br /><div style="display:inline"><p>By initiating physical contact with people, robots can be more useful. For example, a robotic caregiver might make contact to provide physical assistance or facilitate communication. So as to better understand how people respond to robot-initiated touch, we conducted a 2x2 between-subjects experiment with 56 people in which a robotic nurse autonomously touched and wiped the subject's forearm. Our independent variables were whether or not the robot verbally <i>warned</i> the person before contact, and whether the robot verbally indicated that the touch was intended to clean the person's skin (<i>instrumental touch</i>) or to provide comfort (<i>affective touch</i>). On average, regardless of the treatment, participants had a generally positive subjective response. However, with instrumental touch people responded significantly more favorably. Since the physical behavior of the robot was the same for all trials, our results demonstrate that the perceived intent of the robot can significantly influence a person's subjective response to robot-initiated touch. Our results suggest that roboticists should consider this factor in addition to the mechanics of physical interaction. Unexpectedly, we found that participants tended to respond more favorably without a verbal warning. Although inconclusive, our results suggest that verbal warnings prior to contact should be carefully designed, if used at all.</p></div></span> <a id="expcoll162" href="JavaScript: expandcollapse('expcoll162',162)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957819&CFID=105753156&CFTOKEN=18642945">Effect of robot's active touch on people's motivation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456622254&CFID=105753156&CFTOKEN=18642945">Kayako Nakagawa</a>, 
                        <a href="author_page.cfm?id=81310499910&CFID=105753156&CFTOKEN=18642945">Masahiro Shiomi</a>, 
                        <a href="author_page.cfm?id=81100649131&CFID=105753156&CFTOKEN=18642945">Kazuhiko Shinozawa</a>, 
                        <a href="author_page.cfm?id=81482654932&CFID=105753156&CFTOKEN=18642945">Reo Matsumura</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105753156&CFTOKEN=18642945">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105753156&CFTOKEN=18642945">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 465-472</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957819" title="DOI">10.1145/1957656.1957819</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957819&ftid=932782&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957819&ftid=932783&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow163" style="display:inline;"><br /><div style="display:inline">This paper presents the effect of a robot's active touch for improving people's motivation. For services in the education and healthcare fields, a robot might be useful for improving the motivation of performing such repetitive and monotonous tasks as ...</div></span>
          <span id="toHide163" style="display:none;"><br /><div style="display:inline"><p>This paper presents the effect of a robot's active touch for improving people's motivation. For services in the education and healthcare fields, a robot might be useful for improving the motivation of performing such repetitive and monotonous tasks as exercising or taking medicine. Previous research demonstrated with a robot the effect of user touch on improving its impressions, but they did not clarify whether a robot's touch, especially an active touch, has enough influence on people's motive. We implemented an active touch behavior and experimentally investigated its effect on motivation. In the experiment, a robot requested participants to perform a monotonous task with a robot's active touch, a passive touch, or no touch. The result of experiment showed that an active touch by a robot increased the number of working actions and the amount of working time for the task. This suggests that a robot's active touch can support people to improve their motivation. We believe that a robot's active touch behavior is useful for such robot's services as education and healthcare.</p></div></span> <a id="expcoll163" href="JavaScript: expandcollapse('expcoll163',163)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957820&CFID=105753156&CFTOKEN=18642945">Design and assessment of the haptic creature's affect display</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100236385&CFID=105753156&CFTOKEN=18642945">Steve Yohanan</a>, 
                        <a href="author_page.cfm?id=81100608981&CFID=105753156&CFTOKEN=18642945">Karon E. MacLean</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 473-480</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957820" title="DOI">10.1145/1957656.1957820</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957820&ftid=932784&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow164" style="display:inline;"><br /><div style="display:inline">The Haptic Creature is a small, animal-like robot we have developed to investigate the role of touch in communicating emotions between humans and robots. This paper presents a study examining how successful our robot is at communicating its emotional ...</div></span>
          <span id="toHide164" style="display:none;"><br /><div style="display:inline"><p>The Haptic Creature is a small, animal-like robot we have developed to investigate the role of touch in communicating emotions between humans and robots. This paper presents a study examining how successful our robot is at communicating its emotional state through touch. Results show that, regardless of the human's gender or background with animals, the robot is effective in communicating its state of arousal but less so for valence. Also included are descriptions of the design of the Haptic Creature's emotion model and suggested improvements based on results of the study.</p></div></span> <a id="expcoll164" href="JavaScript: expandcollapse('expcoll164',164)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Nonverbal interaction</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Michael Goodrich 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957822&CFID=105753156&CFTOKEN=18642945">Learning to interpret pointing gestures with a time-of-flight camera</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81447604332&CFID=105753156&CFTOKEN=18642945">David Droeschel</a>, 
                        <a href="author_page.cfm?id=81456635196&CFID=105753156&CFTOKEN=18642945">J&#246;rg St&#252;ckler</a>, 
                        <a href="author_page.cfm?id=81100455319&CFID=105753156&CFTOKEN=18642945">Sven Behnke</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 481-488</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957822" title="DOI">10.1145/1957656.1957822</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957822&ftid=932785&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow166" style="display:inline;"><br /><div style="display:inline">Pointing gestures are a common and intuitive way to draw somebody's attention to a certain object. While humans can easily interpret robot gestures, the perception of human behavior using robot sensors is more difficult. In this work, we propose a method ...</div></span>
          <span id="toHide166" style="display:none;"><br /><div style="display:inline"><p>Pointing gestures are a common and intuitive way to draw somebody's attention to a certain object. While humans can easily interpret robot gestures, the perception of human behavior using robot sensors is more difficult.</p> <p>In this work, we propose a method for perceiving pointing gestures using a Time-of-Flight (ToF) camera. To determine the intended pointing target, frequently the line between a person's eyes and hand is assumed to be the pointing direction. However, since people tend to keep the line-of-sight free while they are pointing, this simple approximation is inadequate. Moreover, depending on the distance and angle to the pointing target, the line between shoulder and hand or elbow and hand may yield better interpretations of the pointing direction. In order to achieve a better estimate, we extract a set of body features from depth and amplitude images of a ToF camera and train a model of pointing directions using Gaussian Process Regression.</p> <p>We evaluate the accuracy of the estimated pointing direction in a quantitative study. The results show that our learned model achieves far better accuracy than simple criteria like head-hand, shoulder-hand, or elbow-hand line.</p></div></span> <a id="expcoll166" href="JavaScript: expandcollapse('expcoll166',166)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957823&CFID=105753156&CFTOKEN=18642945">Using spatial and temporal contrast for fluent robot-human hand-overs</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414594397&CFID=105753156&CFTOKEN=18642945">Maya Cakmak</a>, 
                        <a href="author_page.cfm?id=81453611472&CFID=105753156&CFTOKEN=18642945">Siddhartha S. Srinivasa</a>, 
                        <a href="author_page.cfm?id=81414592368&CFID=105753156&CFTOKEN=18642945">Min Kyung Lee</a>, 
                        <a href="author_page.cfm?id=81100476487&CFID=105753156&CFTOKEN=18642945">Sara Kiesler</a>, 
                        <a href="author_page.cfm?id=81100492013&CFID=105753156&CFTOKEN=18642945">Jodi Forlizzi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 489-496</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957823" title="DOI">10.1145/1957656.1957823</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957823&ftid=932786&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1957823&ftid=932787&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow167" style="display:inline;"><br /><div style="display:inline">For robots to get integrated in daily tasks assisting humans, robot-human interactions will need to reach a level of fluency close to that of human-human interactions. In this paper we address the fluency of robot-human hand-overs. From an observational ...</div></span>
          <span id="toHide167" style="display:none;"><br /><div style="display:inline"><p>For robots to get integrated in daily tasks assisting humans, robot-human interactions will need to reach a level of fluency close to that of human-human interactions. In this paper we address the fluency of robot-human hand-overs. From an observational study with our robot HERB, we identify the key problems with a baseline hand-over action. We find that the failure to convey the intention of handing over causes delays in the transfer, while the lack of an intuitive signal to indicate timing of the hand-over causes early, unsuccessful attempts to take the object. We propose to address these problems with the use of <i>spatial</i> contrast, in the form of distinct hand-over poses, and <i>temporal</i> contrast, in the form of unambiguous transitions to the hand-over pose. We conduct a survey to identify distinct hand-over poses, and determine variables of the pose that have most communicative potential for the intent of handing over. We present an experiment that analyzes the effect of the two types of contrast on the fluency of hand-overs. We find that temporal contrast is particularly useful in improving fluency by eliminating early attempts of the human.</p></div></span> <a id="expcoll167" href="JavaScript: expandcollapse('expcoll167',167)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1957824&CFID=105753156&CFTOKEN=18642945">Nonverbal robot-group interaction using an imitated gaze cue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81447593496&CFID=105753156&CFTOKEN=18642945">Nathan Kirchner</a>, 
                        <a href="author_page.cfm?id=81456640198&CFID=105753156&CFTOKEN=18642945">Alen Alempijevic</a>, 
                        <a href="author_page.cfm?id=81100387445&CFID=105753156&CFTOKEN=18642945">Gamini Dissanayake</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 497-504</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1957656.1957824" title="DOI">10.1145/1957656.1957824</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1957824&ftid=932788&dwn=1&CFID=105753156&CFTOKEN=18642945" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow168" style="display:inline;"><br /><div style="display:inline">Ensuring that a particular and unsuspecting member of a group is the recipient of a salient-item hand-over is a complicated interaction. The robot must effectively, expediently and reliably communicate its intentions to advert any tendency within the ...</div></span>
          <span id="toHide168" style="display:none;"><br /><div style="display:inline"><p>Ensuring that a particular and unsuspecting member of a group is the recipient of a salient-item hand-over is a complicated interaction. The robot must effectively, expediently and reliably communicate its intentions to advert any tendency within the group towards antinormative behaviour. In this paper, we study how a robot can establish the participant roles of such an interaction using imitated social and contextual cues. We designed two gaze cues, the first was designed to discourage antinormative behaviour through individualising a particular member of the group and the other to the contrary. We designed and conducted a field experiment (<i>456</i> participants in <i>64</i> trials) in which small groups of people (between <i>3</i> and <i>20</i> people) assembled in front of the robot, which then attempted to pass a salient object to a particular group member by presenting a physical cue, followed by one of two variations of a gaze cue. Our results showed that presenting the individualising cue had a significant (<i>z=3.733, p=0.0002</i>) effect on the robot's ability to ensure that an arbitrary group member did not take the salient object and that the selected participant did.</p></div></span> <a id="expcoll168" href="JavaScript: expandcollapse('expcoll168',168)">expand</a>
          </div>
		  </td>
          </tr>          
          
</table>


</div> 
</div>


 <p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>



 <br />
<div class="footerbody" align="center" >
	

	The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2012 ACM, Inc.<br />
	<a href="http://www.acm.org/publications/policies/usage">Terms of Usage</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;	  
	<a href="http://www.acm.org/about/contact-us">Contact Us</a>

<br /><br />
Useful downloads: 
<a href="http://www.adobe.com/products/acrobat/readstep2.html"><img src="http://dl.acm.org/images/pdf_logo.gif" width="16" height="16" alt="" border="0" /> Adobe Acrobat</a>
&nbsp;&nbsp;
<a href="http://www.apple.com/quicktime/download/" target="_blank"><img src="http://dl.acm.org/images/qtlogo.gif" width="16" height="16" alt="" border="0" /> QuickTime</a>
&nbsp;&nbsp;
<a href="http://www.microsoft.com/windows/windowsmedia/download/default.asp" target="_blank"><img src="http://dl.acm.org/images/wmv.gif" width="16" height="15" alt="" border="0" /> Windows Media Player</a>
&nbsp;&nbsp;
<a href="http://www.real.com/" target="_blank"><img src="http://dl.acm.org/images/realplayer.gif" width="20" height="18" alt="" border="0" /> Real Player</a>

</div> 



<div  id="cf_window1338242483146" class="yuiextdlg">
	
	<div  id="theguide_title" class="x-dlg-hd">
		The ACM Guide to Computing Literature
	 </div>
	<div  id="theguide_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242483149" class="yuiextdlg">
	
	<div  id="thetags_title" class="x-dlg-hd">
		All Tags
	 </div>
	<div  id="thetags_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242483152" class="yuiextdlg">
	
	<div  id="theformats_title" class="x-dlg-hd">
		Export Formats
	 </div>
	<div  id="theformats_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242483154" class="yuiextdlg">
	
	<div  id="theexplaination_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theexplaination_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242483156" class="yuiextdlg">
	
	<div  id="theservices_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theservices_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242483158" class="yuiextdlg">
	
	<div  id="savetobinder_title" class="x-dlg-hd">
		Save to Binder
	 </div>
	<div  id="savetobinder_body" class="x-dlg-bd">
		
		
	 </div>
 </div> 

</body>
</html>