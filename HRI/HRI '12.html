


<!doctype html>


<head><script type="text/javascript">_cf_loadingtexthtml="<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";
_cf_jsonprefix='//';
_cf_clientid='DD969CFC46751E1B4C0849CD7CF794AC';</script><script type="text/javascript" src="/CFIDE/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfform.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/masks.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/FCKeditor/fckeditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/adapter/yui/ext-yui-adapter.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/ext-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/resizable.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dragdrop/dragdrop.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/util.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/build/state/State-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/widget-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dialog/dialogs.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/cf/cf.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />



<title>Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
	
  --></style>
 


<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>


<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  function setDiv() {
	var m = document.getElementById('divmain');
	var mh = m.offsetHeight;
	var t = document.getElementById('divtools');
	var th = t.offsetHeight;
	var tg = document.getElementById('divtags');
	var tgh = tg.offsetHeight;
	var calcheight = mh - th;
	if (tgh > calcheight  ){
	  var x = (th + tgh) - mh;
	  if ( (th + tgh) - mh < 65) {
	  }
	  else {
		 document.getElementById('divtags').innerHTML = ""; 
		 var tg = document.getElementById('divtags');
		 var tgh = tg.offsetHeight;
		 tg.style.height = tgh  + 'px';
	  }
	}
	else {
		tg.style.height = calcheight + 'px';
		document.getElementById('divtags').innerHTML = "";
	}

//  do I need to check after I resize to be sure I didn't go too big?
//	var tg2 = document.getElementById('divtags');
//	var tgh2 = tg.offsetHeight;	
//	if (tgh2 > mh + 65) {
//	  var y = mh + 65;
//	  alert('expanded too much ' + tgh2 + ' should be at most ' + y);
//	  tg.style.height = y + 'px';
//	  document.getElementById('divtags').innerHTML = "";
//	}

  }
</script>

<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>



<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Yanco, Holly; General Chair-Steinfeld, Aaron; Program Chair-Evers, Vanessa; Program Chair-Jenkins, Odest Chadwicke"> <meta name="citation_title" content="Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction"> <meta name="citation_date" content="03/05/2012"> <meta name="citation_isbn" content="978-1-4503-1063-5"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=2157689"> 



<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFFORM');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFDIV');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFTEXTAREA');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFWINDOW');
</script>

<script type="text/javascript">
	var _cf_window_init_1338242675260=function()
	{
		_cf_bind_init_1338242675261=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide_body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242675261);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window1338242675259', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242675260);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242675263=function()
	{
		_cf_bind_init_1338242675264=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags_body','bindExpr':['showthetags.cfm?id=2157689']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242675264);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=2157689',{ modal:false, closable:true, divid:'cf_window1338242675262', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242675263);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242675266=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window1338242675265', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242675266);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242675268=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window1338242675267', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242675268);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242675270=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window1338242675269', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242675270);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242675272=function()
	{
		_cf_bind_init_1338242675273=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder_body','bindExpr':['savetobinder.cfm?id=2157689']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242675273);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=2157689',{ modal:false, closable:true, divid:'cf_window1338242675271', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242675272);
</script>
</head>

<body style="text-align:center" onLoad="window.focus();">

<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'http://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
                        
<script src='AC_RunActiveContent.js' type="text/javascript"></script>




<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>



<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
	
    <tr style="vertical-align:top">
		
		<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text"  ><img src="http://dl.acm.org/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
		</td>
        
        <td style="padding-left: 5px; padding-right:10px; padding-bottom:0px;" class="small-link-text">
        	<table style="width:100%; border-collapse:collapse; padding:0px">
			<tr><td style="text-align:center">
				
                            <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
                    
					</td>		
			</tr>
			</table> 
        </td>
		<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text">
			 <p style="margin-top:0px; margin-bottom:10px;">
					
                            <a href="https://dl.acm.org/signin.cfm?cfid=105753441&amp;cftoken=50690582" class="small-link-text" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
                            &nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm?cfid=105753441&amp;cftoken=50690582"  class="small-link-text" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
						
			 </p>
            
			<table style="padding: 5px; border-collapse:collapse; float:right">
				
				
                            
                            <tr>
                            <td class="small-link-text" style="text-align:right">
                            <form name="qiksearch" action="results.cfm?h=1&amp;cfid=105753441&amp;cftoken=50690582" method="post">
                           
                           
                            
                                     
                                    <span style="margin-left:0px"><label><input type="text" name="query" size="34" value=" " /></label>&nbsp;
                                    <input style="vertical-align:top;" type="image" alt="Search" name="Go" src="http://dl.acm.org/images/search_small.jpg" />
                                    
                                    </span>
							  </form>
                                </td>
                            </tr>
                          
				  
			</table>

			
			
		</td>

	</tr>
    
    
    <tr><td colspan="3" class="small-link-text" style="padding-bottom:5px; padding-top:0px; text-align:center">
		<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
         
         </td>
    </tr>
    </table>
	
<map name="port" id="port" > 
  <area shape="rect" coords="1,1,60,50" href="http://www.acm.org/" alt="ACM Home Page" />
  <area shape="rect" coords="65,1,275,68" href="http://dl.acm.org/dl.cfm?CFID=105753441&CFTOKEN=50690582" alt="ACM Digital Library Home Page" />
</map>

<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
	<tr style="vertical-align:top">
		<td style="padding-right:10px; text-align:left" class="small-link-text">
        	<div id="divmain" style="border:1px solid #356b20;">
				 
				<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
					
                    	<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;"><strong>Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction</strong></h1>
                        
                </div>
                
                  

<table class="medium-text" style="border-collapse:collapse; padding:0px;">

<col style="width:540px" />

<tr style="vertical-align:top">
  <td>
    <table style="border-collapse:collapse; padding:2px;" class="medium-text">
      <col style="width:80px;" />
      <col style="width:auto" />
      <tr style="vertical-align:top">
        
      </tr>
    </table>

	
        <table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
            <col style="width:80px" />
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             General Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a href="results.cfm?query=Name%3A%22Holly%20Yanco%22&querydisp=Name%3A%22Holly%20Yanco%22&termshow=matchboolean&coll=DL&dl=ACM&CFID=105753441&CFTOKEN=50690582" title="Search for Holly Yanco" target="_self">Holly Yanco</a>
                  
            </td>
            <td valign="bottom">
                
                        <small>University of Massachusetts Lowell, USA</small>
                    	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a href="results.cfm?query=Name%3A%22Aaron%20Steinfeld%22&querydisp=Name%3A%22Aaron%20Steinfeld%22&termshow=matchboolean&coll=DL&dl=ACM&CFID=105753441&CFTOKEN=50690582" title="Search for Aaron Steinfeld" target="_self">Aaron Steinfeld</a>
                  
            </td>
            <td valign="bottom">
                
                        <small>Carnegie Mellon University, USA</small>
                    	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             Program Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a href="results.cfm?query=Name%3A%22Vanessa%20Evers%22&querydisp=Name%3A%22Vanessa%20Evers%22&termshow=matchboolean&coll=DL&dl=ACM&CFID=105753441&CFTOKEN=50690582" title="Search for Vanessa Evers" target="_self">Vanessa Evers</a>
                  
            </td>
            <td valign="bottom">
                
                        <small>University of Amsterdam, The Netherlands</small>
                    	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a href="results.cfm?query=Name%3A%22Odest%20Chadwicke%20Jenkins%22&querydisp=Name%3A%22Odest%20Chadwicke%20Jenkins%22&termshow=matchboolean&coll=DL&dl=ACM&CFID=105753441&CFTOKEN=50690582" title="Search for Odest Chadwicke Jenkins" target="_self">Odest Chadwicke Jenkins</a>
                  
            </td>
            <td valign="bottom">
                
                        <small>Brown University, USA</small>
                    	
            </td>
            </tr>
            
        </table>
    

  </td>

  <td rowspan="20" nowrap="nowrap">
	<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
		<tr>
        	<td align="center" style="padding-bottom: 5px;">
			  
               <img src="http://portalparts.acm.org/2160000/2157689/thumb/cover_thumb.jpg" title="Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction" height="100"  width="75" ALT="Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction" /> 
              </td>
              <td valign="top" align="left" nowrap="nowrap">
	             <img src="images/acm_mini.jpg" title="Published by ACM" alt="Published by ACM" /> 2012 Proceeding<br />
                 
        	 </td>
        </tr>
        
        <tr>
        	<td colspan="2" valign="baseline" style="padding-bottom:5px;">
            <img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
            <a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
            </td>
         </tr>
         <tr>
            <td  class="small-text" colspan="2" valign="top" style="padding-left:30px;">
				
	                    	&middot;&nbsp;Downloads (6 Weeks): 1,262<br />
    	                    &middot;&nbsp;Downloads (12 Months): 2,630<br />
                          
                        &middot;&nbsp;Citation Count: 0 
			</td>
         </tr>

	</table>
  </td>
</tr>
</table>

<br clear="all" />

<table border="0" class="medium-text" cellpadding="2" cellspacing="0">

<tr valign="top">
    <td colspan="3"><table border="0" class="medium-text" cellpadding="1" cellspacing="0">

<tr valign="top">
    <td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>

	<tr valign="top">
    	<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
	</tr>
    <tr valign="top">
	    <td style="padding-left:10px;">
		   <a href="http://hri2012.org/" title="Conference Website"  target="_self" class="link-text">HRI'12</a> International Conference on Human-Robot Interaction 
        </td>
	</tr>
    
    <tr valign="top">
	    <td style="padding-left:10px; padding-bottom:10px"> Boston, MA, USA &mdash; March 05 - 08, 2012
                    
                  <br />
                    
                  <a href="http://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text">  &copy;2012</span> 
                  <br />           
                  
      </td>
	</tr>
	 

</table></td>
</tr>

</table>

                  
                 <br clear="all" />
			</div>
			
		</td>
		<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
	
            <div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
              <div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;"><strong>Tools and Resources</strong></h1></div>


<ul title="Tools and Resources" style="list-style: none; list-style-position:inside;
margin-left: 0px;
padding-left: 0em;
text-indent: 5px;
margin-bottom: 0px;">


<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:6px;">
   <span class="small-link-text">TOC Service:</span>
   	  
	  <img src="http://dl.acm.org/images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
      <ul style="margin-left: 0; padding-left: 0; display:inline;">
      	
        <li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
        <li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');"  class="small-link-text">RSS</a></li>
		        
      </ul>
    </span>
</li>

        <li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:6px;">
        <a href="citation.cfm?id=2157689&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
         </span></li>
    




<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Export Formats:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=2157689&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=2157689&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=2157689&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
      </ul>
    </span>
</li>



 
   <li style="list-style-image:url(img/calbullet.jpg);margin-top:15px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Upcoming Conference:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px; margin-left:25px;"><a href="http://humanrobotinteraction.org/2013/" title="ACM/IEEE International Conference on Human-Robot Interaction" class="small-link-text">HRI'13</a></li>
      </ul>
    </span>
	</li>
    


</ul>           

  <!-- ADDTHIS BUTTON BEGIN -->
  
  <!-- ADDTHIS BUTTON END -->

<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>
  <!-- AddThis Button BEGIN -->



<!-- AddThis Button BEGIN -->
<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_slashdot"></a>
<a class="addthis_button_reddit"></a>


<span class="addthis_separator">|</span>
<a href="http://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>
<!-- AddThis Button END -->

  
 

  
  
            </div>
            
		</td>
	</tr>
    
</table>



</div>


<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">




<div id="fback" style="text-align:left; padding-top:10px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="feedback" border="0" /></a>
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design"><strong>Feedback</strong></a>

<span style="padding:10px;">|</span>




<span>Switch to <a href="citation.cfm?id=2157689&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>


</span>

 
<div class="small-text" style="margin-top:10px; margin-bottom:5px;"> 
<br />

    <a href="#abstract"  title="Abstract" style="padding:5px"><span>Abstract</span></a> |
    
    <a href="#formats"  title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
    
    <a href="#authors"  title="Authors" style="padding:5px"><span>Authors</span></a> |
    <a href="#references"  title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
    <a href="#citedby"  title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
    <a href="#indexterms"  title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
    <a href="#source"  title="Publication" style="padding:5px"><span>Publication</span></a> |
    <a href="#revs"  title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |               
	<a href="#comments"  title="Comments" style="padding:5px"><span>Comments</span></a>
	
     |               
	<a href="#prox"  title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
    
</div>
    
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;"/>



</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
       	
			<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

			

		
			
           
			
				
				<p>
					<div style="display:inline"><p>Welcome to Boston! The Seventh Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI 2012) is a highly selective conference that aims to showcase the very best interdisciplinary and multidisciplinary research in human-robot interaction with roots in robotics, social psychology, cognitive science, HCI, human factors, artificial intelligence, engineering, and many more. We invite broad participation and encourage discussion and sharing of ideas across a diverse audience.</p> <p>Robotics is growing increasingly multidisciplinary as it moves towards realizing capable and collaborative robots that meet both human needs of society and technical challenges inherent in real world settings. A joining of the disciplines is essential for enabling robots to help people in their efforts to be more productive and enjoy a high quality of life. In particular, human-robot interaction requires advancement of the state-of-the-art in the empirical, algorithmic, mathematical, social, and engineering aspects of robotics in an integrated manner. Therefore, this year's theme is dedicated to <b>Robots in the Loop</b>, which highlights the importance of autonomously capable robots in enhancing the experiences of human users in everyday life and work activities. HRI 2012 emphasizes embodied robotic systems that operate, collaborate with, learn from, and meet the needs of human users in realworld environments.</p> <p>Full Papers submitted to the conference were thoroughly reviewed and discussed. The process utilized a rebuttal process and a worldwide team of dedicated, interdisciplinary reviewers. Subtle changes were made this year to help better pair reviewers to papers. This year's conference continues the tradition of selectivity with 34 out of 137 (25%) submissions accepted. Due to the joint sponsorship of ACM and IEEE, papers are archived in both the ACM Digital Library and IEEE Xplore.</p> <p>Accompanying the full papers are the brief and lightly reviewed Late Breaking Reports and Videos. For the former, 95 out of 111 (86%) two-page papers were accepted and will be presented as posters at the conference. For the latter, 16 of 30 (52%) short videos were accepted and will be presented during the video session.</p> <p>Rounding out the program are multiple keynote speakers who will discuss topics relevant to HRI, a panel session on telepresence, and several invited short unpublished talks designed to expose the audience to interesting work and motivate interdisciplinary discussion. The keynote speakers this year are Rodney Brooks and Karl Grammer.</p></div>
				</p>
   				
           	</div>
			
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


  <div class="abstract">
        <SPAN><strong>FRONT MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/2160000/2157689/fm/frontmatter.pdf?ip=188.194.239.219&CFID=105753441&CFTOKEN=50690582" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(title page, copyright, welcome, contents, organization, sponsor) 
  </div>          
  
  <div style="margin-top: 10px;"  class="abstract">
        <SPAN><strong>BACK MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/2160000/2157689/bm/backmatter.pdf?ip=188.194.239.219&CFID=105753441&CFTOKEN=50690582" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(author index) 
  </div>          
  
<div style="margin-top: 10px; height: auto; padding: 5px; ">
		
		
        


	</div>

<br clear="all" />
</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<dl title="Authors" style="margin-top:0px">




<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 General Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
										Search for <a title="Search for Holly Yanco" href="results.cfm?query=Name%3A%22Holly%20Yanco%22&amp;querydisp=Name%3A%22Holly%20Yanco%22&amp;termshow=matchboolean&amp;CFID=105753441&CFTOKEN=50690582" target="_self">Holly Yanco</a>
									  
								</span>
					
								<span>
									
                                    <br><br />
                                    
										Search for <a title="Search for Aaron Steinfeld" href="results.cfm?query=Name%3A%22Aaron%20Steinfeld%22&amp;querydisp=Name%3A%22Aaron%20Steinfeld%22&amp;termshow=matchboolean&amp;CFID=105753441&CFTOKEN=50690582" target="_self">Aaron Steinfeld</a>
									  
								</span>
					
			</dd>
                              



<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 Program Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
										Search for <a title="Search for Vanessa Evers" href="results.cfm?query=Name%3A%22Vanessa%20Evers%22&amp;querydisp=Name%3A%22Vanessa%20Evers%22&amp;termshow=matchboolean&amp;CFID=105753441&CFTOKEN=50690582" target="_self">Vanessa Evers</a>
									  
								</span>
					
								<span>
									
                                    <br><br />
                                    
										Search for <a title="Search for Odest Chadwicke Jenkins" href="results.cfm?query=Name%3A%22Odest%20Chadwicke%20Jenkins%22&amp;querydisp=Name%3A%22Odest%20Chadwicke%20Jenkins%22&amp;termshow=matchboolean&amp;CFID=105753441&CFTOKEN=50690582" target="_self">Odest Chadwicke Jenkins</a>
									  
								</span>
					
			</dd>
                              

</dl>
</div>

		  
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	References are not available

</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	Citings are not available
		
 </div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

Index Terms are not available


</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">



<table border="0" class="medium-text" cellpadding="0" cellspacing="5">



    <tr valign="top">
    	<td>Title</td> 
	    <td>
		   <a href="http://hri2012.org/" title="Conference Website"  target="_self" class="link-text">HRI'12</a> International Conference on Human-Robot Interaction 
        </td>
	</tr>
    <tr><td></td><td>Boston, MA, USA &mdash; March 05 - 08, 2012</td></tr> <tr><td>Pages</td><td>500</td></tr> 
                 <tr>
                 
                     <td>Sponsors</td>
                    
                  <td>
                  <a href="sig.cfm?id=SP918&CFID=105753441&CFTOKEN=50690582"> SIGART</a> ACM Special Interest Group on Artificial Intelligence
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="sig.cfm?id=SP923&CFID=105753441&CFTOKEN=50690582"> SIGCHI</a> ACM Special Interest Group on Computer-Human Interaction
                  </td>
                  </tr>
              
                 <tr>
                 
                     <td>In-Cooperations</td>
                    
                  <td>
                  <a name="sponsor"> IEEE-RAS</a> Robotics and Automation
                  </td>
                  </tr>
              
                  <tr><td>Publisher</td><td><a href="http://www.acm.org/publications">ACM</a> New York, NY, USA</td>
				  </tr>
             <tr><td>ISBN</td><td> 978-1-4503-1063-5</td></tr> <tr><td>Order Number</td><td>609124</td></tr> 
			<tr valign="top">
        	<td>Conference</td>
            <td valign="top" align="left"  style="padding-bottom: 25px;">
	            <strong style="padding-right:10px">HRI</strong><a href="event.cfm?id=RE285&CFID=105753441&CFTOKEN=50690582" title="ACM/IEEE International Conference on Human-Robot Interaction">ACM/IEEE International Conference on Human-Robot Interaction</a>
                
                       
                        <a href="event.cfm?id=RE285&CFID=105753441&CFTOKEN=50690582" title="ACM/IEEE International Conference on Human-Robot Interaction"><img border="0" src="http://portalparts.acm.org/event_logos/677/677.jpg" title="HRI logo" height="62"  width="100" ALT="HRI logo" style="vertical-align:top"></a>
						 

        	 </td>
            </tr>
		    <tr><td colspan="2">Paper Acceptance Rate 34 of 137 submissions, 25%</td></tr> <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 227 of 905 submissions, 25%</td></tr>
                       <tr valign="top">
                        <td colspan="2" style="padding-left:25px;">
                        	<table>
                            	<tr><td>
                                        <!-- WebCharts3D v5.1(2077) -->
<IMG SRC="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=Images/2290780240525730.JPG" id="Images_2290780240525730_JPG" name="Images_2290780240525730_JPG" usemap="#Images_2290780240525730_JPG_map" border="0"/>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAB' id='GP1338242675988AAAB'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>140</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAC' id='GP1338242675988AAAC'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>41</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAD' id='GP1338242675988AAAD'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>101</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAE' id='GP1338242675988AAAE'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>22</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAF' id='GP1338242675988AAAF'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>134</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAG' id='GP1338242675988AAAG'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>48</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAH' id='GP1338242675988AAAH'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>120</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAI' id='GP1338242675988AAAI'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>23</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAJ' id='GP1338242675988AAAJ'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>124</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAK' id='GP1338242675988AAAK'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>26</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAL' id='GP1338242675988AAAL'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>149</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAM' id='GP1338242675988AAAM'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>33</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAN' id='GP1338242675988AAAN'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>137</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242675988AAAO' id='GP1338242675988AAAO'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>34</td></tr></table>
<MAP name='Images_2290780240525730_JPG_map'>
<AREA shape='rect' coords='0,0,1,1'/>
<AREA shape="rect" coords="293,179,309,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAO",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAO",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAO",event)'/>
<AREA shape="rect" coords="277,74,293,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAN",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAN",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAN",event)'/>
<AREA shape="rect" coords="253,180,269,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAM",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAM",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAM",event)'/>
<AREA shape="rect" coords="237,62,253,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAL",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAL",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAL",event)'/>
<AREA shape="rect" coords="213,187,229,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAK",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAK",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAK",event)'/>
<AREA shape="rect" coords="197,87,213,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAJ",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAJ",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAJ",event)'/>
<AREA shape="rect" coords="173,190,189,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAI",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAI",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAI",event)'/>
<AREA shape="rect" coords="157,91,173,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAH",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAH",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAH",event)'/>
<AREA shape="rect" coords="133,165,149,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAG",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAG",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAG",event)'/>
<AREA shape="rect" coords="117,77,133,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAF",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAF",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAF",event)'/>
<AREA shape="rect" coords="93,191,109,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAE",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAE",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAE",event)'/>
<AREA shape="rect" coords="77,111,93,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAD",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAD",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAD",event)'/>
<AREA shape="rect" coords="53,172,69,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAC",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAC",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAC",event)'/>
<AREA shape="rect" coords="37,71,53,213" onMouseover='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAB",event,true)' onMouseout='xx_set_visible("Images_2290780240525730_JPG","GP1338242675988AAAB",event,false)' onMousemove='xx_move_tag("Images_2290780240525730_JPG","GP1338242675988AAAB",event)'/>
<AREA shape="rect" coords="160,13,227,27"/>
<AREA shape="rect" coords="89,13,160,27"/>
</MAP>

<script language="javascript" src="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=script.js"></script>

                                      </td>
                                      <td style="padding-left:20px;">
                                             <table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
                                                <tr bgcolor="#ffffff">
                                                  <th style="width:50%">Year</th>
                                                  <th  align="right" style="width:15%">Submitted</th>
                                                  <th  align="right" style="width:15%">Accepted</th>
                                                  <th  align="center">Rate</th>
                                                </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '06</td>
                                                            <td align="right">140</td>
                                                            <td align="right">41</td>
                                                            <td align="center">29%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '07</td>
                                                            <td align="right">101</td>
                                                            <td align="right">22</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '08</td>
                                                            <td align="right">134</td>
                                                            <td align="right">48</td>
                                                            <td align="center">36%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '09</td>
                                                            <td align="right">120</td>
                                                            <td align="right">23</td>
                                                            <td align="center">19%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '10</td>
                                                            <td align="right">124</td>
                                                            <td align="right">26</td>
                                                            <td align="center">21%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '11</td>
                                                            <td align="right">149</td>
                                                            <td align="right">33</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '12</td>
                                                            <td align="right">137</td>
                                                            <td align="right">34</td>
                                                            <td align="center">25%</td>
                                                         </tr>
                                                
                                                 <tr bgcolor="#ffffff">
                                                    <td><strong>Overall</strong></td>
                                                    <td align="right">905</td>
                                                    <td align="right">227</td>
                                                    <td align="center">25%</td>
                                                  </tr>
                                                </table>
                                       </td>
                                     </tr>
                               </table>
                        </td>
                    </tr>
                     
                     
            
</table>


</table>




</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	<br />Reviews are not available for this item
        
        <div align="left" style="margin-top:30px">
					<a title="Computing Reviews" href="ocr_review_main.cfm?CFID=105753441&CFTOKEN=50690582">
                 <img src="http://dl.acm.org/images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
        
        
       		<ul style="list-style:disc; display:inline-block">
	            <li>Access <a href="ocr_review_main.cfm?CFID=105753441&CFTOKEN=50690582" target="_blank">critical reviews</a> of computing literature.</li>
            	<li><a href="http://www.computingreviews.com/Reviewer/"  target="_blank">Become a reviewer</a> for Computing Reviews</li>
            </ul>
        </div>
        
</div>



<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
         
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<div>
<div>
	
	<p style="margin-left:5px;">
    <strong>Be the first to comment</strong>
    	
          	To Post a comment please <a href="signin.cfm?CFID=105753441&CFTOKEN=50690582">sign in or create</a> a free Web account</a>
        
    
    
	 </p>
	   	
 
</div>


</div>

	
		<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
        
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" >

<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction</h5>


<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>

<div style="clear:both">
    
        <div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1957656&picked=prox&CFID=105753441&CFTOKEN=50690582" title="previous: HRI '11"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><span class="link-text">no next proceeding</span></div>
        
</div>


 
<table class="text12" border="0">

  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Robot manipulation and programming</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Greg Trafton 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157691&CFID=105753441&CFTOKEN=50690582">Strategies for human-in-the-loop robotic grasping</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Adam Eric Leeper, Kaijen Hsiao, Matei Ciocarlie, Leila Takayama, David Gossow 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 1-8</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157691" title="DOI">10.1145/2157689.2157691</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157691&ftid=1161689&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157691&ftid=1161690&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow2" style="display:inline;"><br /><div style="display:inline">Human-in-the loop robotic systems have the potential to handle complex tasks in unstructured environments, by combining the cognitive skills of a human operator with autonomous tools and behaviors. Along these lines, we present a system for remote human-in-the-loop ...</div></span>
          <span id="toHide2" style="display:none;"><br /><div style="display:inline"><p>Human-in-the loop robotic systems have the potential to handle complex tasks in unstructured environments, by combining the cognitive skills of a human operator with autonomous tools and behaviors. Along these lines, we present a system for remote human-in-the-loop grasp execution. An operator uses a computer interface to visualize a physical robot and its surroundings, and a point-and-click mouse interface to command the robot. We implemented and analyzed four different strategies for performing grasping tasks, ranging from direct, real-time operator control of the end-effector pose, to autonomous motion and grasp planning that is simply adjusted or confirmed by the operator. Our controlled experiment (N=48) results indicate that people were able to successfully grasp more objects and caused fewer unwanted collisions when using the strategies with more autonomous assistance. We used an untethered robot over wireless communications, making our strategies applicable for remote, human-in-the-loop robotic applications.</p></div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157692&CFID=105753441&CFTOKEN=50690582">Grip forces and load forces in handovers: implications for designing human-robot handover controllers</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Wesley P. Chan, Chris A.C. Parker, H.F. Machiel Van der Loos, Elizabeth A. Croft 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 9-16</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157692" title="DOI">10.1145/2157689.2157692</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157692&ftid=1161691&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow3" style="display:inline;"><br /><div style="display:inline">In this study, we investigate and characterize haptic interaction in human-to-human handovers and identify key features that facilitate safe and efficient object transfer. Eighteen participants worked in pairs and transferred weighted objects to each ...</div></span>
          <span id="toHide3" style="display:none;"><br /><div style="display:inline"><p>In this study, we investigate and characterize haptic interaction in human-to-human handovers and identify key features that facilitate safe and efficient object transfer. Eighteen participants worked in pairs and transferred weighted objects to each other while we measured their grip forces and load forces. Our data show that during object transfer, both the giver and receiver employ a similar strategy for controlling their grip forces in response to changes in load forces. In addition, an implicit social contract appears to exist in which the giver is responsible for ensuring object safety in the handover and the receiver is responsible for maintaining the efficiency of the handover. Compared with prior studies, our analysis of experimental data show that there are important differences between the strategies used by humans for both picking up/placing objects on table and that used for handing over objects, indicating the need for specific robot handover strategies as well. The results of this study will be used to develop a controller for enabling robots to perform object handovers with humans safely, efficiently, and intuitively.</p></div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157693&CFID=105753441&CFTOKEN=50690582">Designing robot learners that ask good questions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Maya Cakmak, Andrea L. Thomaz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 17-24</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157693" title="DOI">10.1145/2157689.2157693</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157693&ftid=1161692&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=2157693&ftid=1161693&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow4" style="display:inline;"><br /><div style="display:inline">Programming new skills on a robot should take minimal time and effort. One approach to achieve this goal is to allow the robot to ask questions. This idea, called Active Learning, has recently caught a lot of attention in the robotics community. However, ...</div></span>
          <span id="toHide4" style="display:none;"><br /><div style="display:inline"><p>Programming new skills on a robot should take minimal time and effort. One approach to achieve this goal is to allow the robot to ask questions. This idea, called Active Learning, has recently caught a lot of attention in the robotics community. However, it has not been explored from a human-robot interaction perspective. In this paper, we identify three types of questions (label, demonstration and feature queries) and discuss how a robot can use these while learning new skills. Then, we present an experiment on human question asking which characterizes the extent to which humans use these question types. Finally, we evaluate the three question types within a human-robot teaching interaction. We investigate the ease with which different types of questions are answered and whether or not there is a general preference of one type of question over another. Based on our findings from both experiments we provide guidelines for designing question asking behaviors on a robot learner.</p></div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157694&CFID=105753441&CFTOKEN=50690582">Robot behavior toolkit: generating effective social behaviors for robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Chien-Ming Huang, Bilge Mutlu 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 25-32</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157694" title="DOI">10.1145/2157689.2157694</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157694&ftid=1161694&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow5" style="display:inline;"><br /><div style="display:inline">Social interaction involves a large number of patterned behaviors that people employ to achieve particular communicative goals. To achieve fluent and effective humanlike communication, robots must seamlessly integrate the necessary social behaviors for ...</div></span>
          <span id="toHide5" style="display:none;"><br /><div style="display:inline"><p>Social interaction involves a large number of patterned behaviors that people employ to achieve particular communicative goals. To achieve fluent and effective humanlike communication, robots must seamlessly integrate the necessary social behaviors for a given interaction context. However, very little is known about how robots might be equipped with a collection of such behaviors and how they might employ these behaviors in social interaction. In this paper, we propose a framework that guides the generation of social behavior for humanlike robots by systematically using specifications of social behavior from the social sciences and contextualizing these specifications in an Activity-Theory-based interaction model. We present the <i>Robot Behavior Toolkit</i>, an open-source implementation of this framework as a Robot Operating System (ROS) module and a community-based repository for behavioral specifications, and an evaluation of the effectiveness of the Toolkit in using these specifications to generate social behavior in a human-robot interaction study, focusing particularly on gaze behavior. The results show that specifications from this knowledge base enabled the Toolkit to achieve positive social, cognitive, and task outcomes, such as improved information recall, collaborative work, and perceptions of the robot.</p></div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Attitudes and responses to social robots</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Leila Takayama 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157696&CFID=105753441&CFTOKEN=50690582">Do people hold a humanoid robot morally accountable for the harm it causes?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81492649737&CFID=105753441&CFTOKEN=50690582">Peter H. Kahn, Jr.</a>, 
                        <a href="author_page.cfm?id=81500655380&CFID=105753441&CFTOKEN=50690582">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81500656859&CFID=105753441&CFTOKEN=50690582">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100631042&CFID=105753441&CFTOKEN=50690582">Brian T. Gill</a>, 
                        <a href="author_page.cfm?id=81350569693&CFID=105753441&CFTOKEN=50690582">Jolina H. Ruckert</a>, 
                        <a href="author_page.cfm?id=81456605597&CFID=105753441&CFTOKEN=50690582">Solace Shen</a>, 
                        <a href="author_page.cfm?id=81456635299&CFID=105753441&CFTOKEN=50690582">Heather E. Gary</a>, 
                        <a href="author_page.cfm?id=81456638149&CFID=105753441&CFTOKEN=50690582">Aimee L. Reichert</a>, 
                        <a href="author_page.cfm?id=81100563869&CFID=105753441&CFTOKEN=50690582">Nathan G. Freier</a>, 
                        <a href="author_page.cfm?id=81350575852&CFID=105753441&CFTOKEN=50690582">Rachel L. Severson</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 33-40</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157696" title="DOI">10.1145/2157689.2157696</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157696&ftid=1161695&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow7" style="display:inline;"><br /><div style="display:inline">Robots will increasingly take on roles in our social lives where they can cause humans harm. When robots do so, will people hold robots morally accountable? To investigate this question, 40 undergraduate students individually engaged in a 15-minute interaction ...</div></span>
          <span id="toHide7" style="display:none;"><br /><div style="display:inline"><p>Robots will increasingly take on roles in our social lives where they can cause humans harm. When robots do so, will people hold robots morally accountable? To investigate this question, 40 undergraduate students individually engaged in a 15-minute interaction with ATR's humanoid robot, Robovie. The interaction culminated in a situation where Robovie incorrectly assessed the participant's performance in a game, and prevented the participant from winning a $20 prize. Each participant was then interviewed in a 50-minute session. Results showed that all of the participants engaged socially with Robovie, and many of them conceptualized Robovie as having mental/emotional and social attributes. Sixty-five percent of the participants attributed some level of moral accountability to Robovie. Statistically, participants held Robovie less accountable than they would a human, but more accountable than they would a vending machine. Results are discussed in terms of the <i>New Ontological Category Hypothesis</i> and robotic warfare.</p></div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157697&CFID=105753441&CFTOKEN=50690582">Social facilitation with social robots?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Nina Riether, Frank Hegel, Britta Wrede, Gernot Horstmann 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 41-48</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157697" title="DOI">10.1145/2157689.2157697</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157697&ftid=1161696&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow8" style="display:inline;"><br /><div style="display:inline">Regarding the future usage of social robots in workplace scenarios, we addressed the question of potential mere robotic presence effects on human performance. Applying the experimental social facilitation paradigm in social robotics, we compared task ...</div></span>
          <span id="toHide8" style="display:none;"><br /><div style="display:inline"><p>Regarding the future usage of social robots in workplace scenarios, we addressed the question of potential mere robotic presence effects on human performance. Applying the experimental social facilitation paradigm in social robotics, we compared task performance of 106 participants on easy and complex cognitive and motoric tasks across three presence groups (alone vs. human present vs. robot present). Results revealed significant evidence for the predicted social facilitation effects for both human and robotic presence compared to an alone condition. Implications of these findings are discussed with regard to the consideration of the interaction of robotic presence and task difficulty in modeling robotic assistance systems.</p></div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157698&CFID=105753441&CFTOKEN=50690582">New measurement of psychological safety for humanoid</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500665153&CFID=105753441&CFTOKEN=50690582">Hiroko Kamide</a>, 
                        <a href="author_page.cfm?id=81500655875&CFID=105753441&CFTOKEN=50690582">Yasushi Mae</a>, 
                        <a href="author_page.cfm?id=81500650186&CFID=105753441&CFTOKEN=50690582">Koji Kawabe</a>, 
                        <a href="author_page.cfm?id=81500662902&CFID=105753441&CFTOKEN=50690582">Satoshi Shigemi</a>, 
                        <a href="author_page.cfm?id=81500654328&CFID=105753441&CFTOKEN=50690582">Masato Hirose</a>, 
                        <a href="author_page.cfm?id=81440608922&CFID=105753441&CFTOKEN=50690582">Tatsuo Arai</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 49-56</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157698" title="DOI">10.1145/2157689.2157698</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157698&ftid=1161697&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow9" style="display:inline;"><br /><div style="display:inline">In this article, we aim to discover the important factors for determining the psychological safety of humanoids and to develop a new psychological scale to measure the degree of safety quantitatively. To discover the factors that determine the psychological ...</div></span>
          <span id="toHide9" style="display:none;"><br /><div style="display:inline"><p>In this article, we aim to discover the important factors for determining the psychological safety of humanoids and to develop a new psychological scale to measure the degree of safety quantitatively. To discover the factors that determine the psychological safety of humanoids from an ordinary person's perspective, we studied 919 Japanese, who observed movies of 11 humanoids and then freely described their impressions about what the safety of each humanoid was for them. Five psychologists categorized all of the obtained descriptions into several categories and then used the categories to compose a new psychological scale. Then, 2,624 different Japanese evaluated the same 11 humanoids using the new scale. Factor analysis on the obtained quantitative data revealed six factors of psychological safety: Performance, Humanness, Acceptance, Harmlessness, Toughness, and Agency. Additional analysis revealed that Performance, Acceptance, Harmlessness, and Toughness were the most important factors for determining the psychological safety of general humanoids. The usability of the new scale is discussed.</p></div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157699&CFID=105753441&CFTOKEN=50690582">Consistency in physical and on-screen action improves perceptions of telepresence robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500644418&CFID=105753441&CFTOKEN=50690582">David Sirkin</a>, 
                        <a href="author_page.cfm?id=81500648447&CFID=105753441&CFTOKEN=50690582">Wendy Ju</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 57-64</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157699" title="DOI">10.1145/2157689.2157699</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157699&ftid=1161698&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=2157699&ftid=1161699&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow10" style="display:inline;"><br /><div style="display:inline">Does augmented movement capability improve people's experiences with telepresent meeting participants? We performed two web-based studies featuring videos of a telepresence robot. In the first study (N=164), participants observed clips of typical conversational ...</div></span>
          <span id="toHide10" style="display:none;"><br /><div style="display:inline"><p>Does augmented movement capability improve people's experiences with telepresent meeting participants? We performed two web-based studies featuring videos of a telepresence robot. In the first study (N=164), participants observed clips of typical conversational gestures performed a) on a stationary screen only, b) with an actuated screen moving in physical space, or c) both on-screen and in-space. In the second study (N=103), participants viewed scenario videos depicting two people interacting with a remote collaborator through a telepresence robot, whose distant actions were a) visible on the screen only, or b) accompanied by local physical motion. These studies suggest that synchronized on-screen and in-space gestures significantly improved viewers' interpretation of the action compared to on-screen or in-space gestures alone, and that in-space gestures positively influenced perceptions of both local and remote participants.</p></div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Robot wizards: robot operation and interfaces</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Michita Imai 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157701&CFID=105753441&CFTOKEN=50690582">Real world haptic exploration for telepresence of the visually impaired</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453649331&CFID=105753441&CFTOKEN=50690582">Chung Hyuk Park</a>, 
                        <a href="author_page.cfm?id=81100328868&CFID=105753441&CFTOKEN=50690582">Ayanna M. Howard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 65-72</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157701" title="DOI">10.1145/2157689.2157701</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157701&ftid=1161700&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow12" style="display:inline;"><br /><div style="display:inline">Robotic assistance through telepresence technology is an emerging area in aiding the visually impaired. By integrating the robotic perception of a remote environment and transferring it to a human user through haptic environmental feedback, the disabled ...</div></span>
          <span id="toHide12" style="display:none;"><br /><div style="display:inline"><p>Robotic assistance through telepresence technology is an emerging area in aiding the visually impaired. By integrating the robotic perception of a remote environment and transferring it to a human user through haptic environmental feedback, the disabled user can increase one's capability to interact with remote environments through the telepresence robot. This paper presents a framework that integrates visual perception from heterogeneous vision sensors and enables real-time interactive haptic represent-ation of the real world through a mobile manipulation robotic system. Specifically, a set of multi-disciplinary algorithms such as stereo-vision processes, three-dimensional map building algorithms, and virtual-proxy haptic rendering processes are integrated into a unified framework to accomplish the goal of real-world haptic exploration successfully. Results of our framework in an indoor environment are displayed, and its performances are analyzed. Quantitative results are provided along with qualitative results through a set of human subject testing. Our future work includes real-time haptic fusion of multi-modal environmental perception and more extensive human subject testing in a prolonged experimental design.</p></div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157702&CFID=105753441&CFTOKEN=50690582">Effects of changing reliability on trust of robot systems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81443595444&CFID=105753441&CFTOKEN=50690582">Munjal Desai</a>, 
                        <a href="author_page.cfm?id=81500662012&CFID=105753441&CFTOKEN=50690582">Mikhail Medvedev</a>, 
                        <a href="author_page.cfm?id=81482640887&CFID=105753441&CFTOKEN=50690582">Marynel V&#225;zquez</a>, 
                        <a href="author_page.cfm?id=81500663344&CFID=105753441&CFTOKEN=50690582">Sean McSheehy</a>, 
                        <a href="author_page.cfm?id=81500664686&CFID=105753441&CFTOKEN=50690582">Sofia Gadea-Omelchenko</a>, 
                        <a href="author_page.cfm?id=81500661471&CFID=105753441&CFTOKEN=50690582">Christian Bruggeman</a>, 
                        <a href="author_page.cfm?id=81500653412&CFID=105753441&CFTOKEN=50690582">Aaron Steinfeld</a>, 
                        <a href="author_page.cfm?id=81500643756&CFID=105753441&CFTOKEN=50690582">Holly Yanco</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 73-80</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157702" title="DOI">10.1145/2157689.2157702</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157702&ftid=1161701&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow13" style="display:inline;"><br /><div style="display:inline">Prior work in human-autonomy interaction has focused on plant systems that operate in highly structured environments. In contrast, many human-robot interaction (HRI) tasks are dynamic and unstructured, occurring in the open world. It is our belief that ...</div></span>
          <span id="toHide13" style="display:none;"><br /><div style="display:inline"><p>Prior work in human-autonomy interaction has focused on plant systems that operate in highly structured environments. In contrast, many human-robot interaction (HRI) tasks are dynamic and unstructured, occurring in the open world. It is our belief that methods developed for the measurement and modeling of trust in traditional automation need alteration in order to be useful for HRI. Therefore, it is important to characterize the factors in HRI that influence trust. This study focused on the influence of changing autonomy reliability. Participants experienced a set of challenging robot handling scenarios that forced autonomy use and kept them focused on autonomy performance. The counterbalanced experiment included scenarios with different low reliability windows so that we could examine how drops in reliability altered trust and use of autonomy. Drops in reliability were shown to affect trust, the frequency and timing of autonomy mode switching, as well as participants' self-assessments of performance. A regression analysis on a number of robot, personal, and scenario factors revealed that participants tie trust more strongly to their own actions rather than robot performance.</p></div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157703&CFID=105753441&CFTOKEN=50690582">Teamwork in controlling multiple robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Fei Gao, Missy L. Cummings, Luca F. Bertuccelli 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 81-88</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157703" title="DOI">10.1145/2157689.2157703</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157703&ftid=1161702&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow14" style="display:inline;"><br /><div style="display:inline">Simultaneously controlling increasing numbers of robots requires multiple operators working together as a team. Helping operators allocate attention among different robots and determining how to construct the human-robot team to promote performance and ...</div></span>
          <span id="toHide14" style="display:none;"><br /><div style="display:inline"><p>Simultaneously controlling increasing numbers of robots requires multiple operators working together as a team. Helping operators allocate attention among different robots and determining how to construct the human-robot team to promote performance and reduce workload are critical questions that must be answered in these settings. To this end, we investigated the effect of team structure and search guidance on operators' performance, subjective workload, work processes and communication. To investigate team structure in an urban search and rescue setting, we compared a pooled condition, in which team members shared control of 24 robots, with a sector condition, in which each team member control half of all the robots. For search guidance, a notification was given when the operator spent too much time on one robot and either suggested or forced the operator to change to another robot. A total of 48 participants completed the experiment with two persons forming one team. The results demonstrate that automated search guidance neither increased nor decreased performance. However, suggested search guidance decreased average task completion time in Sector teams. Search guidance also influenced operators' teleoperation behaviors. For team structure, pooled teams experienced lower subjective workload than sector teams. Pooled teams communicated more than sector teams, but sector teams teleoperated more than pool teams.</p></div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157704&CFID=105753441&CFTOKEN=50690582">Towards human control of robot swarms</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Andreas Kolling, Steven Nunnally, Michael Lewis 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 89-96</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157704" title="DOI">10.1145/2157689.2157704</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157704&ftid=1161703&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow15" style="display:inline;"><br /><div style="display:inline">In this paper we investigate principles of swarm control that enable a human operator to exert influence on and control large swarms of robots. We present two principles, coined selection and beacon control, that differ with respect to ...</div></span>
          <span id="toHide15" style="display:none;"><br /><div style="display:inline"><p>In this paper we investigate principles of swarm control that enable a human operator to exert influence on and control large swarms of robots. We present two principles, coined <i>selection</i> and <i>beacon</i> control, that differ with respect to their temporal and spatial persistence. The former requires active selection of groups of robots while the latter exerts a passive influence on nearby robots. Both principles are implemented in a testbed in which operators exert influence on a robot swarm by switching between a set of behaviors ranging from trivial behaviors up to distributed autonomous algorithms. Performance is tested in a series of complex foraging tasks in environments with different obstacles ranging from open to cluttered and structured. The robotic swarm has only local communication and sensing capabilities with the number of robots ranging from 50 to 200. Experiments with human operators utilizing either <i>selection</i> or <i>beacon</i> control are compared with each other and to a simple autonomous swarm with regard to performance, adaptation to complex environments, and scalability to larger swarms. Our results show superior performance of autonomous swarms in open environments, of selection control in complex environments, and indicate a potential for scaling beacon control to larger swarms.</p></div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157705&CFID=105753441&CFTOKEN=50690582">Designing interfaces for multi-user, multi-robot systems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Adam Rule, Jodi Forlizzi 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 97-104</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157705" title="DOI">10.1145/2157689.2157705</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157705&ftid=1161704&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow16" style="display:inline;"><br /><div style="display:inline">The use of autonomous robots in organizations is expected to increase steadily over the next few decades. Although some empirical work exists that examines how people collaborate with robots, little is known about how to best design interfaces to support ...</div></span>
          <span id="toHide16" style="display:none;"><br /><div style="display:inline"><p>The use of autonomous robots in organizations is expected to increase steadily over the next few decades. Although some empirical work exists that examines how people collaborate with robots, little is known about how to best design interfaces to support operators in understanding aspects of the task or tasks at hand. This paper presents a design investigation to understand how interfaces should be designed to support multi-user, multi-robot teams. Through contextual inquiry, concept generation, and concept evaluation, we determine what operators should see, and with what salience different types of information should be presented. We present our findings through a series of design questions that development teams can use to help define interaction and design interfaces for these systems.</p></div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>LBR highlights</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Astrid Weis 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157707&CFID=105753441&CFTOKEN=50690582">A touchscreen-based 'sandtray' to facilitate, mediate and contextualise human-robot social interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500658840&CFID=105753441&CFTOKEN=50690582">Paul Baxter</a>, 
                        <a href="author_page.cfm?id=81500640571&CFID=105753441&CFTOKEN=50690582">Rachel Wood</a>, 
                        <a href="author_page.cfm?id=81100462469&CFID=105753441&CFTOKEN=50690582">Tony Belpaeme</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 105-106</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157707" title="DOI">10.1145/2157689.2157707</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157707&ftid=1161705&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow18" style="display:inline;"><br /><div style="display:inline">In the development of companion robots capable of any-depth, long-term interaction, social scenarios enable exploration of the robot's capacity to engage a human interactant. These scenarios are typically constrained to structured task-based interactions, ...</div></span>
          <span id="toHide18" style="display:none;"><br /><div style="display:inline"><p>In the development of companion robots capable of any-depth, long-term interaction, social scenarios enable exploration of the robot's capacity to engage a human interactant. These scenarios are typically constrained to structured task-based interactions, to enable the quantification of results for the comparison of differing experimental conditions. This paper introduces a hardware setup to facilitate and mediate human-robot social interaction, simplifying the robot control task while enabling an equalised degree of environmental manipulation for the human and robot, but without implicitly imposing an <i>a priori</i> interaction structure.</p></div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157708&CFID=105753441&CFTOKEN=50690582">Children's knowledge and expectations about robots: a survey for future user-centered design of social robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Eduardo Ben&#237;tez Sandoval, Christian Penaloza 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 107-108</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157708" title="DOI">10.1145/2157689.2157708</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157708&ftid=1161706&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow19" style="display:inline;"><br /><div style="display:inline">This paper seeks to establish a precedent for future development and design of social robots by considering the knowledge and expectations about robots of a group of 296 children. Human-robot interaction experiments were conducted with a Tele-operated ...</div></span>
          <span id="toHide19" style="display:none;"><br /><div style="display:inline"><p>This paper seeks to establish a precedent for future development and design of social robots by considering the knowledge and expectations about robots of a group of 296 children. Human-robot interaction experiments were conducted with a Tele-operated anthropomorphic robot, and surveys were taken before and after the experiments. Children were also asked to perform a drawing of a robot. An image analysis algorithm was developed to classify drawings into 4 types: Anthropomorphic Mechanic/Non Mechanic (AM/AnM) and Non-Anthropomorphic Mechanic/Non Mechanic (nAM/nAnM). Image analysis algorithm was used in combination with human classification using a 2oo3 (two out of three) voting scheme to find children's strongest stereotype about robots. Survey and image analysis results suggest that children in general have some general knowledge about robots, and some children even have a deep understanding and expectations for future robots. Moreover, children's strongest stereotype is directed towards mechanical anthropomorphic systems.</p></div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157709&CFID=105753441&CFTOKEN=50690582">Human-robot interaction: developing trust in robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Deborah R. Billings, Kristin E. Schaefer, Jessie Y.C. Chen, Peter A. Hancock 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 109-110</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157709" title="DOI">10.1145/2157689.2157709</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157709&ftid=1161707&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow20" style="display:inline;"><br /><div style="display:inline">In all human-robot interaction, trust is an important element to consider because the presence or absence of trust certainly impacts the ultimate outcome of that interaction. Limited research exists that delineates the development and maintenance of ...</div></span>
          <span id="toHide20" style="display:none;"><br /><div style="display:inline"><p>In all human-robot interaction, trust is an important element to consider because the presence or absence of trust certainly impacts the ultimate outcome of that interaction. Limited research exists that delineates the development and maintenance of this trust in various operational contexts. Our own prior research has investigated theoretical and empirically supported antecedents of human-robot trust. Here, we describe progress to date relating to the development of a comprehensive human-robot trust model based on our ongoing program of research.</p></div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157710&CFID=105753441&CFTOKEN=50690582">Dynamic gesture vocabulary design for intuitive human-robot dialog</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81488669436&CFID=105753441&CFTOKEN=50690582">Sa&#353;a Bodiro&#382;a</a>, 
                        <a href="author_page.cfm?id=81100309417&CFID=105753441&CFTOKEN=50690582">Helman I. Stern</a>, 
                        <a href="author_page.cfm?id=81100347892&CFID=105753441&CFTOKEN=50690582">Yael Edan</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 111-112</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157710" title="DOI">10.1145/2157689.2157710</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157710&ftid=1161708&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow21" style="display:inline;"><br /><div style="display:inline">This paper presents a generalized method for the design of a gesture vocabulary (GV) for intuitive and natural two-way human-robot dialog. Two GV design methodologies are proposed; one for a robot GV (RGV) and a second for a human GV (HGV). The design ...</div></span>
          <span id="toHide21" style="display:none;"><br /><div style="display:inline"><p>This paper presents a generalized method for the design of a gesture vocabulary (GV) for intuitive and natural two-way human-robot dialog. Two GV design methodologies are proposed; one for a robot GV (RGV) and a second for a human GV (HGV). The design is based on motion gestures exerted from a cohort of subjects in response to a set of tasks needed to execute several robot waiter (RW)-customer dialogs. Using a RW setting as a case study, preliminary experimental results indicate the unique nature of the HGV obtained.</p></div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157711&CFID=105753441&CFTOKEN=50690582">Design of a haptic joystick for shared robot control</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500642953&CFID=105753441&CFTOKEN=50690582">Daniel J. Brooks</a>, 
                        <a href="author_page.cfm?id=81500643757&CFID=105753441&CFTOKEN=50690582">Holly A. Yanco</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 113-114</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157711" title="DOI">10.1145/2157689.2157711</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157711&ftid=1161709&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157712&CFID=105753441&CFTOKEN=50690582">User experience of industrial robots over time</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414616801&CFID=105753441&CFTOKEN=50690582">Roland Buchner</a>, 
                        <a href="author_page.cfm?id=81414618872&CFID=105753441&CFTOKEN=50690582">Daniela Wurhofer</a>, 
                        <a href="author_page.cfm?id=81500660129&CFID=105753441&CFTOKEN=50690582">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81500647816&CFID=105753441&CFTOKEN=50690582">Manfred Tscheligi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 115-116</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157712" title="DOI">10.1145/2157689.2157712</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157712&ftid=1161710&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow23" style="display:inline;"><br /><div style="display:inline">This paper reports about a User Experience (UX) study on industrial robotic arms in the context of a semiconductor factory cleanroom. The goal was to find out (1) if there is a difference in the UX between robots used over years with a strict security ...</div></span>
          <span id="toHide23" style="display:none;"><br /><div style="display:inline"><p>This paper reports about a User Experience (UX) study on industrial robotic arms in the context of a semiconductor factory cleanroom. The goal was to find out (1) if there is a difference in the UX between robots used over years with a strict security perimeter (robot A) and a newly installed robot without security perimeter (robot B), and (2) if the UX ratings of the new robot change over time. Therefore, a UX questionnaire was developed and handed out to the operators working with these robots. The first survey was conducted one week after the deployment of robot B (n=23), the second survey (n=21) six months later. Thereby, we found that time is crucial for experiencing human-robot interaction. Our results showed an improvement between the first and second measurement of UX regarding robot B. Although robot A was significantly better rated than robot B in terms of usability, general UX, cooperation, and stress, we assume that the differences in UX will decrease gradually with prolonged interaction.</p></div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157713&CFID=105753441&CFTOKEN=50690582">Visual cues-based anticipation for percussionist-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Marcelo Cicconet, Mason Bretan, Gil Weinberg 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 117-118</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157713" title="DOI">10.1145/2157689.2157713</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157713&ftid=1161711&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow24" style="display:inline;"><br /><div style="display:inline">Visual cues-based anticipation is a fundamental aspect of human-human interaction, and it plays an especially important role in the time demanding medium of group performance. In this work we explore the importance of visual gesture anticipation in music ...</div></span>
          <span id="toHide24" style="display:none;"><br /><div style="display:inline"><p>Visual cues-based anticipation is a fundamental aspect of human-human interaction, and it plays an especially important role in the time demanding medium of group performance. In this work we explore the importance of visual gesture anticipation in music performance involving human and robot. We study the case in which a human percussionist is playing a four-piece percussion set, and a robot musician is playing either the marimba, or a three-piece percussion set. Computer Vision is used to embed anticipation in the robotic response to the human gestures. We developed two algorithms for anticipation, predicting the strike location about 10 mili-seconds or about 100 mili-seconds before it occurs. Using the second algorithm, we show that the robot outperforms, on average, a group of human subjects, in synchronizing its gesture with a reference strike. We also show that, in the tested group of users, having some time in advance is important for a human to synchronize the strike with a reference player, but, from a certain time, that good influence stops increasing.</p></div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157714&CFID=105753441&CFTOKEN=50690582">Socially constrained management of power resources for social mobile robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81375606595&CFID=105753441&CFTOKEN=50690582">Amol Deshmukh</a>, 
                        <a href="author_page.cfm?id=81100603687&CFID=105753441&CFTOKEN=50690582">Ruth Aylett</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 119-120</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157714" title="DOI">10.1145/2157689.2157714</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157714&ftid=1161712&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow25" style="display:inline;"><br /><div style="display:inline">Autonomous robots acting as companions or assistants in real social environments should be able to sustain and operate over an extended period of time. Generally, autonomous mobile robots draw power from batteries to operate various sensors, actuators ...</div></span>
          <span id="toHide25" style="display:none;"><br /><div style="display:inline"><p>Autonomous robots acting as companions or assistants in real social environments should be able to sustain and operate over an extended period of time. Generally, autonomous mobile robots draw power from batteries to operate various sensors, actuators and perform tasks. Batteries have a limited power life and take a long time to recharge via a power source, which may impede human-robot interaction and task performance. Thus, it is important for social robots to manage their energy, this paper discusses an approach to manage power resources on mobile robot with regard to social aspects for creating life-like autonomous social robots.</p></div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157715&CFID=105753441&CFTOKEN=50690582">Sensorless collision detection and control by physical interaction for wheeled mobile robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Guillaume Doisy 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 121-122</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157715" title="DOI">10.1145/2157689.2157715</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157715&ftid=1161713&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow26" style="display:inline;"><br /><div style="display:inline">In this paper, we present the adaptation of a sensorless (in De Luca's sense [1], i.e., without the use of extra sensors,) collision detection approach previously used on robotic arms to mobile wheeled robots. The method is based on detecting the torque ...</div></span>
          <span id="toHide26" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present the adaptation of a sensorless (in De Luca's sense [1], i.e., without the use of extra sensors,) collision detection approach previously used on robotic arms to mobile wheeled robots. The method is based on detecting the torque disturbance and does not require a model of the robot's dynamics. We then consider the feasibility of developing control by physical interaction strategies using the described adapted technique.</p></div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157716&CFID=105753441&CFTOKEN=50690582">Assistive teleoperation for manipulation tasks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500665324&CFID=105753441&CFTOKEN=50690582">Anca D. Dragan</a>, 
                        <a href="author_page.cfm?id=81453611472&CFID=105753441&CFTOKEN=50690582">Siddhartha S. Srinivasa</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 123-124</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157716" title="DOI">10.1145/2157689.2157716</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157716&ftid=1161714&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157717&CFID=105753441&CFTOKEN=50690582">'If you sound like me, you must be more human': on the interplay of robot and user features on human-robot acceptance and anthropomorphism</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Friederike Eyssel, Dieta Kuchenbrandt, Simon Bobinger, Laura de Ruiter, Frank Hegel 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 125-126</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157717" title="DOI">10.1145/2157689.2157717</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157717&ftid=1161715&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow28" style="display:inline;"><br /><div style="display:inline">In an experiment we manipulated a robot's voice in two ways: First, we varied robot gender; second, we equipped the robot with a human-like or a robot-like synthesized voice. Moreover, we took into account user gender and tested effects of these factors ...</div></span>
          <span id="toHide28" style="display:none;"><br /><div style="display:inline"><p>In an experiment we manipulated a robot's voice in two ways: First, we varied robot gender; second, we equipped the robot with a human-like or a robot-like synthesized voice. Moreover, we took into account user gender and tested effects of these factors on human-robot acceptance, psychological closeness and psychological anthropomorphism. When participants formed an impression of a same-gender robot, the robot was perceived more positively. Participants also felt more psychological closeness to the same-gender robot. Similarly, the same-gender robot was anthropomorphized more strongly, but only when it utilized a human-like voice. Results indicate that a projection mechanism could underlie these effects.</p></div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157718&CFID=105753441&CFTOKEN=50690582">Beyond "spatial ability": examining the impact of multiple individual differences in a perception by proxy framework</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Thomas Fincannon, Florian Jentsch, Brittany Sellers, Joseph R. Keebler 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 127-128</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157718" title="DOI">10.1145/2157689.2157718</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157718&ftid=1161716&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow29" style="display:inline;"><br /><div style="display:inline">Prior research has proposed the use of a Perception by Proxy framework that relies on human perception to support actions of autonomy. Given the importance of human perception, this framework highlights the need to understand how human cognitive ...</div></span>
          <span id="toHide29" style="display:none;"><br /><div style="display:inline"><p>Prior research has proposed the use of a <i>Perception by Proxy</i> framework that relies on human perception to support actions of autonomy. Given the importance of human perception, this framework highlights the need to understand how human cognitive abilities factor into the human-robot dynamic. The following paper uses a military reconnaissance task to examine how cognitive abilities interact with the gradual implementation of autonomy in a <i>Perception by Proxy</i> framework (i.e., autonomy to detect; autonomy to support rerouting) to predict three dimensions of sequential performance (i.e., speeded detection; target identification; rerouting). Results showed that, in addition to effects of autonomy and task setting, different individual abilities predicted unique aspects of performance. This highlights the need to broaden consideration of cognitive abilities in HRI.</p></div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157719&CFID=105753441&CFTOKEN=50690582">Attentional human-robot interaction in simple manipulation tasks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Ernesto Burattini, Alberto Finzi, Silvia Rossi, Mariacarla Staffa 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 129-130</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157719" title="DOI">10.1145/2157689.2157719</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157719&ftid=1161717&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow30" style="display:inline;"><br /><div style="display:inline">We present a robotic control system endowed with attentional mechanisms suitable for balancing the trade off between safe human-robot interaction and effective task execution. These mechanisms allow the robot to increase or decrease the degree of attention ...</div></span>
          <span id="toHide30" style="display:none;"><br /><div style="display:inline"><p>We present a robotic control system endowed with attentional mechanisms suitable for balancing the trade off between safe human-robot interaction and effective task execution. These mechanisms allow the robot to increase or decrease the degree of attention toward relevant activities modulating the frequency of the monitoring rate and the speed associated to the robot movements. In this framework, we consider pick-and-place and give-and-receive attentional behaviors.</p></div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157720&CFID=105753441&CFTOKEN=50690582">'Midas touch' in human-robot interaction: evidence from event-related potentials during the ultimatum game</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Haruaki Fukuda, Masahiro Shiomi, Kayako Nakagawa, Kazuhiro Ueda 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 131-132</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157720" title="DOI">10.1145/2157689.2157720</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157720&ftid=1161718&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow31" style="display:inline;"><br /><div style="display:inline">Interpersonal touch is said to have significant effects on social interaction. We used the ultimatum game to examine whether touch from a robot could inhibit a negative feeling to the robot. We set two experimental conditions: the one was "touch condition" ...</div></span>
          <span id="toHide31" style="display:none;"><br /><div style="display:inline"><p>Interpersonal touch is said to have significant effects on social interaction. We used the ultimatum game to examine whether touch from a robot could inhibit a negative feeling to the robot. We set two experimental conditions: the one was "touch condition" in which unfair proposals were offered to a participant when a robot touched his/her arm and the other was "no touch condition" in which unfair proposals were offered when the same robot did not. We compared Medial Frontal Negativity (MFN) measured by EEG, whose amplitude is correlated with feeling of unfairness, between the two conditions. Result shows that MFN amplitude was larger in the no touch condition than in the touch condition. This indicates that touch from a robot may inhibit a sense of unfairness for the robot. Our finding suggests that touch from a robot could enhance positive feeling to the robot through human-robot interaction.</p></div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157721&CFID=105753441&CFTOKEN=50690582">Facial gesture recognition using active appearance models based on neural evolution</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jorge Garc&#237;ia Bueno, Miguel Gonz&#225;lez-Fierro, Luis Moreno, Carlos Balaguer 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 133-134</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157721" title="DOI">10.1145/2157689.2157721</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157721&ftid=1161719&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMpeg" title="Other Formats Mpeg" href="ft_gateway.cfm?id=2157721&ftid=1161720&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mpeg.gif" alt="Mpeg" class="fulltext_lnk" border="0" />Mpeg</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow32" style="display:inline;"><br /><div style="display:inline">Facial gesture recognition is one of the main topics in HRI. We have developed a novel algorithm who allows to detect emotional states, like happiness, sadness or emotionless. A humanoid robot is able to detect these states with a ratio of success of ...</div></span>
          <span id="toHide32" style="display:none;"><br /><div style="display:inline"><p>Facial gesture recognition is one of the main topics in HRI. We have developed a novel algorithm who allows to detect emotional states, like happiness, sadness or emotionless. A humanoid robot is able to detect these states with a ratio of success of 83% and interact in consequence. We use Active Appearance Models (AAMs) to determinate face features and classify the emotions using neural evolution, based on neural networks and differential evolution algorithm.</p></div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157722&CFID=105753441&CFTOKEN=50690582">Design, integration, and test of a shopping assistance robot system</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Marlon Garcia-Arroyo, Luis Felipe Marin-Urias, Antonio Marin-Hernandez, Guillermo de Jesus Hoyos-Rivera 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 135-136</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157722" title="DOI">10.1145/2157689.2157722</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157722&ftid=1161721&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow33" style="display:inline;"><br /><div style="display:inline">In this paper is described the current work towards the design of a shopping assistant robot system. This system will allow users to keep control of what they are buying; the robot will help the customer by handling its shopping list, carrying with all ...</div></span>
          <span id="toHide33" style="display:none;"><br /><div style="display:inline"><p>In this paper is described the current work towards the design of a shopping assistant robot system. This system will allow users to keep control of what they are buying; the robot will help the customer by handling its shopping list, carrying with all the products, and serving as a companion. In this work is also shown the acceptability studies for this kind of robot.</p></div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157723&CFID=105753441&CFTOKEN=50690582">Handheld operator control unit</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Neal Checka, Shawn Schaffert, David Demirdjian, Jan Falkowski, Daniel H. Grollman 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 137-138</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157723" title="DOI">10.1145/2157689.2157723</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157723&ftid=1161722&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow34" style="display:inline;"><br /><div style="display:inline">Currently, unmanned vehicles support soldiers in a variety of military applications. Typically, a specially-trained user teleoperates these platforms using a large and bulky Operator Control Unit (OCU). The operator's total attention is required for ...</div></span>
          <span id="toHide34" style="display:none;"><br /><div style="display:inline"><p>Currently, unmanned vehicles support soldiers in a variety of military applications. Typically, a specially-trained user teleoperates these platforms using a large and bulky Operator Control Unit (OCU). The operator's total attention is required for controlling the tedious, low-level aspects of the platform, dramatically reducing his personal situational awareness. Furthermore, these OCUs are both platform and mission specific. Ideally, a soldier could instead carry light-weight and portable multi-purpose devices to act as OCUs for multiple platform/mission scenarios. These devices would support a standard set of OCU functionality (e.g., as driving a ground robot) and additional higher-level task operations (e.g., autonomously patrolling an area). This extended abstract presents the development of apps for a handheld platform that enable both low- and high-level control of an unmanned vehicle.</p></div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157724&CFID=105753441&CFTOKEN=50690582">Unveiling robotophobia and cyber-dystopianism: the role of gender, technology and religion on attitudes towards robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Daniel Halpern, James E. Katz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 139-140</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157724" title="DOI">10.1145/2157689.2157724</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157724&ftid=1161723&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow35" style="display:inline;"><br /><div style="display:inline">A survey of 873 undergraduate students was conducted to understand which individual factors affect subjects' attitudes toward robots. A third of participants (N=284) were exposed to a humanoid robot, another third (N=293) to a doggy robot, and the remaining ...</div></span>
          <span id="toHide35" style="display:none;"><br /><div style="display:inline"><p>A survey of 873 undergraduate students was conducted to understand which individual factors affect subjects' attitudes toward robots. A third of participants (N=284) were exposed to a humanoid robot, another third (N=293) to a doggy robot, and the remaining third (N=296) to an android. Results showed that in the humanoid condition individuals recognize more humanlike characteristics in robots than in the other two conditions. However humanoid appearance did not affect participants' attitudes toward robots, as others predictors recognized by previous research indicate, such as gender, religion, and perceived competence with information and communication technologies (ICT).</p></div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157725&CFID=105753441&CFTOKEN=50690582">Assessing workload in human-robot peer-based teams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500651561&CFID=105753441&CFTOKEN=50690582">Caroline E. Harriott</a>, 
                        <a href="author_page.cfm?id=81500665188&CFID=105753441&CFTOKEN=50690582">Glenna L. Buford</a>, 
                        <a href="author_page.cfm?id=81500642898&CFID=105753441&CFTOKEN=50690582">Tao Zhang</a>, 
                        <a href="author_page.cfm?id=81100313646&CFID=105753441&CFTOKEN=50690582">Julie A. Adams</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 141-142</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157725" title="DOI">10.1145/2157689.2157725</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157725&ftid=1161724&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow36" style="display:inline;"><br /><div style="display:inline">The effect of a robotic teammate on a human partner's workload has not been fully quantified. Prior research found that human participants experienced lower workload when working with a robotic partner than when working with a human partner. An evaluation ...</div></span>
          <span id="toHide36" style="display:none;"><br /><div style="display:inline"><p>The effect of a robotic teammate on a human partner's workload has not been fully quantified. Prior research found that human participants experienced lower workload when working with a robotic partner than when working with a human partner. An evaluation investigated whether a similar trend in workload exists for tasks requiring direct and collaborative interaction between the partners, and joint team decision-making. The subjective results indicate a similar trend to the prior results; participants rated workload lower for the more complex task when partnered with a robot than when partnered with a human.</p></div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157726&CFID=105753441&CFTOKEN=50690582">Does a robot that can learn verbs lead to better user perception?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81460641491&CFID=105753441&CFTOKEN=50690582">Dai Hasegawa</a>, 
                        <a href="author_page.cfm?id=81100182818&CFID=105753441&CFTOKEN=50690582">Kenji Araki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 143-144</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157726" title="DOI">10.1145/2157689.2157726</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157726&ftid=1161725&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow37" style="display:inline;"><br /><div style="display:inline">The current understanding is that human-likeness of a robot leads to better human perception. However, the factors have not been thoroughly studied. We conducted a laboratory experiment to examine two questions: how verb acquisition ability affects human ...</div></span>
          <span id="toHide37" style="display:none;"><br /><div style="display:inline"><p>The current understanding is that human-likeness of a robot leads to better human perception. However, the factors have not been thoroughly studied. We conducted a laboratory experiment to examine two questions: how verb acquisition ability affects human perceptions on human-likeness and familiarity of a humanoid robot, intention to use the robot, and enjoyment and satisfaction of the interaction, and whether human-likeness mediates the links between the effects of interaction of verb acquisition between the human perceptions. The experiment involved 48 participants, and we found that the robot that was able to acquire two Japanese verbs, "<i>oku</i> (to put/to place)" and "<i>hanasu</i> (to move away from)," was perceived by participants as more familiar and satisfying than the one that knew the verbs from the beginning. We also found that human-likeness mediated the links between the effect of verb acquisition ability and other perceptions toward the robot.</p></div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157727&CFID=105753441&CFTOKEN=50690582">Towards a computational method of scaling a robot's behavior via proxemics</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Zachary Henkel, Robin R. Murphy, Cindy L. Bethel 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 145-146</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157727" title="DOI">10.1145/2157689.2157727</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157727&ftid=1161726&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow38" style="display:inline;"><br /><div style="display:inline">Humans regulate their social behavior based on proximity to other social actors. Likewise, when a robot fulfills the role of a social actor it too should regulate its interaction based on proximity. This paper describes work in progress to establish ...</div></span>
          <span id="toHide38" style="display:none;"><br /><div style="display:inline"><p>Humans regulate their social behavior based on proximity to other social actors. Likewise, when a robot fulfills the role of a social actor it too should regulate its interaction based on proximity. This paper describes work in progress to establish methods for autonomous modification of social behavior based on proximity and to quantify human preferences between methods of scaling a robot's social behaviors based on distance from a human. The preliminary results of a 72 participant human study examine the reaction to scaling with linear methods and perception-based methods. Results indicate significantly higher ratings in multiple areas (comfort, natural movement, safety, self-control, intelligence, likability, submissiveness (<b>p<.05</b>) when using a perception-based scaling function, as opposed to a linear or no scaling function. Work in progress is analyzing the biometric measures collected.</p></div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157728&CFID=105753441&CFTOKEN=50690582">Using the behavior markup language for human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Aaron Holroyd, Charles Rich 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 147-148</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157728" title="DOI">10.1145/2157689.2157728</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157728&ftid=1161727&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow39" style="display:inline;"><br /><div style="display:inline">This paper describes a Behavior Markup Language (BML) realizer that we developed for use in our research on human-robot interaction. Existing BML realizers used with virtual agents are based on fixed-timing algorithms and because of that are not suitable ...</div></span>
          <span id="toHide39" style="display:none;"><br /><div style="display:inline"><p>This paper describes a Behavior Markup Language (BML) realizer that we developed for use in our research on human-robot interaction. Existing BML realizers used with virtual agents are based on fixed-timing algorithms and because of that are not suitable for robotic applications. Our realizer uses an event-driven architecture, based on Petri nets, to guarantee the specified synchronization constraints in the presence of unpredictable variability in robot control systems. Our implementation is robot independent, open source and uses the Robot Operating System (ROS).</p></div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157729&CFID=105753441&CFTOKEN=50690582">Attracting and controlling human attention through robot's behaviors suited to the situation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81488649088&CFID=105753441&CFTOKEN=50690582">Mohammed Moshiul Hoque</a>, 
                        <a href="author_page.cfm?id=81500651661&CFID=105753441&CFTOKEN=50690582">Tomomi Onuki</a>, 
                        <a href="author_page.cfm?id=81100143285&CFID=105753441&CFTOKEN=50690582">Dipankar Das</a>, 
                        <a href="author_page.cfm?id=81100604144&CFID=105753441&CFTOKEN=50690582">Yoshinori Kobayashi</a>, 
                        <a href="author_page.cfm?id=81100082332&CFID=105753441&CFTOKEN=50690582">Yoshinori Kuno</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 149-150</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157729" title="DOI">10.1145/2157689.2157729</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157729&ftid=1161728&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=2157729&ftid=1161729&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow40" style="display:inline;"><br /><div style="display:inline">A major challenge is to design a robot that can attract and control human attention in various social situations. If a robot would like to communicate a person, it may turn its gaze to him/her for eye contact. However, it is not an easy task for the ...</div></span>
          <span id="toHide40" style="display:none;"><br /><div style="display:inline"><p>A major challenge is to design a robot that can attract and control human attention in various social situations. If a robot would like to communicate a person, it may turn its gaze to him/her for eye contact. However, it is not an easy task for the robot to make eye contact because such a turning action alone may not be enough in all situations, especially when the robot and the human are not facing each other. In this paper, we present an attention control approach through robot's behaviors that can attract a person's attention by three actions: head turning, head shaking, and uttering reference terms corresponding to three viewing situations in which the human vision senses the robot (near peripheral field of view, far peripheral field of view, and out of field of view). After gaining attention, the robot makes eye contact through showing gaze awareness by blinking its eyes, and directs the human attention by eye and head turning behaviors to share an object.</p></div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157730&CFID=105753441&CFTOKEN=50690582">An intentional framework improves memory for a robot's actions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Alicia M. Hymel, Daniel T. Levin 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 151-152</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157730" title="DOI">10.1145/2157689.2157730</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157730&ftid=1161730&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow41" style="display:inline;"><br /><div style="display:inline">Although a number of recent studies have explored people's concepts about robots, almost no research has tested the degree to which these concepts affect people's capacity to understand and remember a robot's actions. In this study, we tested whether ...</div></span>
          <span id="toHide41" style="display:none;"><br /><div style="display:inline"><p>Although a number of recent studies have explored people's concepts about robots, almost no research has tested the degree to which these concepts affect people's capacity to understand and remember a robot's actions. In this study, we tested whether a narrative describing a robot performing basic intentional acts would be easier to remember than a narrative that described similar non-intentional actions. Participants read one of two stories about a robot in which it was either described as having intentional or non-intentional mental representations. Participants who read about the intentional robot were more likely to recall information about the robotic agent, but there was no difference between the two groups in accuracy for questions unrelated to the agent. Additionally, participants who read about the intentional robot were marginally more likely to falsely recall a non-present object that was similar to the objects that the robot did interact with. We conclude that beliefs about a robot affect encoding and recall of its actions, possibly due to a focus on the type of information the agent is believed to "mentally" represent.</p></div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157731&CFID=105753441&CFTOKEN=50690582">Gestonurse: a multimodal robotic scrub nurse</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Mithun George Jacob, Yu-Ting Li, Juan P. Wachs 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 153-154</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157731" title="DOI">10.1145/2157689.2157731</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157731&ftid=1161731&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow42" style="display:inline;"><br /><div style="display:inline">A novel multimodal robotic scrub nurse (RSN) system for the operating room (OR) is presented. The RSN assists the main surgeon by passing surgical instruments. Experiments were conducted to test the system with speech and gesture modalities and average ...</div></span>
          <span id="toHide42" style="display:none;"><br /><div style="display:inline"><p>A novel multimodal robotic scrub nurse (RSN) system for the operating room (OR) is presented. The RSN assists the main surgeon by passing surgical instruments. Experiments were conducted to test the system with speech and gesture modalities and average instrument acquisition times were compared. Experimental results showed that 97% of the gestures were recognized correctly under changes in scale and rotation and that the multimodal system responded faster than the unimodal systems. A relationship similar in form to Fitts's law for instrument picking accuracy is also presented.</p></div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157732&CFID=105753441&CFTOKEN=50690582">Manipulation with soft-fingertips for safe pHRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jorge Armendariz, Rodolfo Garc&#237;a-Rodr&#237;guez, Felipe Machorro-Fern&#225;ndez, Vicente Parra-Vega 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 155-156</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157732" title="DOI">10.1145/2157689.2157732</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157732&ftid=1161732&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow43" style="display:inline;"><br /><div style="display:inline">Manipulation with soft-fingertips is proposed as a tool for interacting with rigid objects, based on an online bilateral teleoperation fuzzy controller, without time delay. The fuzzy inference engine tunes continuously the force feedback gain to increase ...</div></span>
          <span id="toHide43" style="display:none;"><br /><div style="display:inline"><p>Manipulation with soft-fingertips is proposed as a tool for interacting with rigid objects, based on an online bilateral teleoperation fuzzy controller, without time delay. The fuzzy inference engine tunes continuously the force feedback gain to increase awareness of physical interaction so as to grasping is achieved. In contrast to the case of rigid fingertip, wherein infinitely small contact point is assumed, it is argued that our scheme allows safe and intuitive interaction, which has proved effective in an experimental study with 19 subjects. Results indicate success due to feedback of slave contact force to the human user. Subjects consistently judge comfort and easiness of manipulation, with an experimental two-hand prototype.</p></div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157733&CFID=105753441&CFTOKEN=50690582">Studying virtual worlds as medium for telepresence robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453643744&CFID=105753441&CFTOKEN=50690582">Alex Juarez</a>, 
                        <a href="author_page.cfm?id=81100461702&CFID=105753441&CFTOKEN=50690582">Christoph Bartneck</a>, 
                        <a href="author_page.cfm?id=81500657116&CFID=105753441&CFTOKEN=50690582">Loe Feijs</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 157-158</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157733" title="DOI">10.1145/2157689.2157733</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157733&ftid=1161733&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow44" style="display:inline;"><br /><div style="display:inline">This paper presents a study on the effects of using virtual worlds as medium for interaction with telepresence robots. The Prototype for Assisted Communication PAC4 is also introduced. This system connects virtual worlds with real robots, allowing virtual ...</div></span>
          <span id="toHide44" style="display:none;"><br /><div style="display:inline"><p>This paper presents a study on the effects of using virtual worlds as medium for interaction with telepresence robots. The Prototype for Assisted Communication PAC4 is also introduced. This system connects virtual worlds with real robots, allowing virtual world users to access the robot capabilities in a telepresence scenario. A user experiment was conducted to evaluate the effects of PAC4 on the feeling of social presence experienced by virtual world users.</p></div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157734&CFID=105753441&CFTOKEN=50690582">A follow-up on humanoid-mediated stroke physical rehabilitation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Hee-Tae Jung, Yu-Kyong Choe, Jennifer Baird, Roderic A. Grupen 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 159-160</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157734" title="DOI">10.1145/2157689.2157734</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157734&ftid=1161734&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow45" style="display:inline;"><br /><div style="display:inline">We report the results of standardized tests on a single subject with a stroke at 4, 20 and 28 weeks after completion of the study. These results follow from previous work[1]. The subject demonstrated sustained improvement in motor function 28 weeks after ...</div></span>
          <span id="toHide45" style="display:none;"><br /><div style="display:inline"><p>We report the results of standardized tests on a single subject with a stroke at 4, 20 and 28 weeks after completion of the study. These results follow from previous work[1]. The subject demonstrated sustained improvement in motor function 28 weeks after completing the study. In addition to quantitative results, the questionnaire results by the subject and the spouse testify that the subjective user experience was also positive. This further advocates the use of general purpose robots to complement human therapists.</p></div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157735&CFID=105753441&CFTOKEN=50690582">Personality and facial expressions in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Soyoung Jung, Hyoung-taek Lim, Sanghun Kwak, Frank Biocca 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 161-162</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157735" title="DOI">10.1145/2157689.2157735</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157735&ftid=1161735&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow46" style="display:inline;"><br /><div style="display:inline">This paper presents the propensities of preference for Human Robot Interaction (HRI) according to different personalities and facial expressions of humans and robots. This study is based on two types of personalities: extroverted and introverted. According ...</div></span>
          <span id="toHide46" style="display:none;"><br /><div style="display:inline"><p>This paper presents the propensities of preference for Human Robot Interaction (HRI) according to different personalities and facial expressions of humans and robots. This study is based on two types of personalities: extroverted and introverted. According to the personality, the facial expressions are distinguished to express extroversion and introversion. The design of experiment is a 3 (participant's group of personality: extrovert, intermediate, introvert) x 2 (robot's groups of personality) between-subjects experiment (N=40) in which participants interact with KMC-EXPR robots expressing either extroversion or introversion. The results showed an unprecedented hypothesis, unexpected implications and certain propensities of preferences.</p></div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157736&CFID=105753441&CFTOKEN=50690582">Understanding situational awareness in multi-unit supervisory control through data-mining and modeling with real-time strategy games</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Donald J. Kalar, Collin B. Green 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 163-164</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157736" title="DOI">10.1145/2157689.2157736</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157736&ftid=1161736&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow47" style="display:inline;"><br /><div style="display:inline">As robots become increasingly capable and autonomous, the role of a human operator may be to supervise multiple robots and intervene to handle problems and provide strategic guidance. In such cases, the extent to which HRI tools support the human supervisor's ...</div></span>
          <span id="toHide47" style="display:none;"><br /><div style="display:inline"><p>As robots become increasingly capable and autonomous, the role of a human operator may be to supervise multiple robots and intervene to handle problems and provide strategic guidance. In such cases, the extent to which HRI tools support the human supervisor's situational awareness (SA) and ability to intervene in an appropriate and timely fashion will constrain the scale of operations (e.g., the number of robots; the complexity of tasks) that can reasonably be supervised by a single person. One approach to understanding how humans might acquire, maintain, and use situational awareness in multi-robot supervision tasks is to look at video games that require similar activities. We describe our initial efforts at analyzing and modeling data from Real-Time Strategy (RTS) games with the goal of answering basic questions about the nature of situational awareness and supervisory control of multiple semi-autonomous agents.</p></div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157737&CFID=105753441&CFTOKEN=50690582">Cultural studies in the HRI loop</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Andra Keay 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 165-166</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157737" title="DOI">10.1145/2157689.2157737</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157737&ftid=1161737&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow48" style="display:inline;"><br /><div style="display:inline">In this paper, I outline the scope and methodology of cultural studies and the possible applications in HRI. It is not just robots that interact with humans, but humans who interact with humans through the medium of robots. Culture is the context that ...</div></span>
          <span id="toHide48" style="display:none;"><br /><div style="display:inline"><p>In this paper, I outline the scope and methodology of cultural studies and the possible applications in HRI. It is not just robots that interact with humans, but humans who interact with humans through the medium of robots. Culture is the context that informs all our actions. Cultural theory's eclectic methodology is useful for sifting through a large, multidisciplinary and cross-cultural field like HRI, for identifying disputed or neglected areas and for framing future research questions.</p></div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157738&CFID=105753441&CFTOKEN=50690582">Robot competitions as a birth ritual</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Andra Keay 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 167-168</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157738" title="DOI">10.1145/2157689.2157738</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157738&ftid=1161738&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow49" style="display:inline;"><br /><div style="display:inline">Competitions play an important role in introducing new robots to the field. The robot competition is a rite of passage analogous to birth when compared to studies of the technologized relations of birth in American hospitals. The sanctioned recording ...</div></span>
          <span id="toHide49" style="display:none;"><br /><div style="display:inline"><p>Competitions play an important role in introducing new robots to the field. The robot competition is a rite of passage analogous to birth when compared to studies of the technologized relations of birth in American hospitals. The sanctioned recording of robot names as part of the competition marks the roles that robots play in society. This paper explores underlying social factors that may influence robotics.</p></div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157739&CFID=105753441&CFTOKEN=50690582">Applying team heuristics to future human-robot systems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Joseph Roland Keebler, Florian Jentsch, Thomas Fincannon, Irwin Hudson 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 169-170</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157739" title="DOI">10.1145/2157689.2157739</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157739&ftid=1161739&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow50" style="display:inline;"><br /><div style="display:inline">In this paper we briefly describe teaming heuristics as they are applied to human-human teams, and demonstrate their adaptability to human-robot (HR) teams. We discuss a framework developed from Salas's models on teamwork and team training. As HRI technology ...</div></span>
          <span id="toHide50" style="display:none;"><br /><div style="display:inline"><p>In this paper we briefly describe teaming heuristics as they are applied to human-human teams, and demonstrate their adaptability to human-robot (HR) teams. We discuss a framework developed from Salas's models on teamwork and team training. As HRI technology moves from tele-operative control methods to teamwork with intelligent robots, it is pertinent to properly integrate knowledge about teams into the development of robotic systems. This should lead to highly effective team systems, and may provide insight into the design of robotic entities and system protocols.</p></div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157740&CFID=105753441&CFTOKEN=50690582">Deep networks for predicting human intent with respect to objects</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81388598666&CFID=105753441&CFTOKEN=50690582">Richard Kelley</a>, 
                        <a href="author_page.cfm?id=81500661930&CFID=105753441&CFTOKEN=50690582">Liesl Wigand</a>, 
                        <a href="author_page.cfm?id=81500647299&CFID=105753441&CFTOKEN=50690582">Brian Hamilton</a>, 
                        <a href="author_page.cfm?id=81500649236&CFID=105753441&CFTOKEN=50690582">Katie Browne</a>, 
                        <a href="author_page.cfm?id=81100536266&CFID=105753441&CFTOKEN=50690582">Monica Nicolescu</a>, 
                        <a href="author_page.cfm?id=81100536268&CFID=105753441&CFTOKEN=50690582">Mircea Nicolescu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 171-172</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157740" title="DOI">10.1145/2157689.2157740</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157740&ftid=1161740&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow51" style="display:inline;"><br /><div style="display:inline">Effective human-robot interaction requires systems that can accurately infer and predict human intentions. In this paper, we introduce a system that uses stacked denoising autoencoders to perform intent recognition. We introduce the intent recognition ...</div></span>
          <span id="toHide51" style="display:none;"><br /><div style="display:inline"><p>Effective human-robot interaction requires systems that can accurately infer and predict human intentions. In this paper, we introduce a system that uses stacked denoising autoencoders to perform intent recognition. We introduce the intent recognition problem, provide an overview of deep architectures in machine learning, and outline the components of our system. We also provide preliminary results for our system's performance.</p></div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157741&CFID=105753441&CFTOKEN=50690582">User attentive behavior with camera view for in-situ robot control</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482643414&CFID=105753441&CFTOKEN=50690582">Jong-gil Ahn</a>, 
                        <a href="author_page.cfm?id=81488662765&CFID=105753441&CFTOKEN=50690582">Hyunseok Yang</a>, 
                        <a href="author_page.cfm?id=81452598848&CFID=105753441&CFTOKEN=50690582">Gerard Jounghyun Kim</a>, 
                        <a href="author_page.cfm?id=81365596889&CFID=105753441&CFTOKEN=50690582">Namgyu Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 173-174</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157741" title="DOI">10.1145/2157689.2157741</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157741&ftid=1161741&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow52" style="display:inline;"><br /><div style="display:inline">In this poster, we present an experiment that compares three forms of interaction to study the user behavior with regards to the effects of camera view for in-situ robot control. We compared three hand-held interfaces with: (1) no camera view (Nominal), ...</div></span>
          <span id="toHide52" style="display:none;"><br /><div style="display:inline"><p>In this poster, we present an experiment that compares three forms of interaction to study the user behavior with regards to the effects of camera view for in-situ robot control. We compared three hand-held interfaces with: (1) no camera view (Nominal), (2) a camera view/aim is always fixed toward the robot (Fixed) and (3) a camera view with user controlled aim (Free). The three approaches represent different balances between information availability, interface accessibility and the amount of induced attentional shifts. Experiment results have shown that all three interaction models exhibited similar task performance even though the Fixed type induced much less attentional shifts. On the other hand, the users much preferred the Nominal and Free type. Users mostly ignored the camera view despite having to shift one's attention excessively, due to the lack of visual quality, realistic scale and depth information.</p></div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157742&CFID=105753441&CFTOKEN=50690582">Tracking aggregate vs. individual gaze behaviors during a robot-led tour simplifies overall engagement estimates</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Heather Knight, Reid Simmons 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 175-176</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157742" title="DOI">10.1145/2157689.2157742</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157742&ftid=1161742&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow53" style="display:inline;"><br /><div style="display:inline">As an early behavioral study of what non-verbal features a robot tourguide could use to analyze a crowd, personalize an interaction and/or maintain high levels of engagement, we analyze participant gaze statistics in response to a robot tour guide's ...</div></span>
          <span id="toHide53" style="display:none;"><br /><div style="display:inline"><p>As an early behavioral study of what non-verbal features a robot tourguide could use to analyze a crowd, personalize an interaction and/or maintain high levels of engagement, we analyze participant gaze statistics in response to a robot tour guide's deictic gestures. There were thirty-seven participants overall split into nine groups of three to five people each. In groups with the lowest engagement levels aggregate gaze responses in response to the robot deictic gesture involved the fewest total glance shifts, least time spent looking at indicated object and no intra-participant gaze. Our diverse participants had overlapping engagement ratings within their group, and we found that a robot that tracks group rather than individual analytics could capture less noisy and often stronger trends relating gaze features to self-reported engagement scores. Thus we have found indications that aggregate group analysis captures more salient and accurate assessments of overall <i>humans-robot interactions</i>, even with lower resolution features.</p></div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157743&CFID=105753441&CFTOKEN=50690582">Real time interaction with mobile robots using hand gestures</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500664096&CFID=105753441&CFTOKEN=50690582">Kishore Reddy Konda</a>, 
                        <a href="author_page.cfm?id=81500645677&CFID=105753441&CFTOKEN=50690582">Achim K&#246;nigs</a>, 
                        <a href="author_page.cfm?id=81474689654&CFID=105753441&CFTOKEN=50690582">Hannes Schulz</a>, 
                        <a href="author_page.cfm?id=81100314485&CFID=105753441&CFTOKEN=50690582">Dirk Schulz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 177-178</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157743" title="DOI">10.1145/2157689.2157743</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157743&ftid=1161743&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow54" style="display:inline;"><br /><div style="display:inline">We developed a robust real time hand gesture based interaction system to effectively communicate with a mobile robot which can operate in an outdoor environment. The system enables the user to operate a mobile robot using hand gesture based commands. ...</div></span>
          <span id="toHide54" style="display:none;"><br /><div style="display:inline"><p>We developed a robust real time hand gesture based interaction system to effectively communicate with a mobile robot which can operate in an outdoor environment. The system enables the user to operate a mobile robot using hand gesture based commands. In particular the system offers direct on site interaction providing better perception of environment to the user. To overcome the illumination challenges in outdoors, the system operates on depth images. Processed depth images are given as input to a convolutional neural network which is trained to detect static hand gestures.</p> <p>The system is evaluated in real world experiments on a mobile robot to show the operational efficiency in outdoor environment.</p></div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157744&CFID=105753441&CFTOKEN=50690582">Integrating human and computer vision with EEG toward the control of a prosthetic arm</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Eugene Lavely, Geoffrey Meltzner, Rick Thompson 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 179-180</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157744" title="DOI">10.1145/2157689.2157744</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157744&ftid=1161744&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow55" style="display:inline;"><br /><div style="display:inline">We are undertaking the development of a brain computer interface (BCI) [1] for control of an upper limb prosthetic. Our approach exploits electrical neural activity data for motor intent estimation, and eye gaze direction for target selection. These ...</div></span>
          <span id="toHide55" style="display:none;"><br /><div style="display:inline"><p>We are undertaking the development of a brain computer interface (BCI) [1] for control of an upper limb prosthetic. Our approach exploits electrical neural activity data for motor intent estimation, and eye gaze direction for target selection. These data streams are augmented by computer vision (CV) for 3D scene reconstruction, and are integrated with a hierarchical controller to achieve semi-autonomous control. User interfaces for the effective control of the many degrees of freedom (DOF) of advanced prosthetic arms are not yet available [2]. Ideally the combined arm and interface technology provides the user with reliable and dexterous capability for reaching, grasping and fine-scale manipulation. Technologies that improve arm embodiment i.e., the impression by the amputee subject that the arm is a natural part of their body-concept presents an important and difficult challenge to the human-robot interaction research community. Such embodiment is clearly predicated on cross-disciplinary advances, including accurate intent estimation and an and an algorithmic basis for natural arm control.</p></div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157745&CFID=105753441&CFTOKEN=50690582">Human-robot interaction in the MORSE simulator</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          S&#233;verin Lemaignan, Gilberto Echeverria, Michael Karg, Jim Mainprice, Alexandra Kirsch, Rachid Alami 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 181-182</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157745" title="DOI">10.1145/2157689.2157745</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157745&ftid=1161745&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow56" style="display:inline;"><br /><div style="display:inline">Over the last two years, the Modular OpenRobots Simulation Engine (MORSE) project1 went from a simple extension plugged on the Blender's Game Engine to a full-fledged simulation environment for academic robotics. Driven by the requirements ...</div></span>
          <span id="toHide56" style="display:none;"><br /><div style="display:inline"><p>Over the last two years, the Modular OpenRobots Simulation Engine (MORSE) project<sup>1</sup> went from a simple extension plugged on the Blender's <i>Game Engine</i> to a full-fledged simulation environment for academic robotics. Driven by the requirements of several of its developers, tools dedicated to Human-Robot interaction simulation have taken a prominent place in the project. This late breaking report discusses some of the recent additions in this domain, including the immersive experience provided by the integration of the Kinect device as input controller. We also give an overview of the experiences we plan to complete in the coming months.</p></div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157746&CFID=105753441&CFTOKEN=50690582">Vision-based attention estimation and selection for social robot to perform natural interaction in the open world</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Liyuan Li, Xinguo Yu, Jun Li, Gang Wang, Ji-Yu Shi, Yeow Kee Tan, Haizhou Li 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 183-184</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157746" title="DOI">10.1145/2157689.2157746</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157746&ftid=1161746&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=2157746&ftid=1161747&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow57" style="display:inline;"><br /><div style="display:inline">In this paper, a novel vision system is proposed to estimate attention of people from rich visual clues for social robot to perform natural interactions with multiple participants in public environments. The vision detection and recognition modules include ...</div></span>
          <span id="toHide57" style="display:none;"><br /><div style="display:inline"><p>In this paper, a novel vision system is proposed to estimate attention of people from rich visual clues for social robot to perform natural interactions with multiple participants in public environments. The vision detection and recognition modules include multi-person detection and tracking, upper-body pose recognition, face and gaze detection, lip motion analysis for speaking recognition, and facial expression recognition. A computational approach is proposed to generate a quantitative estimation of human attention. The vision system is implemented on a robotic receptionist "EVE" and encouraging results have been obtained.</p></div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157747&CFID=105753441&CFTOKEN=50690582">A prototyping environment for interaction between a human and a robotic multi-agent system</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Michael Lichtenstern, Martin Frassl, Bernhard Perun, Michael Angermann 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 185-186</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157747" title="DOI">10.1145/2157689.2157747</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157747&ftid=1161748&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=2157747&ftid=1161749&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow58" style="display:inline;"><br /><div style="display:inline">In this paper we describe our prototyping environment to study concepts for empowering a single user to control robotic multi-agent systems. We investigate and validate these concepts by experiments with a fleet of hovering robots. Specifically, we report ...</div></span>
          <span id="toHide58" style="display:none;"><br /><div style="display:inline"><p>In this paper we describe our prototyping environment to study concepts for empowering a single user to control robotic multi-agent systems. We investigate and validate these concepts by experiments with a fleet of hovering robots. Specifically, we report on a first experiment in which one robot is equipped with an RGB-D sensor through which the user is enabled to directly interact with a multi-agent system without the need to carry any device.</p></div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157748&CFID=105753441&CFTOKEN=50690582">Explaining robot actions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Meghann Lomas, Robert Chevalier, Ernest Vincent Cross, II, Robert Christopher Garrett, John Hoare, Michael Kopack 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 187-188</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157748" title="DOI">10.1145/2157689.2157748</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157748&ftid=1161750&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow59" style="display:inline;"><br /><div style="display:inline">To increase human trust in robots, we have developed a system that provides insight into robotic behaviors by enabling a robot to answer questions people pose about its actions (e.g., Q: Why did you turn left there? A: "I detected a person at the end ...</div></span>
          <span id="toHide59" style="display:none;"><br /><div style="display:inline"><p>To increase human trust in robots, we have developed a system that provides insight into robotic behaviors by enabling a robot to answer questions people pose about its actions (e.g., Q: Why did you turn left there? A: "I detected a person at the end of the hallway."). Our focus is on generation of this explanation in human-understandable terms <i>despite the mathematical, robot-specific representation and planning system</i> used by the robot to make its decisions and execute its actions. We present our work to date on this topic, including system design and experiments, and discuss areas for future work.</p></div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157749&CFID=105753441&CFTOKEN=50690582">Applying politeness maxims in social robotics polite dialogue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Qin En Looi, Swee Lan See 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 189-190</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157749" title="DOI">10.1145/2157689.2157749</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157749&ftid=1161751&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow60" style="display:inline;"><br /><div style="display:inline">An important element of human-robot interaction, as with inter-human interaction, is conversation. Having previously suggested the Gricean maxims as suitable guidelines for social robotics dialogue, we discovered that a preferable alternative set of ...</div></span>
          <span id="toHide60" style="display:none;"><br /><div style="display:inline"><p>An important element of human-robot interaction, as with inter-human interaction, is conversation. Having previously suggested the Gricean maxims as suitable guidelines for social robotics dialogue, we discovered that a preferable alternative set of guidelines: politeness maxims. In this paper, we will introduce the politeness maxims and propose its enhanced applicability in human-robot interaction to create polite dialogue. Although no experimental results are available to support our proposition, the preliminary analysis of politeness maxims presents a promising future, as guidelines for the synthesis of polite robot dialogue. Through effective dialogue creation, the interaction with human and robots will be more pleasant, indicating a step forward in effective human-robot interaction.</p></div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157750&CFID=105753441&CFTOKEN=50690582">Transfer from a simulation environment to a live robotic environment: are certain demographics better?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Patricia L. McDermott, Alia Fisher, Thomas Carolan, Mark R. Gronowski, Marc Gacy, Michael Overstreet 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 191-192</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157750" title="DOI">10.1145/2157689.2157750</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157750&ftid=1161752&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow61" style="display:inline;"><br /><div style="display:inline">The ability to remotely operate an unmanned vehicle while simultaneously looking for suspicious targets and then classifying those targets is not a trivial skill. This study looked at different training approaches to make better use of simulation as ...</div></span>
          <span id="toHide61" style="display:none;"><br /><div style="display:inline"><p>The ability to remotely operate an unmanned vehicle while simultaneously looking for suspicious targets and then classifying those targets is not a trivial skill. This study looked at different training approaches to make better use of simulation as a first training step. When transferring to a live environment, the operators could be grouped into two categories according to whether they passed live training criteria or not. There were clear performance differences between these groups. The group that failed to pass criteria had poorer performance overall, more SA errors, and spent more time in training. Post-hoc analysis showed differences in the demographics between those who passed and those that did not. Male participants and younger participants were more likely to achieve criteria. There were no differences in gaming experience and perceived sense of direction.</p></div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157751&CFID=105753441&CFTOKEN=50690582">A probabilistic framework for autonomous proxemic control in situated and mobile human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Ross Mead, Maja J. Mataric 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 193-194</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157751" title="DOI">10.1145/2157689.2157751</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157751&ftid=1161753&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow62" style="display:inline;"><br /><div style="display:inline">In this paper, we draw upon insights gained in our previous work on human-human proxemic behavior analysis to develop a novel method for human-robot proxemic behavior production. A probabilistic framework for spatial interaction has been developed that ...</div></span>
          <span id="toHide62" style="display:none;"><br /><div style="display:inline"><p>In this paper, we draw upon insights gained in our previous work on human-human proxemic behavior analysis to develop a novel method for human-robot proxemic behavior production. A probabilistic framework for spatial interaction has been developed that considers the sensory experience of each agent (human or robot) in a co-present social encounter. In this preliminary work, a robot attempts to maintain a set of human body features in its camera field-of-view. This methodology addresses the functional aspects of proxemic behavior in human-robot interaction, and provides an elegant connection between previous approaches.</p></div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157752&CFID=105753441&CFTOKEN=50690582">Control of human-machine interaction for wide area search munitions in the presence of target uncertainty</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Pia E.K. Berg-Yuen, Siddhartha S. Mehta, Eduardo L. Pasiliao, Robert A. Murphey 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 195-196</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157752" title="DOI">10.1145/2157689.2157752</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157752&ftid=1161754&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow63" style="display:inline;"><br /><div style="display:inline">In this report we describe the progress in developing a control architecture for human-in-the-loop wide area search munitions to reduce operator errors in the presence of unreliable automation and operator cognitive limitations. An optimal input tracking ...</div></span>
          <span id="toHide63" style="display:none;"><br /><div style="display:inline"><p>In this report we describe the progress in developing a control architecture for human-in-the-loop wide area search munitions to reduce operator errors in the presence of unreliable automation and operator cognitive limitations. An optimal input tracking controller with adaptive automation uncertainty compensation and real-time workload assessment is developed to improve the system performance. Extensive simulations based on the experimental data involving 12 subjects demonstrate effectiveness of the presented controller.</p></div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157753&CFID=105753441&CFTOKEN=50690582">Referent identification process in human-robot multimodal communication</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Yuta Shibasaki, Takahiro Inaba, Yukiko I. Nakano 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 197-198</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157753" title="DOI">10.1145/2157689.2157753</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157753&ftid=1161755&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow64" style="display:inline;"><br /><div style="display:inline">This paper presents a communication robot that can generate a referent identification conversation with human users. First, we conduct an experiment to collect face-to-face referent identification communication and investigate how the referent is identified ...</div></span>
          <span id="toHide64" style="display:none;"><br /><div style="display:inline"><p>This paper presents a communication robot that can generate a referent identification conversation with human users. First, we conduct an experiment to collect face-to-face referent identification communication and investigate how the referent is identified by exchanging multiple speech turns between the participants. On the basis of the experimental observations, we implement a communication robot that can manage a referent identification conversation with a user by integrating the linguistic information obtained from speech recognition and the vision information obtained from a robot camera.</p></div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157754&CFID=105753441&CFTOKEN=50690582">Listener agent for elderly people with dementia</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Yoichi Sakai, Yuuko Nonaka, Kiyoshi Yasuda, Yukiko I. Nakano 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 199-200</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157754" title="DOI">10.1145/2157689.2157754</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157754&ftid=1161756&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow65" style="display:inline;"><br /><div style="display:inline">With the goal of developing a conversational humanoid that can serve as a companion for people with dementia, we propose an autonomous virtual agent that can generate backchannel feedback, such as head nods and verbal acknowledgement, on the basis of ...</div></span>
          <span id="toHide65" style="display:none;"><br /><div style="display:inline"><p>With the goal of developing a conversational humanoid that can serve as a companion for people with dementia, we propose an autonomous virtual agent that can generate backchannel feedback, such as head nods and verbal acknowledgement, on the basis of acoustic information in the user's speech. The system is also capable of speech recognition and language understanding functionalities, which are potentially useful for evaluating the cognitive status of elderly people on a daily basis.</p></div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157755&CFID=105753441&CFTOKEN=50690582">Can you hold my hand?: physical warmth in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jiaqi Nie, Michelle Pak, Angie Lorena Marin, S. Shyam Sundar 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 201-202</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157755" title="DOI">10.1145/2157689.2157755</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157755&ftid=1161757&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow66" style="display:inline;"><br /><div style="display:inline">This study investigates whether the temperature of a robot's hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot ...</div></span>
          <span id="toHide66" style="display:none;"><br /><div style="display:inline"><p>This study investigates whether the temperature of a robot's hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot hand or (2) holding a cold robot hand or (3) not holding a robot hand. The effects of this simulated 'human touch' on HRI were examined in the context of viewing a horror film clip. Results suggest that experiences of physical warmth and handholding increase feelings of friendship and trust toward the robot. However, the discrepancy between the expectation of an actual human touch and the mechanical appearance of a robot could result in negative effects.</p></div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157756&CFID=105753441&CFTOKEN=50690582">Captain may i?: proxemics study examining factors that influence distance between humanoid robots, children, and adults, during human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Sandra Y. Okita, Victor Ng-Thow-Hing, Ravi Kiran Sarvadevabhatla 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 203-204</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157756" title="DOI">10.1145/2157689.2157756</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157756&ftid=1161758&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow67" style="display:inline;"><br /><div style="display:inline">This proxemics study examines whether the physical distance between robots and humans differ based on the following factors: 1) age: children vs. adults, 2) who initiates the approach: humans approaching the robot vs. robot approaching humans, 3) prompting: ...</div></span>
          <span id="toHide67" style="display:none;"><br /><div style="display:inline"><p>This proxemics study examines whether the physical distance between robots and humans differ based on the following factors: 1) age: children vs. adults, 2) who initiates the approach: humans approaching the robot vs. robot approaching humans, 3) prompting: verbal invitation vs. non-verbal gesture (e.g., beckoning), and 4) informing: announcement vs. permission vs. nothing. Results showed that both verbal and non-verbal prompting had significant influence on physical distance. Physiological data is also used to detect the appropriate timing of approach for a more natural and comfortable interaction.</p></div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157757&CFID=105753441&CFTOKEN=50690582">Online gaming with robots vs. computers as allies vs. opponents</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500644117&CFID=105753441&CFTOKEN=50690582">Eunil Park</a>, 
                        <a href="author_page.cfm?id=81484653922&CFID=105753441&CFTOKEN=50690582">Ki Joon Kim</a>, 
                        <a href="author_page.cfm?id=81500648494&CFID=105753441&CFTOKEN=50690582">S. Shyam Sundar</a>, 
                        <a href="author_page.cfm?id=81500657746&CFID=105753441&CFTOKEN=50690582">Angel P. del Pobil</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 205-206</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157757" title="DOI">10.1145/2157689.2157757</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157757&ftid=1161759&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow68" style="display:inline;"><br /><div style="display:inline">A 2 x 2 between-subjects experiment was conducted to examine the effects of the type of artificial agent (robot vs. computer) and the role of the agent (ally vs. enemy) on people's perceptions and evaluations of the agent when playing a video game. Participants ...</div></span>
          <span id="toHide68" style="display:none;"><br /><div style="display:inline"><p>A 2 x 2 between-subjects experiment was conducted to examine the effects of the type of artificial agent (robot vs. computer) and the role of the agent (ally vs. enemy) on people's perceptions and evaluations of the agent when playing a video game. Participants perceived that playing the game with a robot was more enjoyable and easier than playing with a computer. Regardless of the agent type, participants reported that playing the game was more enjoyable when the agent played as an ally rather than as their opponent. Implications of notable findings are discussed.</p></div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157758&CFID=105753441&CFTOKEN=50690582">The effects of immersive tendency and need to belong on human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500661325&CFID=105753441&CFTOKEN=50690582">Ki Joon Kim</a>, 
                        <a href="author_page.cfm?id=81482643896&CFID=105753441&CFTOKEN=50690582">Eunil Park</a>, 
                        <a href="author_page.cfm?id=81500648493&CFID=105753441&CFTOKEN=50690582">S. Shyam Sundar</a>, 
                        <a href="author_page.cfm?id=81100147388&CFID=105753441&CFTOKEN=50690582">Angel P. del Pobil</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 207-208</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157758" title="DOI">10.1145/2157689.2157758</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157758&ftid=1161760&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow69" style="display:inline;"><br /><div style="display:inline">Do individual differences in dispositional behavioral tendencies, such as immersive tendency and need to belong, play a significant role in human-robot interaction? To answer this question, the present study conducted a 2 x 2 between-subjects experiment ...</div></span>
          <span id="toHide69" style="display:none;"><br /><div style="display:inline"><p>Do individual differences in dispositional behavioral tendencies, such as immersive tendency and need to belong, play a significant role in human-robot interaction? To answer this question, the present study conducted a 2 x 2 between-subjects experiment to examine the effects of immersive tendency (high vs. low) and need to belong (high vs. low) on individuals' perceptions of a social robot. Preliminary data analyses revealed that participants with a higher level of immersive tendency and need to belong showed greater attachment and trust towards the robot, and were more satisfied with their relationship with the robot than participants with a lower level of immersive tendency and need to belong. In addition, participants with a higher level of immersive tendency experienced greater feelings of social presence. Implications of notable findings are discussed.</p></div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157759&CFID=105753441&CFTOKEN=50690582">Mechanical model of human lower arm</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Borut Povse, Darko Koritnik, Tadej Bajd, Marko Munih 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 209-210</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157759" title="DOI">10.1145/2157689.2157759</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157759&ftid=1161761&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow70" style="display:inline;"><br /><div style="display:inline">The paper describes the development of a passive mechanical lower arm (PMLA) intended for physical human-robot interaction studies. Our research is focused on cooperation of a small industrial robot and human operator where collision is expected only ...</div></span>
          <span id="toHide70" style="display:none;"><br /><div style="display:inline"><p>The paper describes the development of a passive mechanical lower arm (PMLA) intended for physical human-robot interaction studies. Our research is focused on cooperation of a small industrial robot and human operator where collision is expected only between the robot end-effector and the lower arm of the human worker. A mathematical model of the passive human lower arm was built and adopted for the control of the PMLA. The mathematical model was optimized using the data from the experiments performed with human volunteers and implemented into the control scheme. The experiments with human volunteers were performed with safely low contact forces. The emulation system of the human lower arm was thoroughly evaluated in the robot impact experiments while using plane and line robot end-effector tools. During the experiment the impact force and the impact energy density were measured and compared to the measurements of the investigation with human volunteers. The PMLA proved to be a good emulation system of the passive human lower arm.</p></div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157760&CFID=105753441&CFTOKEN=50690582">Shared gaze in remote spoken hri during distributed military operation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Zahar Prasov 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 211-212</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157760" title="DOI">10.1145/2157689.2157760</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157760&ftid=1161762&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow71" style="display:inline;"><br /><div style="display:inline">Collaboration between distributed human and robot partners during military operations is becoming more necessary. In order to enable efficient real-time communication, it is important to develop user interfaces that support robust spoken language understanding ...</div></span>
          <span id="toHide71" style="display:none;"><br /><div style="display:inline"><p>Collaboration between distributed human and robot partners during military operations is becoming more necessary. In order to enable efficient real-time communication, it is important to develop user interfaces that support robust spoken language understanding capabilities. As a step toward achieving this objective, this work examines the role of <i>shared gaze</i> between a human and robot during remote spoken collaboration engaged in a distributed military operations. Preliminary results have shown that an interface that supports shared gaze between a human and robot for a remote collaborative HRI search task has potential to improve automated language understanding as well as task efficiency.</p></div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157761&CFID=105753441&CFTOKEN=50690582">A social robot as an aloud reader: putting together recognition and synthesis of voice and gestures for HRI experimentation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81481651123&CFID=105753441&CFTOKEN=50690582">Arnaud Ramey</a>, 
                        <a href="author_page.cfm?id=81500648359&CFID=105753441&CFTOKEN=50690582">Javier F. Gorostiza</a>, 
                        <a href="author_page.cfm?id=81100060496&CFID=105753441&CFTOKEN=50690582">Miguel A. Salichs</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 213-214</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157761" title="DOI">10.1145/2157689.2157761</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157761&ftid=1161763&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow72" style="display:inline;"><br /><div style="display:inline">Advances in voice recognition have made possible applications in robotics controlled by voice only. However, user input through gestures and robot output gestures both create a more vivid interaction experience. In this article, we present an aloud reading ...</div></span>
          <span id="toHide72" style="display:none;"><br /><div style="display:inline"><p>Advances in voice recognition have made possible applications in robotics controlled by voice only. However, user input through gestures and robot output gestures both create a more vivid interaction experience. In this article, we present an aloud reading application offering all these interaction methods for the HRI-research robot Maggie. It gives us a testbed for user studies investigating the effect of these additional interaction methods.</p></div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157762&CFID=105753441&CFTOKEN=50690582">Modified social force model with face pose for human collision avoidance</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500664217&CFID=105753441&CFTOKEN=50690582">Photchara Ratsamee</a>, 
                        <a href="author_page.cfm?id=81440602123&CFID=105753441&CFTOKEN=50690582">Yasushi Mae</a>, 
                        <a href="author_page.cfm?id=81453639014&CFID=105753441&CFTOKEN=50690582">Kenichi Ohara</a>, 
                        <a href="author_page.cfm?id=81440620228&CFID=105753441&CFTOKEN=50690582">Tomohito Takubo</a>, 
                        <a href="author_page.cfm?id=81500658313&CFID=105753441&CFTOKEN=50690582">Tatsuo Arai</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 215-216</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157762" title="DOI">10.1145/2157689.2157762</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157762&ftid=1161764&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow73" style="display:inline;"><br /><div style="display:inline">In order for robots to be a part of human society, their social accceptance is an important issue if smooth interaction with humans is to be achieved. We propose a modified social force model that allows robots to move naturally like humans, based on ...</div></span>
          <span id="toHide73" style="display:none;"><br /><div style="display:inline"><p>In order for robots to be a part of human society, their social accceptance is an important issue if smooth interaction with humans is to be achieved. We propose a modified social force model that allows robots to move naturally like humans, based on estimated human motion and face pose. We add to the previous model the effect of the force due to face pose, in order to predict human motion and compute the robot motion itself. Our approach was implemented and tested on a real humanoid robot in a situation in which a human is confronted with a robot in an indoor environment. Experimental results illustrate that the robot is able to perform human-like navigation by avoiding the human in a face-to-face confrontation. Our system provides accurate face pose tracking that allows a robot to have a more realistic behaviour compared to the original social force model.</p></div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157763&CFID=105753441&CFTOKEN=50690582">The Roomba mood ring: an ambient-display robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500660347&CFID=105753441&CFTOKEN=50690582">Daniel J. Rea</a>, 
                        <a href="author_page.cfm?id=81500658474&CFID=105753441&CFTOKEN=50690582">James E. Young</a>, 
                        <a href="author_page.cfm?id=81100459517&CFID=105753441&CFTOKEN=50690582">Pourang Irani</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 217-218</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157763" title="DOI">10.1145/2157689.2157763</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157763&ftid=1161765&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow74" style="display:inline;"><br /><div style="display:inline">We present a robot augmented with an ambient display that communicates using a multi-color halo. We use this robot in a public caf&#233;-style setting where people vote on which colors the robot will display: we ask people to select a color which "best ...</div></span>
          <span id="toHide74" style="display:none;"><br /><div style="display:inline"><p>We present a robot augmented with an ambient display that communicates using a multi-color halo. We use this robot in a public caf&#233;-style setting where people vote on which colors the robot will display: we ask people to select a color which "best represents their mood". People can vote from a mobile device (e.g., smart phone or laptop) through a web interface. Thus, the robot's display is an abstract aggregate of the current mood of the room. Our research investigates how a robot with an ambient display may integrate into a space. For example, how will the robot alter how people use or perceive the environment, or how people will interact with the robot itself? In this paper we describe our initial prototype, an iRobot Roomba augmented with lights, and highlight the research questions driving our exploration, including initial study design.</p></div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157764&CFID=105753441&CFTOKEN=50690582">How to use non-linguistic utterances to convey emotion in child-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Robin Read, Tony Belpaeme 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 219-220</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157764" title="DOI">10.1145/2157689.2157764</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157764&ftid=1161766&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow75" style="display:inline;"><br /><div style="display:inline">Vocal affective displays are vital for achieving engaging and effective Human-Robot Interaction. The same can be said for linguistic interaction also, however, while emphasis may be placed upon linguistic interaction, there are also inherent risks: users ...</div></span>
          <span id="toHide75" style="display:none;"><br /><div style="display:inline"><p>Vocal affective displays are vital for achieving engaging and effective Human-Robot Interaction. The same can be said for linguistic interaction also, however, while emphasis may be placed upon linguistic interaction, there are also inherent risks: users are bound to a single language, and breakdowns are frequent due to current technical limitations. This work explores the potential of non-linguistic utterances. A recent study is briefly outlined in which school children were asked to rate a variety of non-linguistic utterances on an affective level using a facial gesture tool. Results suggest, for example, that utterance rhythm may be an influential independent factor, whilst the pitch contour of an utterance may have little importance. Also evidence for categorical perception of emotions is presented, an issue that may impact important areas of HRI away from vocal displays of affect.</p></div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157765&CFID=105753441&CFTOKEN=50690582">Ask, inform, or act: communication with a robotic patient before haptic action</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Timothy J. Martin, Allison P. Rzepczynski, Laurel D. Riek 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 221-222</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157765" title="DOI">10.1145/2157689.2157765</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157765&ftid=1161767&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow76" style="display:inline;"><br /><div style="display:inline">Currently in medical education, clinical students learn how to interact with real patients via simulated patients, which are inexpressive, teleoperated robot mannequins. We obtained five simulations that used such a robot to explore verbal communication ...</div></span>
          <span id="toHide76" style="display:none;"><br /><div style="display:inline"><p>Currently in medical education, clinical students learn how to interact with real patients via simulated patients, which are inexpressive, teleoperated robot mannequins. We obtained five simulations that used such a robot to explore verbal communication between clinical students and the robot patient, specifically if the students sought approval before performing haptic-actions. We found that in our sample, student clinicians frequently acted without seeking approval or providing information to the robot patient. We hope to further our studies in order to identify if either current training of clinical students in communication is ineffective, or if the robot patients are too nonhuman-like and inexpressive to engender appropriate communication.</p></div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157766&CFID=105753441&CFTOKEN=50690582">Creating human-robot rapport with mobile sculpture</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Tina Yue, Alexandra E. Janiw, Aaron Huus, Salvador Agui&#241;aga, Megan Archer, Krista Hoefle, Laurel D. Riek 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 223-224</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157766" title="DOI">10.1145/2157689.2157766</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157766&ftid=1161768&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow77" style="display:inline;"><br /><div style="display:inline">There is much discussion in the robotics community concerning the nature of people's impressions of robots. This pilot study employed the use of mobile robots coupled with artistic elements to create an environment conducive to human participation. PhotoBot ...</div></span>
          <span id="toHide77" style="display:none;"><br /><div style="display:inline"><p>There is much discussion in the robotics community concerning the nature of people's impressions of robots. This pilot study employed the use of mobile robots coupled with artistic elements to create an environment conducive to human participation. PhotoBot took photos of participants (n = 16) in a gallery space and provided them with a physical copy of their image, while ProjectorBot displayed 3D Kinect imagery for participants to view. Participants completed a self-report measure of rapport (Bernieri's Rapport Criterion); the results of which suggest that they experienced a high degree of positive interaction with the robots.</p></div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157767&CFID=105753441&CFTOKEN=50690582">Unsupervised clustering of people from 'skeleton' data</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Adrian Ball, David Rye, Fabio Ramos, Mari Velonaki 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 225-226</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157767" title="DOI">10.1145/2157689.2157767</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157767&ftid=1161769&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow78" style="display:inline;"><br /><div style="display:inline">This paper investigates the possibility of recognising individual persons from their walking gait using three-dimensional 'skeleton' data from an inexpensive consumer-level sensor, the Microsoft 'Kinect'. In an experimental pilot study it is shown that ...</div></span>
          <span id="toHide78" style="display:none;"><br /><div style="display:inline"><p>This paper investigates the possibility of recognising individual persons from their walking gait using three-dimensional 'skeleton' data from an inexpensive consumer-level sensor, the Microsoft 'Kinect'. In an experimental pilot study it is shown that the K-means algorithm - as a candidate unsupervised clustering algorithm - is able to cluster gait samples from four persons with a nett accuracy of 43.6%.</p></div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157768&CFID=105753441&CFTOKEN=50690582">Immersive human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Anara Sandygulova, Abraham G. Campbell, Mauro Dragone, G.M.P O'Hare 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 227-228</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157768" title="DOI">10.1145/2157689.2157768</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157768&ftid=1161770&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow79" style="display:inline;"><br /><div style="display:inline">Networked robotic applications enable robots to operate in distant, hazardous, or otherwise inaccessible environments, such as search and rescue, surveillance, and exploration applications. The most difficult challenge which persists for such systems ...</div></span>
          <span id="toHide79" style="display:none;"><br /><div style="display:inline"><p>Networked robotic applications enable robots to operate in distant, hazardous, or otherwise inaccessible environments, such as search and rescue, surveillance, and exploration applications.</p> <p>The most difficult challenge which persists for such systems is that of supporting effective human-robot interaction, as this usually demands managing dynamic views, changeable interaction modalities, and adaptive levels of robotic autonomy.</p> <p>In contrast of sophisticated screen-based graphical user interfaces (GUIs), the solution proposed herein is to enable more natural human-robot interaction modalities through a networked immersive user interface. This paper describes the creation of one such shared space where to test such an approach, with both simulated and real robots.</p></div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157769&CFID=105753441&CFTOKEN=50690582">Don't stand so close to me: users' attitudinal and behavioral responses to personal space invasion by robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500663814&CFID=105753441&CFTOKEN=50690582">Aziez Sardar</a>, 
                        <a href="author_page.cfm?id=81500665327&CFID=105753441&CFTOKEN=50690582">Michiel Joosse</a>, 
                        <a href="author_page.cfm?id=81500660127&CFID=105753441&CFTOKEN=50690582">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81500647210&CFID=105753441&CFTOKEN=50690582">Vanessa Evers</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 229-230</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157769" title="DOI">10.1145/2157689.2157769</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157769&ftid=1161771&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow80" style="display:inline;"><br /><div style="display:inline">When in a human environment, one might expect that a social robot would act according to the social norms people expect of each other. When someone does not adhere to a prevalent social norm, people usually feel threatened and disturbed. Thus, insight ...</div></span>
          <span id="toHide80" style="display:none;"><br /><div style="display:inline"><p>When in a human environment, one might expect that a social robot would act according to the social norms people expect of each other. When someone does not adhere to a prevalent social norm, people usually feel threatened and disturbed. Thus, insight is needed into what is perceived as socially normative behavior for robots. We conducted an experiment in which an agent approached a participant in order to determine the effect of personal space invasion. We manipulated the agent-type (human/robot) and the approach speed (slow/fast) of the agent towards the participant. Unexpectedly, our results show that the participants displayed more compensatory behavior in the robot condition than in the human condition. We consider this response toward personal space invasion as indication that people react in a similar way to robots as they do to humans, however with more intensity.</p></div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157770&CFID=105753441&CFTOKEN=50690582">Coupled inverse-forward models for action execution leading to tool-use in a humanoid robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Guido Schillaci, Verena Vanessa Hafner, Bruno Lara 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 231-232</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157770" title="DOI">10.1145/2157689.2157770</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157770&ftid=1161772&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow81" style="display:inline;"><br /><div style="display:inline">We propose a computational model based on inverse-forward model pairs for the simulation and execution of actions. The models are implemented on a humanoid robot and are used to control reaching actions with the arms. In the experimental setup a tool ...</div></span>
          <span id="toHide81" style="display:none;"><br /><div style="display:inline"><p>We propose a computational model based on inverse-forward model pairs for the simulation and execution of actions. The models are implemented on a humanoid robot and are used to control reaching actions with the arms. In the experimental setup a tool has been attached to the left arm of the robot extending its covered action space. The preliminary investigations carried out aim at studying how the use of tools modifies the body scheme of the robot. The system performs action simulations before the actual executions. For each of the arms, predicted end-effector positions are compared with the desired one and the internal pair presenting the lowest error is selected for action execution. This allows the robot to decide on performing an action either with its hand alone or with the one with the attached tool.</p></div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157771&CFID=105753441&CFTOKEN=50690582">Developing guidelines for in-the-field control of a team of robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500659573&CFID=105753441&CFTOKEN=50690582">Megha Sharma</a>, 
                        <a href="author_page.cfm?id=81500658473&CFID=105753441&CFTOKEN=50690582">James E. Young</a>, 
                        <a href="author_page.cfm?id=81100567948&CFID=105753441&CFTOKEN=50690582">Rasit Eskicioglu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 233-234</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157771" title="DOI">10.1145/2157689.2157771</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157771&ftid=1161773&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow82" style="display:inline;"><br /><div style="display:inline">In this work we explore the development of guidelines for creating "in-the-field" interfaces for enabling a single user to remotely control multiple robots. The problem of controlling a remote team of robots is complex, requiring a user to monitor and ...</div></span>
          <span id="toHide82" style="display:none;"><br /><div style="display:inline"><p>In this work we explore the development of guidelines for creating "in-the-field" interfaces for enabling a single user to remotely control multiple robots. The problem of controlling a remote team of robots is complex, requiring a user to monitor and interpret robotic state and sensor information in real time, and to simultaneously communicate direction commands to the robots. The result is that a robot controller is often seated at a console; for many relevant applications such as search and rescue or firefighting this removes the user from the field of action, rendering them unable to directly participate in a task at hand.</p> <p>Therefore, one challenge in HRI is to develop efficient interfaces that will enable a user to effectively control and monitor a team of robots in the field. In our project we explore various interface designs in terms of supporting this goal, taking the approach of involving a panel of professionals in the design process to direct exploration and development.</p></div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157772&CFID=105753441&CFTOKEN=50690582">Is the social robot probo an added value for social story intervention for children with autism spectrum disorders?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Ramona Simut, Cristina Pop, Jelle Saldien, Alina Rusu, Sebastian Pintea, Johan Vanderfaeillie, Daniel David, Bram Vanderborght 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 235-236</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157772" title="DOI">10.1145/2157689.2157772</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157772&ftid=1161774&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow83" style="display:inline;"><br /><div style="display:inline">In this paper, we describe the first results of using the robot Probo as a facilitator in Social Story Intervention for children with autism spectrum disorders (ASD). Four preschoolers diagnosed with ASD participated in this research. For each of them, ...</div></span>
          <span id="toHide83" style="display:none;"><br /><div style="display:inline"><p>In this paper, we describe the first results of using the robot Probo as a facilitator in Social Story Intervention for children with autism spectrum disorders (ASD). Four preschoolers diagnosed with ASD participated in this research. For each of them, a specific social skill deficit was identified, like sharing toys, saying Thank you, saying Hello, and an individualized Social Story was developed. The stories were told by both the therapist and the robot in different intervention phases. Afterwards an experimental task was created where the child needed to exercise the ability targeted by the story. The results of this study showed that the participant needed a decreased level of prompt to perform the targeted behavior, when the story was told by the robot compared to the intervention with the human storyteller. Therefore, this preliminary study created great expectancies about the potential of Robot Assisted Therapy as an added value for ASD interventions.</p></div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157773&CFID=105753441&CFTOKEN=50690582">Animal-inspired human-robot interaction: a robotic tail for communicating state</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500651556&CFID=105753441&CFTOKEN=50690582">Ashish Singh</a>, 
                        <a href="author_page.cfm?id=81500658472&CFID=105753441&CFTOKEN=50690582">James E. Young</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 237-238</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157773" title="DOI">10.1145/2157689.2157773</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157773&ftid=1161775&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow84" style="display:inline;"><br /><div style="display:inline">We present a robotic tail interface for enabling a robot to communicate its state to people. Our interface design follows an animal-inspired methodology where we map the robot's state to its tail output, leveraging people's existing knowledge of and ...</div></span>
          <span id="toHide84" style="display:none;"><br /><div style="display:inline"><p>We present a robotic tail interface for enabling a robot to communicate its state to people. Our interface design follows an animal-inspired methodology where we map the robot's state to its tail output, leveraging people's existing knowledge of and experiences with animals for human-robot interaction. In this paper we detail our robotic-tail design approach and our prototype implementations, and outline our future steps.</p></div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157774&CFID=105753441&CFTOKEN=50690582">Spatial language experiments for a robot fetch task</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Marjorie Skubic, Laura Carlson, Jared Miller, Xiao Ou Li, Zhiyu Huo 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 239-240</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157774" title="DOI">10.1145/2157689.2157774</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157774&ftid=1161776&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow85" style="display:inline;"><br /><div style="display:inline">This paper outlines a new study that investigates spatial language for use in human-robot communication. The scenario studied is a home setting in which the elderly resident has misplaced an object, such as eyeglasses, and the robot will help the resident ...</div></span>
          <span id="toHide85" style="display:none;"><br /><div style="display:inline"><p>This paper outlines a new study that investigates spatial language for use in human-robot communication. The scenario studied is a home setting in which the elderly resident has misplaced an object, such as eyeglasses, and the robot will help the resident find the object. We present results from phase I of the study in which we investigate spatial language generated to a human addressee or a robot addressee in a virtual environment.</p></div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157775&CFID=105753441&CFTOKEN=50690582">Potential measures for detecting trust changes</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500662910&CFID=105753441&CFTOKEN=50690582">Poornima Kaniarasu</a>, 
                        <a href="author_page.cfm?id=81500653411&CFID=105753441&CFTOKEN=50690582">Aaron Steinfeld</a>, 
                        <a href="author_page.cfm?id=81500645684&CFID=105753441&CFTOKEN=50690582">Munjal Desai</a>, 
                        <a href="author_page.cfm?id=81100443257&CFID=105753441&CFTOKEN=50690582">Holly Yanco</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 241-242</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157775" title="DOI">10.1145/2157689.2157775</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157775&ftid=1161777&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow86" style="display:inline;"><br /><div style="display:inline">It is challenging to quantitatively measure a user's trust in a robot system using traditional survey methods due to their invasiveness and tendency to disrupt the flow of operation. Therefore, we analyzed data from an existing experiment to identify ...</div></span>
          <span id="toHide86" style="display:none;"><br /><div style="display:inline"><p>It is challenging to quantitatively measure a user's trust in a robot system using traditional survey methods due to their invasiveness and tendency to disrupt the flow of operation. Therefore, we analyzed data from an existing experiment to identify measures which (1) have face validity for measuring trust and (2) align with the collected post-run trust measures. Two measures are promising as real-time indications of a drop in trust. The first is the time between the most recent warning and when the participant reduces the robot's autonomy level. The second is the number of warnings prior to the reduction of the autonomy level.</p></div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157776&CFID=105753441&CFTOKEN=50690582">Affect misattribution procedure: an implicit technique to measure user experience in hri</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500664529&CFID=105753441&CFTOKEN=50690582">Ewald Strasser</a>, 
                        <a href="author_page.cfm?id=81381602151&CFID=105753441&CFTOKEN=50690582">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81500647815&CFID=105753441&CFTOKEN=50690582">Manfred Tscheligi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 243-244</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157776" title="DOI">10.1145/2157689.2157776</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157776&ftid=1161778&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow87" style="display:inline;"><br /><div style="display:inline">This paper suggests new methodology for measuring User Experience in HRI. We suggest using implicit attitude to predict User Experience (Affect). Therefore we show a first validation study. The study uses short videos of a robot (IURO - Interactive Urban ...</div></span>
          <span id="toHide87" style="display:none;"><br /><div style="display:inline"><p>This paper suggests new methodology for measuring User Experience in HRI. We suggest using implicit attitude to predict User Experience (Affect). Therefore we show a first validation study. The study uses short videos of a robot (IURO - Interactive Urban Robot) approaching a person and asking for the. IURO either approached a walking or a standing person. We measured people's implicit attitude towards the robot with the Affect Misattribution Procedure (AMP). The results show that a walking person being approached by the robot evolves an implicitly more negative attitude in the observing participant whereas corresponding questionnaire items showed no difference in attitude for the approach behaviour. We conclude from these results that measuring implicit attitude in HRI is valuable for the evaluation of the User Experience of a robot.</p></div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157777&CFID=105753441&CFTOKEN=50690582">Policy transformation for learning from demonstration</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Halit Bener Suay, Sonia Chernova 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 245-246</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157777" title="DOI">10.1145/2157689.2157777</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157777&ftid=1161779&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow88" style="display:inline;"><br /><div style="display:inline">Many different robot learning from demonstration methods have been applied and tested in various environments recently. Representation of learned plans, tasks and policies often depends on the technique due to method-specific parameters. An agent that ...</div></span>
          <span id="toHide88" style="display:none;"><br /><div style="display:inline"><p>Many different robot learning from demonstration methods have been applied and tested in various environments recently. Representation of learned plans, tasks and policies often depends on the technique due to method-specific parameters. An agent that is able to switch between representations can apply its knowledge to different algorithms. This flexibility can be useful for a human teacher when training the agent. In this work we present a process to convert learned policies with two specific methods, Confidence-Based Autonomy (CBA) and Interactive Reinforcement Learning (Int-RL), to each other. Our finding suggests that it is possible for an agent to learn a policy with either CBA or Int-RL method and execute the task with the other with the benefit of previously learned knowledge.</p></div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157778&CFID=105753441&CFTOKEN=50690582">Exploration of intention expression for robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Ivan Shindev, Yu Sun, Michael Coovert, Jenny Pavlova, Tiffany Lee 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 247-248</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157778" title="DOI">10.1145/2157689.2157778</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157778&ftid=1161780&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow89" style="display:inline;"><br /><div style="display:inline">This paper presents a novel exploration on how to enable a robot to express its intention so that the humans and robot can form a synergic relationship. A systematic design approach is proposed to obtain a set of possible intentions for a given robot ...</div></span>
          <span id="toHide89" style="display:none;"><br /><div style="display:inline"><p>This paper presents a novel exploration on how to enable a robot to express its intention so that the humans and robot can form a synergic relationship. A systematic design approach is proposed to obtain a set of possible intentions for a given robot from three levels of intentions. A visual intention expression system approach is developed to visualize the intentions and implemented on a mobile robot and a manipulator to demonstrate the intention expression concept.</p></div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157779&CFID=105753441&CFTOKEN=50690582">Dimensions of people's attitudes toward robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Daisuke Suzuki, Hiroyuki Umemuro 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 249-250</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157779" title="DOI">10.1145/2157689.2157779</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157779&ftid=1161781&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow90" style="display:inline;"><br /><div style="display:inline">The purpose of this study was to investigate dimensions that construct people's attitudes toward robots. In the first phase, investigations using free description and group interview were conducted to extract potential elements for attitudes toward robots. ...</div></span>
          <span id="toHide90" style="display:none;"><br /><div style="display:inline"><p>The purpose of this study was to investigate dimensions that construct people's attitudes toward robots. In the first phase, investigations using free description and group interview were conducted to extract potential elements for attitudes toward robots. In the second phase, a questionnaire battery was developed based on the elements extracted, and a survey investigation was conducted with the questionnaire. A factor analysis was conducted on the responses to the questionnaire, and nine factors were extracted as dimensions of people's attitudes toward robots.</p></div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157780&CFID=105753441&CFTOKEN=50690582">Extending chatterbot system into multimodal interaction framework with embodied contextual understanding</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jeffrey Too Chuan Tan, Tetsunari Inamura 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 251-252</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157780" title="DOI">10.1145/2157689.2157780</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157780&ftid=1161782&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow91" style="display:inline;"><br /><div style="display:inline">This work aims to realize multimodal interaction with embodied contextual understanding based on the simple chatterbot system. A system framework is proposed to integrate the dialogue system into a 3D simulation platform, SIGVerse to attain multimodal ...</div></span>
          <span id="toHide91" style="display:none;"><br /><div style="display:inline"><p>This work aims to realize multimodal interaction with embodied contextual understanding based on the simple chatterbot system. A system framework is proposed to integrate the dialogue system into a 3D simulation platform, SIGVerse to attain multimodal interaction. The chatterbot AIML implementations are described in the achievement of the conversations with embodied contextual understanding in HRI simulations.</p></div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157781&CFID=105753441&CFTOKEN=50690582">Learning verbs by teaching a care-receiving robot by children: an experimental report</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Fumihide Tanaka, Shizuko Matsuzoe 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 253-254</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157781" title="DOI">10.1145/2157689.2157781</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157781&ftid=1161783&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow92" style="display:inline;"><br /><div style="display:inline">We investigate the use of care-receiving robot (CRR) for the purpose of supporting childhood education. In contrast to the conventional teaching agents that are designed to play the role of human teachers or caregivers, the robot here receives cares ...</div></span>
          <span id="toHide92" style="display:none;"><br /><div style="display:inline"><p>We investigate the use of care-receiving robot (CRR) for the purpose of supporting childhood education. In contrast to the conventional teaching agents that are designed to play the role of human teachers or caregivers, the robot here receives cares from children. We hypothesize that by using this CRR, we may construct a new educational framework whose goal is to promote children's spontaneous <i>learning by teaching</i> through teaching the CRR. The paper describes an experiment for investigating whether a CRR can promote children's learning English verbs through teaching it.</p></div></span> <a id="expcoll92" href="JavaScript: expandcollapse('expcoll92',92)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157782&CFID=105753441&CFTOKEN=50690582">A tricycle-style teleoperational interface that remotely controls a robot for classroom children</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Fumihide Tanaka, Toshimitsu Takahashi 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 255-256</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157782" title="DOI">10.1145/2157689.2157782</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157782&ftid=1161784&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow93" style="display:inline;"><br /><div style="display:inline">We consider the application of telepresence robots for supporting childhood education. One challenge here is to develop a teleoperational robot system that can be manipulated by children themselves. There are two requirements for realizing such a system. ...</div></span>
          <span id="toHide93" style="display:none;"><br /><div style="display:inline"><p>We consider the application of telepresence robots for supporting childhood education. One challenge here is to develop a teleoperational robot system that can be manipulated by children themselves. There are two requirements for realizing such a system. First, the system has to be sufficiently intuitive so that child users can control it without the need for detailed instructions. Second, the control of the system should have some amount of enjoyment so that child users do not get bored. To satisfy these requirements, we introduce a tricycle-style teleoperational interface that remotely controls a robot. We also report field tests that are currently being conducted at English learning schools for children in Japan.</p></div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157783&CFID=105753441&CFTOKEN=50690582">Prosody-driven robot ARM gestures generation in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Amir Aly, Adriana Tapus 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 257-258</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157783" title="DOI">10.1145/2157689.2157783</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157783&ftid=1161785&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow94" style="display:inline;"><br /><div style="display:inline">In multimodal human-robot interaction(HRI), the process of communication can be established through verbal, non-verbal, and/or para-verbal cues. The linguistic literature [3] shows that para-verbal and non-verbal communications are naturally synchronized. ...</div></span>
          <span id="toHide94" style="display:none;"><br /><div style="display:inline"><p>In multimodal human-robot interaction(HRI), the process of communication can be established through verbal, non-verbal, and/or para-verbal cues. The linguistic literature [3] shows that para-verbal and non-verbal communications are naturally synchronized. This research focuses on the relation between non-verbal and para-verbal communication by mapping prosody cues to the corresponding arm gestures. Our approach for synthesizing arm gestures uses the coupled hidden Markov models (CHMMs), which could be seen as a collection of HMMs modeling the segmented prosodic characteristics' stream and the segmented rotation characteristics' streams of the two arms' articulations [4][1]. Nao robot was used for tests.</p></div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157784&CFID=105753441&CFTOKEN=50690582">A practical comparison of three robot learning from demonstration algorithms</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Russell Toris, Halit Bener Suay, Sonia Chernova 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 261-262</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157784" title="DOI">10.1145/2157689.2157784</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157784&ftid=1161786&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow95" style="display:inline;"><br /><div style="display:inline">Research on robot learning from demonstration has seen significant growth in recent years, but existing evaluations have focused exclusively on algorithmic performance and not on usability factors, especially with respect to na&#239;ve users. Here we ...</div></span>
          <span id="toHide95" style="display:none;"><br /><div style="display:inline"><p>Research on robot learning from demonstration has seen significant growth in recent years, but existing evaluations have focused exclusively on algorithmic performance and not on usability factors, especially with respect to na&#239;ve users. Here we present findings from a comparative user study in which we asked non-experts to evaluate three distinctively different robot learning from demonstration algorithms - Behavior Networks, Interactive Reinforcement Learning, and Confidence Based Autonomy. Participants in the study showed a preference for interfaces where they controlled the robot directly (teleoperation and guidance) instead of providing retroactive feedback for past actions (reward and correction). Our results show that the best policy performance in most metrics was achieved using the Confidence Based Autonomy algorithm.</p></div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157785&CFID=105753441&CFTOKEN=50690582">An assistive robot contest: designs and interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Igor M. Verner, David J. Ahlgren 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 263-264</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157785" title="DOI">10.1145/2157689.2157785</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157785&ftid=1161787&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow96" style="display:inline;"><br /><div style="display:inline">This paper considers engineering and educational challenges of the assistive robot contest RoboWaiter and interactions experienced by participants, both students and people with disabilities.</div></span>
          <span id="toHide96" style="display:none;"><br /><div style="display:inline"><p>This paper considers engineering and educational challenges of the assistive robot contest RoboWaiter and interactions experienced by participants, both students and people with disabilities.</p></div></span> <a id="expcoll96" href="JavaScript: expandcollapse('expcoll96',96)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157786&CFID=105753441&CFTOKEN=50690582">Interaction with animated robots in science museum programs: how children learn?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Alex Polishuk, Igor Michael Verner 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 265-266</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157786" title="DOI">10.1145/2157689.2157786</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157786&ftid=1161788&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow97" style="display:inline;"><br /><div style="display:inline">This paper examines learning through student interaction with animated robots, as implemented in robot theatre performances and workshops in the science museum.</div></span>
          <span id="toHide97" style="display:none;"><br /><div style="display:inline"><p>This paper examines learning through student interaction with animated robots, as implemented in robot theatre performances and workshops in the science museum.</p></div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157787&CFID=105753441&CFTOKEN=50690582">A survey on robot appearances</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81467666180&CFID=105753441&CFTOKEN=50690582">Astrid Marieke von der P&#252;tten</a>, 
                        <a href="author_page.cfm?id=81332510009&CFID=105753441&CFTOKEN=50690582">Nicole C. Kr&#228;mer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 267-268</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157787" title="DOI">10.1145/2157689.2157787</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157787&ftid=1161789&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow98" style="display:inline;"><br /><div style="display:inline">Against the background of the uncanny valley hypothesis [1] and its conceptual shortcomings this study aims at identifying design characteristics which determine the evaluation of robots. We conducted a web-based survey with standardized pictures of ...</div></span>
          <span id="toHide98" style="display:none;"><br /><div style="display:inline"><p>Against the background of the uncanny valley hypothesis [1] and its conceptual shortcomings this study aims at identifying design characteristics which determine the evaluation of robots. We conducted a web-based survey with standardized pictures of 40 robots which were evaluated by 151 participants. A cluster analysis revealed six clusters of robots. The results are discussed with regard to implications for the uncanny valley hypothesis.</p></div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157788&CFID=105753441&CFTOKEN=50690582">Tele-operated robot control using attitude aware smartphones</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Amber M. Walker, David P. Miller 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 269-270</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157788" title="DOI">10.1145/2157689.2157788</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157788&ftid=1161790&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow99" style="display:inline;"><br /><div style="display:inline">Smartphones have put video communications, computation, and proprioceptive sensing (e.g. accelerometers and gyros) into the hands of hundreds of millions of consumers. These small, microelectromechanical systems can be used in many applications, including ...</div></span>
          <span id="toHide99" style="display:none;"><br /><div style="display:inline"><p>Smartphones have put video communications, computation, and proprioceptive sensing (e.g. accelerometers and gyros) into the hands of hundreds of millions of consumers. These small, microelectromechanical systems can be used in many applications, including remote control. This study proposes using smartphones with proprioception as handheld robot controllers and aims to determine feasibility of accelerometers as control inputs for tele-operation while defining heuristics for use. Initial results indicate accelerometers are suitable for tele-operation commands, but identify specific design characteristics meriting further investigation.</p></div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157789&CFID=105753441&CFTOKEN=50690582">HRI research: the interdisciplinary challenge or the dawning of the discipline?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Astrid Weiss 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 271-272</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157789" title="DOI">10.1145/2157689.2157789</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157789&ftid=1161845&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow100" style="display:inline;"><br /><div style="display:inline">The Human-Robot Interaction (HRI) research field has developed more and more over the past 10 to 20 years. It is still a relatively young community, which is in the process of developing its characteristics, such as being interdisciplinary, innovative, ...</div></span>
          <span id="toHide100" style="display:none;"><br /><div style="display:inline"><p>The Human-Robot Interaction (HRI) research field has developed more and more over the past 10 to 20 years. It is still a relatively young community, which is in the process of developing its characteristics, such as being interdisciplinary, innovative, responsible, technical, and many others. Similarly, like in the development of the Human-Computer Interaction (HCI) community, being interdisciplinary is essential, but if we have a look on the current situation, HCI became more of an autonomous discipline nowadays. Where is the HRI community heading to in this respect? This paper should reflect in accordance to the "epistemic living spaces" concept on some stereotypical statements by researchers working in HRI. The reflection shows that in three phases of a researcher's career (orientation, positioning, and stabilizing and expanding) show a tendency towards the discipline and away from interdisciplinary work and that the forth phase (attachment) needs to be strengthened, independently of disciplinary or interdisciplinary approaches.</p></div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157790&CFID=105753441&CFTOKEN=50690582">Immersion with robots in large virtual environments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81466646883&CFID=105753441&CFTOKEN=50690582">Xianshi Xie</a>, 
                        <a href="author_page.cfm?id=81466643767&CFID=105753441&CFTOKEN=50690582">Qiufeng Lin</a>, 
                        <a href="author_page.cfm?id=81335499510&CFID=105753441&CFTOKEN=50690582">Haojie Wu</a>, 
                        <a href="author_page.cfm?id=81500644108&CFID=105753441&CFTOKEN=50690582">Julie A. Adams</a>, 
                        <a href="author_page.cfm?id=81100500707&CFID=105753441&CFTOKEN=50690582">Bobby E. Bodenheimer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 273-274</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157790" title="DOI">10.1145/2157689.2157790</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157790&ftid=1161791&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow101" style="display:inline;"><br /><div style="display:inline">This paper presents a mixed reality system for combining real robots, humans, and virtual robots. The system tracks and controls physical robots in local physical space, and inserts them into a virtual environment (VE). The system allows a human to locomote ...</div></span>
          <span id="toHide101" style="display:none;"><br /><div style="display:inline"><p>This paper presents a mixed reality system for combining real robots, humans, and virtual robots. The system tracks and controls physical robots in local physical space, and inserts them into a virtual environment (VE). The system allows a human to locomote in a VE larger than the physically tracked space of the laboratory through a form of redirected walking. An evaluation assessed the conditions under which subjects found the system to be the most immersive.</p></div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157791&CFID=105753441&CFTOKEN=50690582">Effect of scenario media on human-robot interaction evaluation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Qianli Xu, Jamie Suat Ling Ng, Yian Ling Cheong, Odelia Yiling Tan, Ji Bin Wong, Benedict Tiong Chee Tay, Taezoon Park 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 275-276</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157791" title="DOI">10.1145/2157689.2157791</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157791&ftid=1161792&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow102" style="display:inline;"><br /><div style="display:inline">Different media used to present the human-robot interaction (HRI) scenarios may affect users' perception of a robot in the user studies. We investigated how different scenario media (text, video, and live interaction) might influence user evaluation ...</div></span>
          <span id="toHide102" style="display:none;"><br /><div style="display:inline"><p>Different media used to present the human-robot interaction (HRI) scenarios may affect users' perception of a robot in the user studies. We investigated how different scenario media (text, video, and live interaction) might influence user evaluation of social robots based on a controlled experiment. We found that multiple aspects of user acceptance were influenced by the scenario media. Moreover, more design problems and redesign proposals were elicited when users were exposed to media with higher fidelity. The results led to useful insights into choosing scenario media in HRI evaluation.</p></div></span> <a id="expcoll102" href="JavaScript: expandcollapse('expcoll102',102)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157792&CFID=105753441&CFTOKEN=50690582">Development of a Jenga game manipulator having multi-articulated fingers</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100409152&CFID=105753441&CFTOKEN=50690582">Tsuneo Yoshikawa</a>, 
                        <a href="author_page.cfm?id=81500652777&CFID=105753441&CFTOKEN=50690582">Tatsuya Sugiura</a>, 
                        <a href="author_page.cfm?id=81453636470&CFID=105753441&CFTOKEN=50690582">Seiji Sugiyama</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 277-278</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157792" title="DOI">10.1145/2157689.2157792</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157792&ftid=1161793&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow103" style="display:inline;"><br /><div style="display:inline">This paper describes current status of our effort to develop a robot system that can play Jenga game against human players. Unlike most of the previous Jenga robots that use grippers, this robot is equipped with a hand with two multiarticulated fingers ...</div></span>
          <span id="toHide103" style="display:none;"><br /><div style="display:inline"><p>This paper describes current status of our effort to develop a robot system that can play Jenga game against human players. Unlike most of the previous Jenga robots that use grippers, this robot is equipped with a hand with two multiarticulated fingers covered by soft skin and does not have any major mechanical constraint in playing the game in a natural way as human players do. An experimental result of a game played between the robot and a human player is presented.</p></div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157793&CFID=105753441&CFTOKEN=50690582">Robot gesture and user acceptance of information in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500649033&CFID=105753441&CFTOKEN=50690582">Aelee Kim</a>, 
                        <a href="author_page.cfm?id=81500658982&CFID=105753441&CFTOKEN=50690582">Hyejin Kum</a>, 
                        <a href="author_page.cfm?id=81500663543&CFID=105753441&CFTOKEN=50690582">Ounjeong Roh</a>, 
                        <a href="author_page.cfm?id=81482656476&CFID=105753441&CFTOKEN=50690582">Sangseok You</a>, 
                        <a href="author_page.cfm?id=81321494092&CFID=105753441&CFTOKEN=50690582">Sukhan Lee</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 279-280</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157793" title="DOI">10.1145/2157689.2157793</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157793&ftid=1161846&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow104" style="display:inline;"><br /><div style="display:inline">This study explores how human users respond to coordinated and uncoordinated gestures of a robot as an information deliverer. A between-subject experiment was conducted using the Wizard of Oz method, with 63 participants randomly assigned to one of four ...</div></span>
          <span id="toHide104" style="display:none;"><br /><div style="display:inline"><p>This study explores how human users respond to coordinated and uncoordinated gestures of a robot as an information deliverer. A between-subject experiment was conducted using the Wizard of Oz method, with 63 participants randomly assigned to one of four conditions (voice-only vs. no-gesture vs. coordinated gesture vs. uncoordinated gesture) taking an artwork class in a museum-like setting. The robot was explaining the information of the artworks with modalities accordingly designed to each condition. Results showed that the coordinated gesture was not aiding information delivery. However, there were notable relations between the coordinated gesture and intimacy, homogeneity, and involvement. These results have theoretical implications for cognitive load of working memory and practical implications for designing and deploying dynamic humanoid robots for museum tour guide.</p></div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157794&CFID=105753441&CFTOKEN=50690582">Establishment of spatial formation by a mobile guide robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500658543&CFID=105753441&CFTOKEN=50690582">Mohammad Abu Yousuf</a>, 
                        <a href="author_page.cfm?id=81500650244&CFID=105753441&CFTOKEN=50690582">Yoshinori Kobayashi</a>, 
                        <a href="author_page.cfm?id=81500659012&CFID=105753441&CFTOKEN=50690582">Yoshinori Kuno</a>, 
                        <a href="author_page.cfm?id=81350572857&CFID=105753441&CFTOKEN=50690582">Keiichi Yamazaki</a>, 
                        <a href="author_page.cfm?id=81500660389&CFID=105753441&CFTOKEN=50690582">Akiko Yamazaki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 281-282</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157794" title="DOI">10.1145/2157689.2157794</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157794&ftid=1161794&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow105" style="display:inline;"><br /><div style="display:inline">A mobile museum guide robot is expected to establish a proper spatial formation with the visitors. After observing the videotaped scenes of human guide-visitors interaction at actual museum galleries, we have developed a mobile robot that can guide multiple ...</div></span>
          <span id="toHide105" style="display:none;"><br /><div style="display:inline"><p>A mobile museum guide robot is expected to establish a proper spatial formation with the visitors. After observing the videotaped scenes of human guide-visitors interaction at actual museum galleries, we have developed a mobile robot that can guide multiple visitors inside the gallery from one exhibit to another. The mobile guide robot is capable of establishing spatial formation known as "F-formation" at the beginning of explanation. It can also use a systematic procedure known as "pause and restart" depending on the situation through which a framework of mutual orientation between the speaker (robot) and visitors is achieved. The effectiveness of our method has been confirmed through experiments.</p></div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157795&CFID=105753441&CFTOKEN=50690582">Navigating in public space: participants' evaluation of a robot's approach behavior</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81488671534&CFID=105753441&CFTOKEN=50690582">Jakub A. Z&#322;otowski</a>, 
                        <a href="author_page.cfm?id=81500660125&CFID=105753441&CFTOKEN=50690582">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81500647814&CFID=105753441&CFTOKEN=50690582">Manfred Tscheligi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 283-284</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157795" title="DOI">10.1145/2157689.2157795</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157795&ftid=1161795&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow106" style="display:inline;"><br /><div style="display:inline">The results from an empirical study on the impact of a robot's approach trajectories on its social acceptance are presented. An online survey presenting short videos of a robot (IURO - Interactive Urban RObot) approaching a person in a public space and ...</div></span>
          <span id="toHide106" style="display:none;"><br /><div style="display:inline"><p>The results from an empirical study on the impact of a robot's approach trajectories on its social acceptance are presented. An online survey presenting short videos of a robot (IURO - Interactive Urban RObot) approaching a person in a public space and asking for help was shown to the users. IURO either approached a walking or standing person. The results show that walking participants preferred to be approached from the front left or front right direction rather than frontally. However, when they are standing all three approach directions were acceptable.</p></div></span> <a id="expcoll106" href="JavaScript: expandcollapse('expcoll106',106)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Conversation and proxemics</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Kerstin Fischer 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157797&CFID=105753441&CFTOKEN=50690582">Generation of nodding, head tilting and eye gazing for human-robot dialogue interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456617871&CFID=105753441&CFTOKEN=50690582">Chaoran Liu</a>, 
                        <a href="author_page.cfm?id=81350582479&CFID=105753441&CFTOKEN=50690582">Carlos T. Ishi</a>, 
                        <a href="author_page.cfm?id=81500656858&CFID=105753441&CFTOKEN=50690582">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81500647100&CFID=105753441&CFTOKEN=50690582">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 285-292</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157797" title="DOI">10.1145/2157689.2157797</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157797&ftid=1161847&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=2157797&ftid=1161796&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow108" style="display:inline;"><br /><div style="display:inline">Head motion occurs naturally and in synchrony with speech during human dialogue communication, and may carry paralinguistic information, such as intentions, attitudes and emotions. Therefore, natural-looking head motion by a robot is important for smooth ...</div></span>
          <span id="toHide108" style="display:none;"><br /><div style="display:inline"><p>Head motion occurs naturally and in synchrony with speech during human dialogue communication, and may carry paralinguistic information, such as intentions, attitudes and emotions. Therefore, natural-looking head motion by a robot is important for smooth human-robot interaction. Based on rules inferred from analyses of the relationship between head motion and dialogue acts, this paper proposes a model for generating head tilting and nodding, and evaluates the model using three types of humanoid robot (a very human-like android, "Geminoid F", a typical humanoid robot with less facial degrees of freedom, "Robovie R2", and a robot with a 3-axis rotatable neck and movable lips, "Telenoid R2"). Analysis of subjective scores shows that the proposed model including head tilting and nodding can generate head motion with increased naturalness compared to nodding only or directly mapping people's original motions without gaze information. We also find that an upwards motion of a robot's face can be used by robots which do not have a mouth in order to provide the appearance that utterance is taking place. Finally, we conduct an experiment in which participants act as visitors to an information desk attended by robots. As a consequence, we verify that our generation model performs equally to directly mapping people's original motions with gaze information in terms of perceived naturalness.</p></div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157798&CFID=105753441&CFTOKEN=50690582">Designing persuasive robots: how robots might persuade people using vocal and nonverbal cues</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500652520&CFID=105753441&CFTOKEN=50690582">Vijay Chidambaram</a>, 
                        <a href="author_page.cfm?id=81500648076&CFID=105753441&CFTOKEN=50690582">Yueh-Hsuan Chiang</a>, 
                        <a href="author_page.cfm?id=81500660251&CFID=105753441&CFTOKEN=50690582">Bilge Mutlu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 293-300</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157798" title="DOI">10.1145/2157689.2157798</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157798&ftid=1161797&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow109" style="display:inline;"><br /><div style="display:inline">Social robots have to potential to serve as personal, organizational, and public assistants as, for instance, diet coaches, teacher's aides, and emergency respondents. The success of these robots - whether in motivating users to adhere to a diet regimen ...</div></span>
          <span id="toHide109" style="display:none;"><br /><div style="display:inline"><p>Social robots have to potential to serve as personal, organizational, and public assistants as, for instance, diet coaches, teacher's aides, and emergency respondents. The success of these robots - whether in motivating users to adhere to a diet regimen or in encouraging them to follow evacuation procedures in the case of a fire - will rely largely on their ability to <i>persuade</i> people. Research in a range of areas from political communication to education suggest that the nonverbal behaviors of a human speaker play a key role in the persuasiveness of the speaker's message and the listeners' compliance with it. In this paper, we explore how a robot might effectively use these behaviors, particularly vocal and bodily cues, to persuade users. In an experiment with 32 participants, we evaluate how manipulations in a robot's use of nonverbal cues affected participants' perceptions of the robot's persuasiveness and their compliance with the robot's suggestions across four conditions: (1) no vocal or bodily cues, (2) vocal cues only, (3) bodily cues only, and (4) vocal and bodily cues. The results showed that participants complied with the robot's suggestions significantly more when it used nonverbal cues than they did when it did not use these cues and that bodily cues were more effective in persuading participants than vocal cues were. Our model of persuasive nonverbal cues and experimental results have direct implications for the design of persuasive behaviors for humanlike robots.</p></div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157799&CFID=105753441&CFTOKEN=50690582">How do people walk side-by-side?: using a computational model of human behavior for a social robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81482661849&CFID=105753441&CFTOKEN=50690582">Luis Yoichi Morales Saiki</a>, 
                        <a href="author_page.cfm?id=81321498073&CFID=105753441&CFTOKEN=50690582">Satoru Satake</a>, 
                        <a href="author_page.cfm?id=81500646361&CFID=105753441&CFTOKEN=50690582">Rajibul Huq</a>, 
                        <a href="author_page.cfm?id=81500656305&CFID=105753441&CFTOKEN=50690582">Dylan Glas</a>, 
                        <a href="author_page.cfm?id=81500655378&CFID=105753441&CFTOKEN=50690582">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81500647099&CFID=105753441&CFTOKEN=50690582">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 301-308</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157799" title="DOI">10.1145/2157689.2157799</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157799&ftid=1161798&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157799&ftid=1161848&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow110" style="display:inline;"><br /><div style="display:inline">This paper presents a computational model for side-by-side walking for human-robot interaction (HRI). In this work we address the importance of future motion utility (motion anticipation) of the two walking partners. Previous studies only considered ...</div></span>
          <span id="toHide110" style="display:none;"><br /><div style="display:inline"><p>This paper presents a computational model for side-by-side walking for human-robot interaction (HRI). In this work we address the importance of future motion utility (motion anticipation) of the two walking partners.</p> <p>Previous studies only considered a robot moving alongside a person without collisions with simple velocity-based predictions. In contrast, our proposed model includes two major considerations. First, it considers the current goal, modeling side-by-side walking, as a process of moving towards a goal while maintaining a relative position with the partner. Second, it takes the partner's utility into consideration; it models side-by-side walking as a phenomenon where two agents maximize mutual utilities rather than only considering a single agent utility. The model is constructed and validated with a set of trajectories from pairs of people recorded in side-by-side walking. Finally, our proposed model was tested in an autonomous robot walking side-by-side with participants and demonstrated to be effective.</p></div></span> <a id="expcoll110" href="JavaScript: expandcollapse('expcoll110',110)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157800&CFID=105753441&CFTOKEN=50690582">A techno-sociological solution for designing a museum guide robot: regarding choosing an appropriate visitor</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100620449&CFID=105753441&CFTOKEN=50690582">Akiko Yamazaki</a>, 
                        <a href="author_page.cfm?id=81500659987&CFID=105753441&CFTOKEN=50690582">Keiichi Yamazaki</a>, 
                        <a href="author_page.cfm?id=81500658880&CFID=105753441&CFTOKEN=50690582">Takaya Ohyama</a>, 
                        <a href="author_page.cfm?id=81500650246&CFID=105753441&CFTOKEN=50690582">Yoshinori Kobayashi</a>, 
                        <a href="author_page.cfm?id=81500659015&CFID=105753441&CFTOKEN=50690582">Yoshinori Kuno</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 309-316</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157800" title="DOI">10.1145/2157689.2157800</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157800&ftid=1161799&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=2157800&ftid=1161800&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow111" style="display:inline;"><br /><div style="display:inline">In this paper, we present our work designing a robot that explains an exhibit to multiple visitors in a museum setting, based on ethnographic analysis of interactions between expert human guides and visitors. During the ethnographic analysis, we discovered ...</div></span>
          <span id="toHide111" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present our work designing a robot that explains an exhibit to multiple visitors in a museum setting, based on ethnographic analysis of interactions between expert human guides and visitors. During the ethnographic analysis, we discovered that expert human guides employ some identical strategies and practices in their explanations. In particular, one of these is to involve all visitors by posing a question to an appropriate visitor among them, which we call the "creating a puzzle" sequence. This is done in order to draw visitors' attention towards not only the exhibit and but also the guide's explanation. While creating a puzzle, the human guide can monitor visitors' responses and choose an "appropriate" visitor (i.e. one who is likely to provide an answer). Based on these findings, sociologists and engineers together developed a guide robot that coordinates verbal and non-verbal actions in posing a question or "a puzzle" that will draw visitors' attention, and then explain the exhibit for multiple visitors. During the explanation, the robot chooses an "appropriate" visitor. We tested the robot at an actual museum. The results show that our robot increases visitors' engagement and interaction with the guide, as well as interaction and engagement among visitors.</p></div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">PANEL SESSION: <strong>Panel</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Katherine Tsui, Stephen von Rump 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157802&CFID=105753441&CFTOKEN=50690582">Robots in the loop: telepresence robots in everyday life</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Katherine M. Tsui, Stephen Von Rump, Hiroshi Ishiguro, Leila Takayama, Peter N. Vicars 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 317-318</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157802" title="DOI">10.1145/2157689.2157802</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157802&ftid=1161801&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow113" style="display:inline;"><br /><div style="display:inline">This year's human-robot interaction (HRI) conference focuses on "robots in the loop" and how robots are capable of enhancing the experiences of human users in everyday life and work. Telepresence robots allow operators the ability to participate in remote ...</div></span>
          <span id="toHide113" style="display:none;"><br /><div style="display:inline"><p>This year's human-robot interaction (HRI) conference focuses on "robots in the loop" and how robots are capable of enhancing the experiences of human users in everyday life and work. Telepresence robots allow operators the ability to participate in remote locations through their mobility and live bidirectional audio and video feeds. Using robotic telepresence, students with chronic illnesses are attending their regular classes, physicians are conducting virtual "home visits" for recovering patients, and remote teammates are having conversations beyond the office conference room.</p> <p>This panel gathers experts from academia, business, and industry to discuss their experiences in developing robotic telepresences and "ah ha" moments reported from field use. Topics include how telepresence is defined, the practical use cases and application domains, the social and practical challenges encountered by operators and people physically present with the robots, and the implications for design of telepresence robots given these considerations.</p></div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Living and working with service robots</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Astrid Weiss 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157804&CFID=105753441&CFTOKEN=50690582">Personalization in HRI: a longitudinal field experiment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500641040&CFID=105753441&CFTOKEN=50690582">Min Kyung Lee</a>, 
                        <a href="author_page.cfm?id=81500651587&CFID=105753441&CFTOKEN=50690582">Jodi Forlizzi</a>, 
                        <a href="author_page.cfm?id=81100476487&CFID=105753441&CFTOKEN=50690582">Sara Kiesler</a>, 
                        <a href="author_page.cfm?id=81100077697&CFID=105753441&CFTOKEN=50690582">Paul Rybski</a>, 
                        <a href="author_page.cfm?id=81500665354&CFID=105753441&CFTOKEN=50690582">John Antanitis</a>, 
                        <a href="author_page.cfm?id=81500662877&CFID=105753441&CFTOKEN=50690582">Sarun Savetsila</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 319-326</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157804" title="DOI">10.1145/2157689.2157804</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157804&ftid=1161849&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow115" style="display:inline;"><br /><div style="display:inline">Creating and sustaining rapport between robots and people is critical for successful robotic services. As a first step towards this goal, we explored a personalization strategy with a snack delivery robot. We designed a social robotic snack delivery ...</div></span>
          <span id="toHide115" style="display:none;"><br /><div style="display:inline"><p>Creating and sustaining rapport between robots and people is critical for successful robotic services. As a first step towards this goal, we explored a personalization strategy with a snack delivery robot. We designed a social robotic snack delivery service, and, for half of the participants, personalized the service based on participants' service usage and interactions with the robot. The service ran for each participant for two months. We evaluated this strategy during a 4-month field experiment. The results show that, as compared with the social service alone, adding personalized service improved rapport, cooperation, and engagement with the robot during service encounters.</p></div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157805&CFID=105753441&CFTOKEN=50690582">Exploring the role of robots in home organization</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Caroline Pantofaru, Leila Takayama, Tully Foote, Bianca Soto 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 327-334</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157805" title="DOI">10.1145/2157689.2157805</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157805&ftid=1161802&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow116" style="display:inline;"><br /><div style="display:inline">Technologists have long wanted to put robots in the home, making robots truly personal and present in every aspect of our lives. It has not been clear, however, exactly what these robots should do in the home. The difficulty of tasking robots with home ...</div></span>
          <span id="toHide116" style="display:none;"><br /><div style="display:inline"><p>Technologists have long wanted to put robots in the home, making robots truly personal and present in every aspect of our lives. It has not been clear, however, exactly what these robots should do in the home. The difficulty of tasking robots with home chores comes not only from the significant technical challenges, but also from the strong emotions and expectations people have about their home lives. In this paper, we explore one possible set of tasks a robot could perform, home organization and storage tasks. Using the technique of <i>need finding</i>, we interviewed a group of people regarding the reality of organization in their home; the successes, failures, family dynamics and practicalities surrounding organization. These interviews are abstracted into a set of frameworks and design implications for home robotics, which we contribute to the community as inspiration and hypotheses for future robot prototypes to test.</p></div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157806&CFID=105753441&CFTOKEN=50690582">The domesticated robot: design guidelines for assisting older adults to age in place</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jenay M. Beer, Cory-Ann Smarr, Tiffany L. Chen, Akanksha Prakash, Tracy L. Mitzner, Charles C. Kemp, Wendy A. Rogers 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 335-342</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157806" title="DOI">10.1145/2157689.2157806</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157806&ftid=1161803&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow117" style="display:inline;"><br /><div style="display:inline">Many older adults wish to remain in their own homes as they age [16]. However, challenges in performing home upkeep tasks threaten an older adult's ability to age in place. Even healthy independently living older adults experience challenges in maintaining ...</div></span>
          <span id="toHide117" style="display:none;"><br /><div style="display:inline"><p>Many older adults wish to remain in their own homes as they age [16]. However, challenges in performing home upkeep tasks threaten an older adult's ability to age in place. Even healthy independently living older adults experience challenges in maintaining their home [13]. Challenges with home tasks can be compensated through technology, such as home robots. However, for home robots to be adopted by older adult users, they must be designed to meet older adults' needs for assistance and the older users must be amenable to robot assistance for those needs. We conducted a needs assessment to (1) assess older adults' openness to assistance from robots; and (2) understand older adults' opinions about using an assistive robot to help around the home. We administered questionnaires and conducted structured group interviews with 21 independently living older adults (ages 65-93). The questionnaire data suggest that older adults prefer robot assistance for cleaning and fetching/organizing tasks overall. However their assistance preferences discriminated between tasks. The interview data provided insight as to <i>why</i> they hold such preferences. Older adults reported benefits of robot assistance (e.g., the robot compensating for limitations, saving them time and effort, completing undesirable tasks, and performing tasks at a high level of performance). Participants also reported concerns such as the robot damaging the environment, being unreliable at or incapable of doing a task, doing tasks the older adult would rather do, or taking up too much space/storage. These data, along with specific comments from participant interviews, provide the basis for preliminary recommendations for designing mobile manipulator robots to support aging in place.</p></div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157807&CFID=105753441&CFTOKEN=50690582">The effect of monitoring by cameras and robots on the privacy enhancing behaviors of older adults</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Kelly Caine, Selma &#352;abanovic, Mary Carter 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 343-350</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157807" title="DOI">10.1145/2157689.2157807</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157807&ftid=1161804&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow118" style="display:inline;"><br /><div style="display:inline">This paper describes the results of an experimental study in which older adult participants interacted with three monitoring technologies designed to support their ability to age in place in their own home - a camera, a stationary robot, and a mobile ...</div></span>
          <span id="toHide118" style="display:none;"><br /><div style="display:inline"><p>This paper describes the results of an experimental study in which older adult participants interacted with three monitoring technologies designed to support their ability to age in place in their own home - a camera, a stationary robot, and a mobile robot. The aim of our study was to evaluate users' perceptions of privacy and their tendencies to engage in privacy enhancing behaviors (PEBs) by comparing the three conditions. We found that privacy concerns lead older adults to change their behavior in a home environment while being monitored by cameras or embodied robots. We expected participants to engage in more PEBs when they interacted with a mobile robot, which provided embodied cues of ongoing monitoring; surprisingly, we found the opposite to be true - the camera was the condition in which participants performed more PEBs. We describe the results of quantitative and qualitative analyses of our survey, interview, and observational data and discuss the implications of our study for human-robot interaction, the study of privacy and technology, and the design of assistive robots for monitoring older adults.</p></div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Robots for children</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Hideaki Kuzuoka 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157809&CFID=105753441&CFTOKEN=50690582">Children learning with a social robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81311482839&CFID=105753441&CFTOKEN=50690582">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81311486071&CFID=105753441&CFTOKEN=50690582">Michihiro Shimada</a>, 
                        <a href="author_page.cfm?id=81325488901&CFID=105753441&CFTOKEN=50690582">Satoshi Koizumi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 351-358</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157809" title="DOI">10.1145/2157689.2157809</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157809&ftid=1161850&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMpeg" title="Other Formats Mpeg" href="ft_gateway.cfm?id=2157809&ftid=1161805&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mpeg.gif" alt="Mpeg" class="fulltext_lnk" border="0" />Mpeg</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow120" style="display:inline;"><br /><div style="display:inline">We used a social robot as a teaching assistant in a class for children's collaborative learning. In the class, a group of 6th graders learned together using Lego Mindstorms. The class consisted of seven lessons with Robovie, a social robot, followed ...</div></span>
          <span id="toHide120" style="display:none;"><br /><div style="display:inline"><p>We used a social robot as a teaching assistant in a class for children's collaborative learning. In the class, a group of 6th graders learned together using Lego Mindstorms. The class consisted of seven lessons with Robovie, a social robot, followed by one lesson to test their individual achievement. Robovie managed the class and explained how to use Lego Mindstorms. In addition to such basic management behaviors for the class, we prepared social behaviors for building relationships with the children and encouraging them. The result shows that the social behavior encouraged children to work more in the first two lessons, but did not affect them in later lessons. On the other hand, social behavior contributed to building relationships and attaining better social acceptance.</p></div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157810&CFID=105753441&CFTOKEN=50690582">Blended reality characters</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100332062&CFID=105753441&CFTOKEN=50690582">David Robert</a>, 
                        <a href="author_page.cfm?id=81100258451&CFID=105753441&CFTOKEN=50690582">Cynthia Breazeal</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 359-366</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157810" title="DOI">10.1145/2157689.2157810</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157810&ftid=1161806&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow121" style="display:inline;"><br /><div style="display:inline">We present the idea and formative design of a blended reality character, a new class of character able to maintain visual and kinetic continuity between the fully physical and fully virtual. The interactive character's embodiment fluidly transitions ...</div></span>
          <span id="toHide121" style="display:none;"><br /><div style="display:inline"><p>We present the idea and formative design of a blended reality character, a new class of character able to maintain visual and kinetic continuity between the fully physical and fully virtual. The interactive character's embodiment fluidly transitions from an animated character on-screen to a small, alphabet block-shaped mobile robot designed as a platform for informal learning through play. We present the design and results of our study with thirty-four children aged three and a half to seven conducted using non-reactive, unobtrusive observational methods and a validated evaluation instrument. Our claim is that young children have accepted the idea, persistence and continuity of blended reality characters. Furthermore, we found that children are more deeply engaged with blended reality characters and are more fully immersed in blended reality play as co-protagonists in the experience, in comparison to interactions with strictly screen-based representations. As substantiated through the use of quantitative and qualitative analysis of drawings and verbal utterances, the study shows that young children produce longer, detailed and more imaginative descriptions of their experiences following blended reality play. The desire to continue engaging in blended reality play as expressed by children's verbal requests to revisit and extend their play time with the character positively affirms the potential for the development of an informal learning platform with sustained appeal to young children.</p></div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157811&CFID=105753441&CFTOKEN=50690582">Modelling empathic behaviour in a robotic game companion for children: an ethnographic study in real-world settings</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81367599529&CFID=105753441&CFTOKEN=50690582">Iolanda Leite</a>, 
                        <a href="author_page.cfm?id=81335488884&CFID=105753441&CFTOKEN=50690582">Ginevra Castellano</a>, 
                        <a href="author_page.cfm?id=81367593336&CFID=105753441&CFTOKEN=50690582">Andr&#233; Pereira</a>, 
                        <a href="author_page.cfm?id=81500654217&CFID=105753441&CFTOKEN=50690582">Carlos Martinho</a>, 
                        <a href="author_page.cfm?id=81500653186&CFID=105753441&CFTOKEN=50690582">Ana Paiva</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 367-374</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157811" title="DOI">10.1145/2157689.2157811</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157811&ftid=1161807&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157811&ftid=1161851&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow122" style="display:inline;"><br /><div style="display:inline">The idea of autonomous social robots capable of assisting us in our daily lives is becoming more real every day. However, there are still many open issues regarding the social capabilities that those robots should have in order to make daily interactions ...</div></span>
          <span id="toHide122" style="display:none;"><br /><div style="display:inline"><p>The idea of autonomous social robots capable of assisting us in our daily lives is becoming more real every day. However, there are still many open issues regarding the social capabilities that those robots should have in order to make daily interactions with humans more natural. For example, the role of affective interactions is still unclear. This paper presents an ethnographic study conducted in an elementary school where 40 children interacted with a social robot capable of recognising and responding empathically to some of the children's affective states. The findings suggest that the robot's empathic behaviour affected positively how children perceived the robot. However, the empathic behaviours should be selected carefully, under the risk of having the opposite effect. The target application scenario and the particular preferences of children seem to influence the degree of empathy that social robots should be endowed with.</p></div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Animating robot behavior</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Sonia Chernova 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157813&CFID=105753441&CFTOKEN=50690582">Enhancing interaction through exaggerated motion synthesis</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Michael J. Gielniak, Andrea L. Thomaz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 375-382</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157813" title="DOI">10.1145/2157689.2157813</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157813&ftid=1161808&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow124" style="display:inline;"><br /><div style="display:inline">Other than eye gaze and referential gestures (e.g. pointing), the relationship between robot motion and observer attention is not well understood. We explore this relationship to achieve social goals, such as influencing human partner behavior or directing ...</div></span>
          <span id="toHide124" style="display:none;"><br /><div style="display:inline"><p>Other than eye gaze and referential gestures (e.g. pointing), the relationship between robot motion and observer attention is not well understood. We explore this relationship to achieve social goals, such as influencing human partner behavior or directing attention. We present an algorithm that creates exaggerated variants of a motion in real-time. Through two experiments we confirm that exaggerated motion is perceptibly different than the input motion, provided that the motion is sufficiently exaggerated. We found that different levels of exaggeration correlate to human expectations of robot-like, human-like, and cartoon-like motion. We present empirical evidence that use of exaggerated motion in experiments enhances the interaction through the benefits of increased engagement and perceived entertainment value. Finally, we provide statistical evidence that exaggerated motion causes predictable human partner gaze direction and better retention of interaction details.</p></div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157814&CFID=105753441&CFTOKEN=50690582">The illusion of robotic life: principles and practices of animation for robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81488665021&CFID=105753441&CFTOKEN=50690582">Tiago Ribeiro</a>, 
                        <a href="author_page.cfm?id=81500653187&CFID=105753441&CFTOKEN=50690582">Ana Paiva</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 383-390</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157814" title="DOI">10.1145/2157689.2157814</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157814&ftid=1161809&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157814&ftid=1161810&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow125" style="display:inline;"><br /><div style="display:inline">This paper describes our approach on the development of the expression of emotions on a robot with constrained facial expressions. We adapted principles and practices of animation from Disney&#8482; and other animators for robots, and applied ...</div></span>
          <span id="toHide125" style="display:none;"><br /><div style="display:inline"><p>This paper describes our approach on the development of the expression of emotions on a robot with constrained facial expressions. We adapted principles and practices of animation from Disney<sup>&#8482;</sup> and other animators for robots, and applied them on the development of emotional expressions for the EMYS robot. Our work shows that applying animation principles to robots is beneficial for human understanding of the robots' emotions.</p></div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157815&CFID=105753441&CFTOKEN=50690582">Trajectories and keyframes for kinesthetic teaching: a human-robot interaction perspective</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Baris Akgun, Maya Cakmak, Jae Wook Yoo, Andrea Lockerd Thomaz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 391-398</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157815" title="DOI">10.1145/2157689.2157815</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157815&ftid=1161852&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157815&ftid=1161811&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow126" style="display:inline;"><br /><div style="display:inline">Kinesthetic teaching is an approach to providing demonstrations to a robot in Learning from Demonstration whereby a human physically guides a robot to perform a skill. In the common usage of kinesthetic teaching, the robot's trajectory during a demonstration ...</div></span>
          <span id="toHide126" style="display:none;"><br /><div style="display:inline"><p>Kinesthetic teaching is an approach to providing demonstrations to a robot in Learning from Demonstration whereby a human physically guides a robot to perform a skill. In the common usage of kinesthetic teaching, the robot's trajectory during a demonstration is recorded from start to end. In this paper we consider an alternative, <i>keyframe demonstrations</i>, in which the human provides a sparse set of consecutive keyframes that can be connected to perform the skill. We present a user-study (<i>n</i>=34) comparing the two approaches and highlighting their complementary nature. The study also tests and shows the potential benefits of iterative and adaptive versions of keyframe demonstrations. Finally, we introduce a hybrid method that combines trajectories and keyframes in a single demonstration.</p></div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>HRI 2012 video session</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Martin Saerbeck 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157817&CFID=105753441&CFTOKEN=50690582">SDT: a "konkon" interface to buildup the connotation interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500660933&CFID=105753441&CFTOKEN=50690582">Yu Arita</a>, 
                        <a href="author_page.cfm?id=81500662071&CFID=105753441&CFTOKEN=50690582">Hirota Shinya</a>, 
                        <a href="author_page.cfm?id=81500643872&CFID=105753441&CFTOKEN=50690582">Yuta Yoshiike</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=105753441&CFTOKEN=50690582">P. Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=105753441&CFTOKEN=50690582">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 399-400</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157817" title="DOI">10.1145/2157689.2157817</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157817&ftid=1161812&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157817&ftid=1161813&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow128" style="display:inline;"><br /><div style="display:inline">The advantage of the proposed approach is that users can invent their own communication protocol (based on the knock patterns) to communicate with the creature. This approach is a novel concept to establish future human-robot communication protocols ...</div></span>
          <span id="toHide128" style="display:none;"><br /><div style="display:inline"><p>The advantage of the proposed approach is that users can invent their own communication protocol (based on the knock patterns) to communicate with the creature. This approach is a novel concept to establish future human-robot communication protocols within many contexts or human centric applications. However, through symbolic communication (knock-based communication) a human is able to convey to the robot to adapt and communicate with their personalized communication protocol.</p></div></span> <a id="expcoll128" href="JavaScript: expandcollapse('expcoll128',128)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157818&CFID=105753441&CFTOKEN=50690582">Human-swarm interaction through distributed cooperative gesture recognition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100420713&CFID=105753441&CFTOKEN=50690582">Alessandro Giusti</a>, 
                        <a href="author_page.cfm?id=81500645516&CFID=105753441&CFTOKEN=50690582">Jawad Nagi</a>, 
                        <a href="author_page.cfm?id=81100622805&CFID=105753441&CFTOKEN=50690582">Luca M. Gambardella</a>, 
                        <a href="author_page.cfm?id=81467671092&CFID=105753441&CFTOKEN=50690582">St&#233;phane Bonardi</a>, 
                        <a href="author_page.cfm?id=81405595799&CFID=105753441&CFTOKEN=50690582">Gianni A. Di Caro</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 401-402</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157818" title="DOI">10.1145/2157689.2157818</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157818&ftid=1161853&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157818&ftid=1161814&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157819&CFID=105753441&CFTOKEN=50690582">Field trials of the block-shaped edutainment robot hangulbot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Sonya S. Kwak, Eun Ho Kim, Jimyung Kim, Youngbin Son, Inveom Kwak, Jun-Shin Park, Eun Wook Lee 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 403-404</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157819" title="DOI">10.1145/2157689.2157819</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157819&ftid=1161815&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157819&ftid=1161816&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow130" style="display:inline;"><br /><div style="display:inline">The objective of this study was to develop an edutainment robot which provides multi-sensory learning experiences to improve users' space perception and creativity. In particular, we focused on developing educational content for the study of the Korean ...</div></span>
          <span id="toHide130" style="display:none;"><br /><div style="display:inline"><p>The objective of this study was to develop an edutainment robot which provides multi-sensory learning experiences to improve users' space perception and creativity. In particular, we focused on developing educational content for the study of the Korean alphabet (Hangul) using the robot. On the basis of the phonemic and modular nature of Hangul, we devised a block-shaped edutainment robot for the study of Hangul. The robot known as "HangulBot" is composed of a consonant block and a vowel block. By rotating and rearranging those blocks, a user can create different characters. To enable the robot to perceive the arrangement of the blocks and the distance between a consonant block and a vowel block, IR LEDs and photo transistors were used. The eight IR LEDs in the consonant block generate different radiation signals, and the vowel block perceives the arrangement of the blocks by receiving the signals. The distance between the two blocks is estimated by measuring the thresholding, and the corresponding sound of each arrangement is then played through a speaker installed in the vowel block. We executed two short-term field trials with a twenty-seven month old child in June of 2011 and November of 2011 to ascertain children's initial reaction to HangulBot and how their reaction would change over time. While the results are preliminary, we noted several interesting findings. First, after several trials by the mother, the child felt comfortable with HangulBot. Second, the child intuitively followed the corresponding speech sounds which were generated by HangulBot according to the arrangement of the blocks. That is to say, the sound generated after the arranging the block intuitively induced the child to follow the sound. Third, the child's initial reaction to HangulBot was mostly block play, but after five months later, her reaction to the robot included not only block play but also active learning of the Korean alphabet. This result indicates that HangulBot could be an effective edutainment tool which improves space perception and creativity as well as linguistic abilities by stimulating both sides of the brain.</p></div></span> <a id="expcoll130" href="JavaScript: expandcollapse('expcoll130',130)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157820&CFID=105753441&CFTOKEN=50690582">Human-human vs. human-robot teamed investigation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456634287&CFID=105753441&CFTOKEN=50690582">Caroline E. Harriott</a>, 
                        <a href="author_page.cfm?id=81500665181&CFID=105753441&CFTOKEN=50690582">Glenna L. Buford</a>, 
                        <a href="author_page.cfm?id=81482645412&CFID=105753441&CFTOKEN=50690582">Tao Zhang</a>, 
                        <a href="author_page.cfm?id=81456611985&CFID=105753441&CFTOKEN=50690582">Julie A. Adams</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 405-406</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157820" title="DOI">10.1145/2157689.2157820</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157820&ftid=1161854&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=2157820&ftid=1161817&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow131" style="display:inline;"><br /><div style="display:inline">Clips from an evaluation where participants, each paired with either a human or robot partner, were deployed to search a hallway for suspicious items, in a manner similar to tactics used by first responders handling bomb threats are presented. The teams ...</div></span>
          <span id="toHide131" style="display:none;"><br /><div style="display:inline"><p>Clips from an evaluation where participants, each paired with either a human or robot partner, were deployed to search a hallway for suspicious items, in a manner similar to tactics used by first responders handling bomb threats are presented. The teams used natural, verbal communication to collaborate, determine where hazards were located, and which items were suspicious. The video demonstrates that the investigations in both conditions played out in a similar manner and participants were able to complete the investigations successfully with a robot partner; however, sometimes the participants were uncertain how to interact with the robot.</p></div></span> <a id="expcoll131" href="JavaScript: expandcollapse('expcoll131',131)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157821&CFID=105753441&CFTOKEN=50690582">Acting lesson with robot: emotional gestures</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Heather Knight, Matthew Gray 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 407-408</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157821" title="DOI">10.1145/2157689.2157821</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157821&ftid=1161818&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=2157821&ftid=1161819&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow132" style="display:inline;"><br /><div style="display:inline">In this video, real-life acting professor Matthew Gray tutors Data the Robot (a Nao model) to improve his expression of emotion via Chekhov's Psychological Gestures. Though the video narrative is fictional and the robot actions pre-programmed, the aim ...</div></span>
          <span id="toHide132" style="display:none;"><br /><div style="display:inline"><p>In this video, real-life acting professor Matthew Gray tutors Data the Robot (a Nao model) to improve his expression of emotion via Chekhov's Psychological Gestures. Though the video narrative is fictional and the robot actions pre-programmed, the aim of the dramatization is to introduce an acting methodology that social robots could use to leverage full body affect expressions.</p> <p>The video begins with Gray leading Nao in traditional human actor warm-up exercises. Next, Gray shows Data a video of his students practicing Chekhov psychological gestures [4] [11].</p> <p>Finally, Data tries out some 'push' gestures himself. By pairing the 'push' gesture with text, the viewer is intended to unconsciously associate the words with an outpouring of emotion. Finally, Data's programmer, Knight, arrives to pick up the robot from his lesson, "until next time".</p> <p>This video playfully introduces full-body emotional gestures. The benefit of such movement-based full-body expressions is that they do not necessarily require a robot to have human-like facial expressions nor humanoid form to be effective (though the interplay of psychological gesture with multi-modal expressions could provide fertile terrain for future research). Instead, these full-body motions are translations of an actor's motive/intent that suffuse the whole form (e.g. expansion, sluggishness, lightness).</p> <p>We note that there are various schools of physical theater dedicated to understanding movement [5]. Related investigations in the robotics world that have applied acting method or practice to social robot design or architecture also include [2][3][6][7][8][9][10].</p> <p>As Blaire writes about in her text on acting and neuroscience [1], the discovery of mirror neurons in our brain have led some dramaturges to theorize that audience members simulate the gestures of the performers through their own neural circuitry for interpretation. If so, full body gestures may be able to tap into our emotional experience in a uniquely human way. We hope this will be the first of several spirited demonstration videos that explore intersections wherein human acting methodologies might benefit the development of robot non-verbal expressions.</p></div></span> <a id="expcoll132" href="JavaScript: expandcollapse('expcoll132',132)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157822&CFID=105753441&CFTOKEN=50690582">MARIOBOT: marionette robot that interact with an audience</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Woong-Ji Kim, Sun-Wook Choi, Chong Ho Lee 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 409-410</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157822" title="DOI">10.1145/2157689.2157822</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157822&ftid=1161855&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157822&ftid=1161820&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow133" style="display:inline;"><br /><div style="display:inline">A marionette is a puppet operated by human manipulator using strings and control bar. It is an ancient and universal form of performing art. Marionettes have been used for entertainment for centuries because people are fascinated with unique and funny ...</div></span>
          <span id="toHide133" style="display:none;"><br /><div style="display:inline"><p>A marionette is a puppet operated by human manipulator using strings and control bar. It is an ancient and universal form of performing art. Marionettes have been used for entertainment for centuries because people are fascinated with unique and funny movements of marionettes.</p> <p>In this video, we introduce a new type of entertainment robot for theatrical performances. Our MARIOBOT (marionette robot) is driven by a robotic controller that consists of eight motors and their coupling components. Each motor pulls the string connected to corresponding marionette's joint. This robot freely moves along the stage hung by a mobile base platform.</p> <p>In order to show the capabilities of our robot actor on the stage, this video with scripts demonstrates the performance of the robot. We also show a few audience-robot interaction methods, as from the perspective of the modern performances on the stage, the feature of interaction with the audience draws much of interests and appeals to the audience [1]. The audience loses attention quickly during a passive performance. Therefore, we need to consider appropriate HRI techniques to keep the audience's attention lively. However typical stage environment in the theater is not well-conditioned to accommodate the vision based HRI methods, because of: luminance of theater, distance between robot and the audience, etc. To overcome these problems, we present effective methods for interaction with the audience and the situations related to these methods on the video together with the system overview.</p></div></span> <a id="expcoll133" href="JavaScript: expandcollapse('expcoll133',133)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157823&CFID=105753441&CFTOKEN=50690582">Introducing students grades 6-12 to expressive robotics</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          David V. Lu, Ross Mead 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 411-412</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157823" title="DOI">10.1145/2157689.2157823</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157823&ftid=1161821&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=2157823&ftid=1161822&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow134" style="display:inline;"><br /><div style="display:inline">Every year, tens of thousands of middle school and high school students participate in robotics competitions, such as Botball, FIRST, and VEX. This provides them with an excellent introduction to the ins and outs of building robots and programming them ...</div></span>
          <span id="toHide134" style="display:none;"><br /><div style="display:inline"><p>Every year, tens of thousands of middle school and high school students participate in robotics competitions, such as Botball, FIRST, and VEX. This provides them with an excellent introduction to the ins and outs of building robots and programming them to autonomously accomplish specific tasks. However, the rules of many of these competitions often limit or prohibit human interaction with the robots. As a result, students are not exposed to and are, thus, not encouraged to think about human-robot interaction (HRI) and its potential impacts on society.</p> <p>To start getting a new generation of students thinking about HRI, we held a workshop entitled "Expressive Robotics: Motion and Emotion" at the 2011 Global Conference on Educational Robotics. The focus of the workshop was to teach the students how to program robots to express emotion and intent just through the robot's physical actions. The robot used by each participating group was not unlike the ones they used in competitions: an iRobot Create base provided a mobile platform, upon which stood a three degree-of-freedom arm, serving as an articulated spine and head.</p> <p>Each group was first asked to program the robot in a way that expressed one of Eckman's six basic emotions. They were then tasked with implementing a simple keyframe animation system to control the robot's limited degrees of freedom, which they could then modify to illustrate certain principles from theatre and animation. They used their animation system to tell a story purely through robot motion and interactions with props, as seen in Figure 1. Students were encouraged to focus on making the robot's emotion and intent apparent to a human audience.</p> <p>Ultimately, the workshop proved to be a great success; it was very well-received by all involved, who encouraged us to make this an annual event at the conference. We felt that this type of workshop served as a fine proof-of-concept for introducing students grades 6-12 to human factors in robotics, and could be extended throughout the entire K-12 education pipeline. Furthermore, we believe that exposure to sociable robotics has the potential to increase interest and self-efficacy of underrepresented student populations, particularly girls, in STEM-related activities<sup>1</sup>.</p></div></span> <a id="expcoll134" href="JavaScript: expandcollapse('expcoll134',134)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157824&CFID=105753441&CFTOKEN=50690582">Multi-user multi-touch multi-robot command and control of multiple simulated robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Eric McCann, Sean McSheehy, Holly Yanco 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 413-414</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157824" title="DOI">10.1145/2157689.2157824</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157824&ftid=1161856&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=2157824&ftid=1161823&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow135" style="display:inline;"><br /><div style="display:inline">This video demonstrates three users sharing control of eight simulated robots with a Microsoft Surface and two Apple iPads using our Multi-user Multi-touch Multi-robot Command and Control Interface. The command and control interfaces are all capable ...</div></span>
          <span id="toHide135" style="display:none;"><br /><div style="display:inline"><p>This video demonstrates three users sharing control of eight simulated robots with a Microsoft Surface and two Apple iPads using our Multi-user Multi-touch Multi-robot Command and Control Interface.</p> <p>The command and control interfaces are all capable of moving their world camera through space, tasking one or more robots with a series of waypoints, and assuming manual control of a single robot for inspection of its sensors and tele-operation. They display full-screen images sent from their user's world camera, overlaid with icons that show the position and selection state of each robot in the camera's field of view, dots that indicate each robot's current destination, and rectangles that correspond to each other user's field of view.</p> <p>One multi-touch interface runs on a Microsoft Surface, and the others on Apple iPads; they all have the same functional capabilities, other than a few differences due to the form factor and touch sensing method used by the platforms. The Surface interface is able to interpret gestures that include more than just finger tips, such as placing both fists on the screen to make all robots stop and wait for new commands. As iPads sense touch capacitively, they do not support detection of such gestures. The Surface interface allows its user to move their world camera while simultaneously teleoperating one of the robots with our Dynamically Resizing Ergonomic and Multi-touch Controller (DREAM Controller) [1, 2]. On the iPads, however, the command and control mode and teleoperation mode are mutually exclusive.</p> <p>The robots are simulated in Microsoft Robotics Developer Studio. Each user's world camera has similar movement capabilities to a quad-copter. The UDP communications between users and robots are all handled by a single server that routes messages to the appropriate targets, allowing scalability of both the number of robots and users.</p></div></span> <a id="expcoll135" href="JavaScript: expandcollapse('expcoll135',135)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157825&CFID=105753441&CFTOKEN=50690582">Encounters: from talking heads to swarming heads</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Damith C. Herath, Christian Kroos,  Stelarc 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 415-416</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157825" title="DOI">10.1145/2157689.2157825</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157825&ftid=1161824&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=2157825&ftid=1161825&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow136" style="display:inline;"><br /><div style="display:inline">Robots at home and work has been a key theme in science fiction since the genre began. It is only now that we see this come in to realization, albeit in very basic forms such as the robot vacuum cleaners and various entertainment robotic platforms. In ...</div></span>
          <span id="toHide136" style="display:none;"><br /><div style="display:inline"><p>Robots at home and work has been a key theme in science fiction since the genre began. It is only now that we see this come in to realization, albeit in very basic forms such as the robot vacuum cleaners and various entertainment robotic platforms. In this video we highlight a number of projects woven around the iRobot Create research robot platform and an embodied conversational agent called the Prosthetic Head - an installation work by Stelarc. We start the visual journey by taking a satirical look at some of the parallels between a commercial communication product and the Prosthetic Head. The journey then moves through telepresence robotics, gesture based robot human interaction. The robots featured in the video are driven by an attention and behavioral system. Finally, the video concludes with a preview of the "Swarming Heads" - an interactive installation.</p></div></span> <a id="expcoll136" href="JavaScript: expandcollapse('expcoll136',136)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157826&CFID=105753441&CFTOKEN=50690582">Johnny-0, a compliant, force-controlled and interactive humanoid autonomous robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Fran&#231;ois Ferland, Arnaud Aumont, Dominic L&#233;tourneau, Marc-Antoine Legault, Fran&#231;ois Michaud 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 417-418</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157826" title="DOI">10.1145/2157689.2157826</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157826&ftid=1161857&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157826&ftid=1161826&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow137" style="display:inline;"><br /><div style="display:inline">Johnny-0, shown in Figure 1, is our new humanoid robot which integrates an expressive face on an orientable head, two arms with 4 degrees of freedom (DOF) each and grippers, mounted on an omnidirectional, non-holonomic mobile platform. Our underlying ...</div></span>
          <span id="toHide137" style="display:none;"><br /><div style="display:inline"><p>Johnny-0, shown in Figure 1, is our new humanoid robot which integrates an expressive face on an orientable head, two arms with 4 degrees of freedom (DOF) each and grippers, mounted on an omnidirectional, non-holonomic mobile platform. Our underlying goal with Johnny-0 is to design a platform capable of natural reciprocal interaction (motion, language, touch, affect) with humans, to address integration issues associated with advanced motion, interaction and cognition capabilities on the same platform, and their use in unconstrained real world conditions. To do so, compliance is a necessity to provide natural and safe interactions.</p> <p>One distinctive element of Johnny-0 is that it uses force-controlled actuators (called Differential Elastic Actuators, or DEA) for active steering of its mobile platform, and for interactive control of its 4-DOF arms. Compliance at the mobile platform level allows a person to physically guide the robot without having to push it from a specific location on the platform [1]. Motion can also be constrained to avoid obstacles and collisions, providing natural physical interaction with the robot. Impedance control of each joint enables infinite combination of arm behaviors, from zero impedance for free movement with gravity compensation, to high stiffness constraining the arms to precise positions or ranges of movement. Stiffness can be configured to create virtual constraints in cartesian space, providing force feedback to the user about movement's limitations of the arms. For instance, stiffening the arms in certain poses could indicate to the user that the arms are restrained to move into a specific volume. Beyond these limits, any pushing or pulling force can be perceived by the mobile base, and can be interpreted as an intention to move the robot around.</p> <p>Combining compliance to other sensors (e.g., Kinect motion sensor) and a robot head capable of facial expression allows Johnny-0 to detect incoming people and adjust the impedance of its actuators accordingly (e.g., extend its gripper to greet them), and express its state based on how people physically interact with it (e.g., displaying surprise when the user move the arms beyond specific limits).</p></div></span> <a id="expcoll137" href="JavaScript: expandcollapse('expcoll137',137)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157827&CFID=105753441&CFTOKEN=50690582">Situation understanding bot through language and environment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500642952&CFID=105753441&CFTOKEN=50690582">Daniel J. Brooks</a>, 
                        <a href="author_page.cfm?id=81472641280&CFID=105753441&CFTOKEN=50690582">Constantine Lignos</a>, 
                        <a href="author_page.cfm?id=81500662011&CFID=105753441&CFTOKEN=50690582">Mikhail S. Medvedev</a>, 
                        <a href="author_page.cfm?id=81500654556&CFID=105753441&CFTOKEN=50690582">Ian Perera</a>, 
                        <a href="author_page.cfm?id=81500659263&CFID=105753441&CFTOKEN=50690582">Cameron Finucane</a>, 
                        <a href="author_page.cfm?id=81436598936&CFID=105753441&CFTOKEN=50690582">Vasumathi Raman</a>, 
                        <a href="author_page.cfm?id=81500662396&CFID=105753441&CFTOKEN=50690582">Abraham Shultz</a>, 
                        <a href="author_page.cfm?id=81500663338&CFID=105753441&CFTOKEN=50690582">Sean McSheehy</a>, 
                        <a href="author_page.cfm?id=81481645506&CFID=105753441&CFTOKEN=50690582">Adam Norton</a>, 
                        <a href="author_page.cfm?id=81416597256&CFID=105753441&CFTOKEN=50690582">Hadas Kress-Gazit</a>, 
                        <a href="author_page.cfm?id=81100583233&CFID=105753441&CFTOKEN=50690582">Mitch Marcus</a>, 
                        <a href="author_page.cfm?id=81500643754&CFID=105753441&CFTOKEN=50690582">Holly A. Yanco</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 419-420</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157827" title="DOI">10.1145/2157689.2157827</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157827&ftid=1161827&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=2157827&ftid=1161828&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow138" style="display:inline;"><br /><div style="display:inline">This video shows a demonstration of a fully autonomous robot, an iRobot ATRV-JR, which can be given commands using natural language. Users type commands to the robot on a tablet computer, which are then parsed and processed using semantic analysis. This ...</div></span>
          <span id="toHide138" style="display:none;"><br /><div style="display:inline"><p>This video shows a demonstration of a fully autonomous robot, an iRobot ATRV-JR, which can be given commands using natural language. Users type commands to the robot on a tablet computer, which are then parsed and processed using semantic analysis. This information is used to build a plan representing the high level autonomous behaviors the robot should perform [2][1]. The robot can be given commands to be executed immediately (e.g., "Search the floor for hostages.") as well as standing orders for use over the entire run (e.g., "Let me know if you see any bombs.").</p> <p>In the scenario shown in the video, the robot is asked to identify and defuse bombs, as well as to report if it finds any hostages or bad guys. Users can also query the robot through this interface. The robot conveys information to the user through text and a graphical interface on a tablet computer. The system can add icons to the map displayed and highlight areas of the map to convey concepts such as "I am here".</p> <p>The video contains segments taken from a continuous 20 minute long run, shown at 4x speed. This work is a demonstration of a larger project called Situation Understanding Bot Through Language and Environment (SUBTLE). For more information, see www.subtlebot.org.</p></div></span> <a id="expcoll138" href="JavaScript: expandcollapse('expcoll138',138)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157828&CFID=105753441&CFTOKEN=50690582">How to sustain long-term interaction between children and ROBOSEM in english class</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jeonghye Han, Bokhyun Kang, Seongju Park, Seongwook Hong 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 421-422</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157828" title="DOI">10.1145/2157689.2157828</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157828&ftid=1161829&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=2157828&ftid=1161858&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow139" style="display:inline;"><br /><div style="display:inline">According to studies confirming that robot assisted learning (RAL) can positively contribute to improving learners' motivation and achievement for language learning [1, 4], RAL is facing its diffusion through the demand of parents and government. However, ...</div></span>
          <span id="toHide139" style="display:none;"><br /><div style="display:inline"><p>According to studies confirming that robot assisted learning (RAL) can positively contribute to improving learners' motivation and achievement for language learning [1, 4], RAL is facing its diffusion through the demand of parents and government. However, the biggest obstacle to the long-term interaction between humans and robots is the robots' low-success rate of visual and voice recognition, as well as the limitation of artificial intelligence for the daily-life HRI [3]. This study demonstrated ROBOSEM's ability to sustain long term interaction between children and a robot in an elementary English class from the pilot studies with IROBIQ, called Langbot [1]. Five factors are of concern in sustaining the long-term interaction between children and ROBOSEM, as shown in Figure 2: (1) enhancing the recognition ability of ROBOSEM with class materials, such as marker hats, bracelet watches embodied RFID tags, Wiimocon, etc; (2) sharing the birth story of ROBOSEM, which works to increase children's tolerance toward weak recognition from the result of [2]; (3) making a favorable impression, such as by flashing children's faces on the screen; (4) telling the history of a child's personal learning activities; and (5) tele-operation by the intelligence of a human being.</p></div></span> <a id="expcoll139" href="JavaScript: expandcollapse('expcoll139',139)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157829&CFID=105753441&CFTOKEN=50690582">Implementing human questioning strategies into quizzing-robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500658879&CFID=105753441&CFTOKEN=50690582">Takaya Ohyama</a>, 
                        <a href="author_page.cfm?id=81500660977&CFID=105753441&CFTOKEN=50690582">Yasutomo Maeda</a>, 
                        <a href="author_page.cfm?id=81500646144&CFID=105753441&CFTOKEN=50690582">Chiaki Mori</a>, 
                        <a href="author_page.cfm?id=81500650245&CFID=105753441&CFTOKEN=50690582">Yoshinori Kobayashi</a>, 
                        <a href="author_page.cfm?id=81500659013&CFID=105753441&CFTOKEN=50690582">Yoshinori Kuno</a>, 
                        <a href="author_page.cfm?id=81500655278&CFID=105753441&CFTOKEN=50690582">Rio Fujita</a>, 
                        <a href="author_page.cfm?id=81500659986&CFID=105753441&CFTOKEN=50690582">Keiichi Yamazaki</a>, 
                        <a href="author_page.cfm?id=81500661130&CFID=105753441&CFTOKEN=50690582">Shun Miyazawa</a>, 
                        <a href="author_page.cfm?id=81500660391&CFID=105753441&CFTOKEN=50690582">Akiko Yamazaki</a>, 
                        <a href="author_page.cfm?id=81500661373&CFID=105753441&CFTOKEN=50690582">Keiko Ikeda</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 423-424</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157829" title="DOI">10.1145/2157689.2157829</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157829&ftid=1161830&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157829&ftid=1161831&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow140" style="display:inline;"><br /><div style="display:inline">From our ethnographic studies on various kinds of museums, we discovered that guides routinely propose questions to visitors in order to draw their attention towards both his/her explanation and the exhibit. The guides' question sequences tend to begin ...</div></span>
          <span id="toHide140" style="display:none;"><br /><div style="display:inline"><p>From our ethnographic studies on various kinds of museums, we discovered that guides routinely propose questions to visitors in order to draw their attention towards both his/her explanation and the exhibit. The guides' question sequences tend to begin with a pre-question which serves to not only monitor visitors' behavior and responses, but to also alert visitors that a primary question would follow. We implemented this questioning-strategy with our robot system and investigated whether this strategy would also work in human-robot interaction. We developed a vision system that enables the robot to choose an appropriate visitor by monitoring a visitor's response from the initiation of a pre-question to the following pause. Results indicate that this questioning-strategy works effectively in human-robot interaction. In this experiment, the robot asked visitors about a photograph. At the pre-question, the robot delivered a rather easy question followed by a more challenging question (Figure 1). More participants turned their head away from the exhibition when they were not sure about their answer to the question. They either faced away from the robot, or smiled wryly at the robot or at each other. These types of behaviors index participants' states of knowledge, which we could utilize to develop a system by which the robot could choose an appropriate candidate by computational recognition.</p></div></span> <a id="expcoll140" href="JavaScript: expandcollapse('expcoll140',140)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157830&CFID=105753441&CFTOKEN=50690582">Whole-body imitation of human motions with a nao humanoid</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jonas Koenemann, Maren Bennewitz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 425-426</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157830" title="DOI">10.1145/2157689.2157830</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157830&ftid=1161832&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157830&ftid=1161859&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow141" style="display:inline;"><br /><div style="display:inline">We present a system that enables a humanoid robot to imitate complex whole-body motions of humans in real time. For recording the human motions, any sensor system capable of inferring the joint angle trajectories can be used. In our work, we capture ...</div></span>
          <span id="toHide141" style="display:none;"><br /><div style="display:inline"><p>We present a system that enables a humanoid robot to imitate complex whole-body motions of humans in real time. For recording the human motions, any sensor system capable of inferring the joint angle trajectories can be used. In our work, we capture the human data with an Xsens MVN motion capture system consisting of inertial sensors attached to the body. Our framework converts the human joint angles to the robot's joint angles in real time. Here, we use a mapping between the human's and the robot's joints to ensure feasibility of the motion. The focus of our system lies in ensuring static stability when the motions are executed which is a challenging task, depending on the complexity of the movements. To avoid falls of the robot that might occur when using direct imitation of the joint angle trajectories due to the different weight distribution, we developed an approach that actively balances the center of mass over the support polygon of the robot's feet. At every point in time, our approach ensures that the robot is in a statically stable configuration, i.e., that the ground projection of the center of mass lies within the convex hull of the foot contact points. To achieve this, we apply inverse kinematics given valid foot positions that satisfy the stability criterion and generate the corresponding leg joint angles. In more detail, our system first finds valid positions for the robot's feet by determining a target plane and its orientation, so that the feet can be placed planar and the robot's center of mass is over the support polygon. The new positions of the feet are chosen as the projection on the target plane. Afterwards, the corresponding leg joint angles are calculated via inverse kinematics. To determine whether the configuration is in the double support modus, and if not, which foot is the stance foot, we evaluate the position of the center of mass relative to the feet.</p> <p>As can be seen in the experiments with a Nao humanoid, our approach leads to a highly stable imitation of challenging human movements (see also Fig. 1). In contrast to recent approaches that capture human data using a Kinect-like sensor and only imitate arm movements while keeping the body static, our system can deal with complex, whole-body motions. Note that our approach does not require a prior learning phase but computes stable configurations online and almost in real time as can be seen in the accompanying video.</p> <p>We are currently working on imitating motions to learn complex navigation actions such as climbing up staircases or walking down ramps. Our system can also be used for tele-operated tasks that include whole-body movements where stability needs to be guaranteed in order to successfully fulfill the mission.</p></div></span> <a id="expcoll141" href="JavaScript: expandcollapse('expcoll141',141)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157831&CFID=105753441&CFTOKEN=50690582">Roboscopie: a theatre performance for a human and a robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          S&#233;verin Lemaignan, Mamoun Gharbi, Jim Mainprice, Matthieu Herrb, Rachid Alami 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 427-428</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157831" title="DOI">10.1145/2157689.2157831</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157831&ftid=1161833&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157832&CFID=105753441&CFTOKEN=50690582">Demonstrating Maori Haka with kinect and nao robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Thammathip Piumsomboon, Rory Clifford, Christoph Bartneck 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 429-430</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157832" title="DOI">10.1145/2157689.2157832</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157832&ftid=1161834&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=2157832&ftid=1161860&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow143" style="display:inline;"><br /><div style="display:inline">In this video, "Nao Haka", four robots and a haka leader perform a traditional Maori Haka. The Haka leader, who performs the main actions is supported by Aldebaran Nao Robots, which are controlled by an external performer, using a Microsoft Kinect as ...</div></span>
          <span id="toHide143" style="display:none;"><br /><div style="display:inline"><p>In this video, "Nao Haka", four robots and a haka leader perform a traditional Maori Haka. The Haka leader, who performs the main actions is supported by Aldebaran Nao Robots, which are controlled by an external performer, using a Microsoft Kinect as the input device. This device allows for full-body user tracking. This Video was made as a supportive gesture towards the All Blacks Rugby World Cup Campaign 2011.</p></div></span> <a id="expcoll143" href="JavaScript: expandcollapse('expcoll143',143)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Perception and recognition</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Kai Oliver Arras 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157834&CFID=105753441&CFTOKEN=50690582">The cocktail party robot: sound source separation and localisation with an active binaural head</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500663441&CFID=105753441&CFTOKEN=50690582">Antoine Deleforge</a>, 
                        <a href="author_page.cfm?id=81100268726&CFID=105753441&CFTOKEN=50690582">Radu Horaud</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 431-438</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157834" title="DOI">10.1145/2157689.2157834</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157834&ftid=1161835&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow145" style="display:inline;"><br /><div style="display:inline">Human-robot communication is often faced with the difficult problem of interpreting ambiguous auditory data. For example, the acoustic signals perceived by a humanoid with its on-board microphones contain a mix of sounds such as speech, music, electronic ...</div></span>
          <span id="toHide145" style="display:none;"><br /><div style="display:inline"><p>Human-robot communication is often faced with the difficult problem of interpreting ambiguous auditory data. For example, the acoustic signals perceived by a humanoid with its on-board microphones contain a mix of sounds such as speech, music, electronic devices, all in the presence of attenuation and reverberations. In this paper we propose a novel method, based on a generative probabilistic model and on active binaural hearing, allowing a robot to robustly perform sound-source separation and localization. We show how interaural spectral cues can be used within a constrained mixture model specifically designed to capture the richness of the data gathered with two microphones mounted onto a human-like artificial head. We describe in detail a novel EM algorithm, we analyse its initialization, speed of convergence and complexity, and we assess its performance with both simulated and real data.</p></div></span> <a id="expcoll145" href="JavaScript: expandcollapse('expcoll145',145)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157835&CFID=105753441&CFTOKEN=50690582">Multi-party human-robot interaction with distant-talking speech recognition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Randy Gomez, Tatsuya Kawahara, Keisuke Nakamura, Kazuhiro Nakadai 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 439-446</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157835" title="DOI">10.1145/2157689.2157835</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157835&ftid=1161836&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow146" style="display:inline;"><br /><div style="display:inline">Speech is one of the most natural medium for human communication, which makes it vital to human-robot interaction. In real environments where robots are deployed, distant-talking speech recognition is difficult to realize due to the effects of reverberation. ...</div></span>
          <span id="toHide146" style="display:none;"><br /><div style="display:inline"><p>Speech is one of the most natural medium for human communication, which makes it vital to human-robot interaction. In real environments where robots are deployed, distant-talking speech recognition is difficult to realize due to the effects of reverberation. This leads to the degradation of speech recognition and understanding, and hinders a seamless human-robot interaction. To minimize this problem, traditional speech enhancement techniques optimized for human perception are adopted to achieve robustness in human-robot interaction. However, human and machine perceive speech differently: an improvement in speech recognition performance may not automatically translate to an improvement in human-robot interaction experience (as perceived by the users). In this paper, we propose a method in optimizing speech enhancement techniques specifically to improve automatic speech recognition (ASR) with emphasis on the human-robot interaction experience. Experimental results using real reverberant data in a multi-party conversation, show that the proposed method improved human-robot interaction experience in severe reverberant conditions compared to the traditional techniques.</p></div></span> <a id="expcoll146" href="JavaScript: expandcollapse('expcoll146',146)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157836&CFID=105753441&CFTOKEN=50690582">Do you remember that shop?: computational model of spatial memory for shopping companion robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500655349&CFID=105753441&CFTOKEN=50690582">Takahiro Matsumoto</a>, 
                        <a href="author_page.cfm?id=81500641913&CFID=105753441&CFTOKEN=50690582">Satoru Satake</a>, 
                        <a href="author_page.cfm?id=81500655379&CFID=105753441&CFTOKEN=50690582">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81500653683&CFID=105753441&CFTOKEN=50690582">Michita Imai</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105753441&CFTOKEN=50690582">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 447-454</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157836" title="DOI">10.1145/2157689.2157836</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157836&ftid=1161837&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=2157836&ftid=1161861&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow147" style="display:inline;"><br /><div style="display:inline">We aim to develop a shopping companion robot that can share experience with users. In this study, we focused on the shared memory acquired when a robot walks together with a user. We developed a computational model of memory recall of visited locations ...</div></span>
          <span id="toHide147" style="display:none;"><br /><div style="display:inline"><p>We aim to develop a shopping companion robot that can share experience with users. In this study, we focused on the shared memory acquired when a robot walks together with a user. We developed a computational model of memory recall of visited locations in a shopping mall. The model was developed with data collection from 30 participants. We found that shop size, color intensity of facade, relative visibility, and time elapsed are the influencing features for recall. The model was used in a scenario of a shopping companion robot. The robot, Robovie, autonomously follows a user while inferring the user's memory recall of shops in the visited route. When the user asks the location of other shops, Robovie replied with <i>destination description</i>, referring to the known locations inferred with the model of the user's memory recall. With this scenario, we verified the effectiveness of the developed computational model of memory recall. The evaluation experiment revealed that the model outputs shops that the participants are likely to recall, and makes the directions given easier to understand.</p></div></span> <a id="expcoll147" href="JavaScript: expandcollapse('expcoll147',147)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157837&CFID=105753441&CFTOKEN=50690582">Color anomaly detection and suggestion for wilderness search and rescue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Bryan S. Morse, Daniel Thornton, Michael A. Goodrich 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 455-462</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157837" title="DOI">10.1145/2157689.2157837</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157837&ftid=1161838&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow148" style="display:inline;"><br /><div style="display:inline">In wilderness search and rescue, objects not native or typical to a scene may provide clues that indicate the recent presence of the missing person. This paper presents the results of augmenting an aerial wilderness search-and-rescue system with an automated ...</div></span>
          <span id="toHide148" style="display:none;"><br /><div style="display:inline"><p>In wilderness search and rescue, objects not native or typical to a scene may provide clues that indicate the recent presence of the missing person. This paper presents the results of augmenting an aerial wilderness search-and-rescue system with an automated spectral anomaly detector for identifying unusually colored objects. The detector dynamically builds a model of the natural coloring in the scene and identifies outlier pixels, which are then filtered both spatially and temporally to find unusually colored objects. These objects are then highlighted in the search video as suggestions for the user, thus shifting a portion of the user's task from scanning the video to verifying the suggestions. This paper empirically evaluates multiple potential detectors then incorporates the best-performing detector into a suggestion system. User study results demonstrate that even with an imperfect detector users' detection increased significantly. Results further indicate that users' false positive rates did not increase, though performance in a secondary task did decrease. Furthermore, users subjectively reported that the use of detector-based suggestions made the overall task easier. These results suggest that such suggestion-based systems for search can increase overall searcher performance but that additional external tasks should be limited.</p></div></span> <a id="expcoll148" href="JavaScript: expandcollapse('expcoll148',148)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Talking with robots: linguistics and natural language</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Bilge Mutlu 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157839&CFID=105753441&CFTOKEN=50690582">Levels of embodiment: linguistic analyses of factors influencing hri</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Kerstin Fischer, Katrin S. Lohan, Kilian Foth 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 463-470</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157839" title="DOI">10.1145/2157689.2157839</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157839&ftid=1161839&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow150" style="display:inline;"><br /><div style="display:inline">In this paper, we investigate the role of physical embodiment of a robot and its degrees of freedom in HRI. Both factors have been suggested to be relevant in definitions of embodiment, and so far we do not understand their effects on the way people ...</div></span>
          <span id="toHide150" style="display:none;"><br /><div style="display:inline"><p>In this paper, we investigate the role of physical embodiment of a robot and its degrees of freedom in HRI. Both factors have been suggested to be relevant in definitions of embodiment, and so far we do not understand their effects on the way people interact with robots very well. Linguistic analyses of verbal interactions with robots differing with respect to physical embodiment and degrees of freedom provide a useful methodology to investigate factors conditioning human-robot interaction. Results show that both physical embodiment and degrees of freedom influence interaction, and that the effect of physical embodiment is located in the interpersonal domain, concerning in how far the robot is perceived as an interaction partner, whereas degrees of freedom influence the way users project the suitability of the robot for the current task.</p></div></span> <a id="expcoll150" href="JavaScript: expandcollapse('expcoll150',150)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157840&CFID=105753441&CFTOKEN=50690582">Tell me when and why to do it!: run-time planner model updates via natural language instruction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Rehj Cantrell, Kartik Talamadupula, Paul Schermerhorn, J. Benton, Subbarao Kambhampati, Matthias Scheutz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 471-478</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157840" title="DOI">10.1145/2157689.2157840</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157840&ftid=1161840&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow151" style="display:inline;"><br /><div style="display:inline">Robots are currently being used in and developed for critical HRI applications such as search and rescue. In these scenarios, humans operating under changeable and high-stress conditions must communicate effectively with autonomous agents, necessitating ...</div></span>
          <span id="toHide151" style="display:none;"><br /><div style="display:inline"><p>Robots are currently being used in and developed for critical HRI applications such as search and rescue. In these scenarios, humans operating under changeable and high-stress conditions must communicate effectively with autonomous agents, necessitating that such agents be able to respond quickly and effectively to rapidly-changing conditions and expectations. We demonstrate a robot planner that is able to utilize new information, specifically information originating in spoken input produced by human operators.</p> <p>We show that the robot is able to learn the pre- and postconditions of previously-unknown action sequences from <i>natural</i> language constructions, and immediately update (1) its knowledge of the current state of the environment, and (2) its underlying world model, in order to produce new and updated plans that are consistent with this new information. While we demonstrate in detail the robot's successful operation with a specific example, we also discuss the dialogue module's inherent scalability, and investigate how well the robot is able to respond to natural language commands from untrained users.</p></div></span> <a id="expcoll151" href="JavaScript: expandcollapse('expcoll151',151)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157841&CFID=105753441&CFTOKEN=50690582">Talking with robots about objects: a system-level evaluation in HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Julia Peltason, Nina Riether, Britta Wrede, Ingo L&#252;tkebohle 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 479-486</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157841" title="DOI">10.1145/2157689.2157841</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157841&ftid=1161862&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow152" style="display:inline;"><br /><div style="display:inline">We present the design process, realization and evaluation of a robot system for nteractive object learning. The system-oriented evaluation, in particular, addresses an open problem for the evaluation of systems, where overall user satisfaction depends ...</div></span>
          <span id="toHide152" style="display:none;"><br /><div style="display:inline"><p>We present the design process, realization and evaluation of a robot system for nteractive object learning. The system-oriented evaluation, in particular, addresses an open problem for the evaluation of systems, where overall user satisfaction depends not only on the performance of the parts, but also on their combination, and on user behavior. Based on the PARADISE method known from spoken dialog systems, we have defined and applied internal and external metrics for fine-grained and largely automatable identification of such relationships. Through evaluation with n=28 subjects, indicator functions explaining up to 55% of variation in several satisfaction metrics were found. Furthermore, we demonstrate that the system's interaction style reduces the need for instruction and successfully recovers partial failures.</p></div></span> <a id="expcoll152" href="JavaScript: expandcollapse('expcoll152',152)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">WORKSHOP SESSION: <strong>Workshops &#38; tutorials</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157843&CFID=105753441&CFTOKEN=50690582">Human-agent-robot teamwork</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81500658285&CFID=105753441&CFTOKEN=50690582">Jeffrey M. Bradshaw</a>, 
                        <a href="author_page.cfm?id=81500647905&CFID=105753441&CFTOKEN=50690582">Virginia Dignum</a>, 
                        <a href="author_page.cfm?id=81500660459&CFID=105753441&CFTOKEN=50690582">Catholijn M. Jonker</a>, 
                        <a href="author_page.cfm?id=81100440761&CFID=105753441&CFTOKEN=50690582">Maarten Sierhuis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 487-488</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157843" title="DOI">10.1145/2157689.2157843</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157843&ftid=1161841&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow154" style="display:inline;"><br /><div style="display:inline">Teamwork has become a widely accepted metaphor for describing the nature of multi-robot and multi-agent cooperation. By virtue of teamwork models, team members attempt to manage general responsibilities and commitments to each other in a coherent fashion ...</div></span>
          <span id="toHide154" style="display:none;"><br /><div style="display:inline"><p>Teamwork has become a widely accepted metaphor for describing the nature of multi-robot and multi-agent cooperation. By virtue of teamwork models, team members attempt to manage general responsibilities and commitments to each other in a coherent fashion that both enhances performance and facilitates recovery when unanticipated problems arise. Whereas early research on teamwork focused mainly on interaction within groups of autonomous agents or robots, there is a growing interest in leveraging human participation effectively. Unlike autonomous systems designed primarily to take humans out of the loop, many important applications require people, agents, and robots to work together in close and relatively continuous interaction. For software agents and robots to participate in teamwork alongside people in carrying out complex real-world tasks, they must have some of the capabilities that enable natural and effective teamwork among groups of people. Just as important, developers of such systems need tools and methodologies to assure that such systems will work together reliably and safely, even when they have been designed independently.</p> <p>The purpose of the HART workshop is to explore theories, methods, and tools in support of humans, agents and robots working together in teams. Position papers that combine findings from fields such as computer science, artificial intelligence, cognitive science, anthropology, social and organizational psychology, human-computer interaction to address the problem of HART are strongly encouraged. The workshop will formulate perspectives on the current state-of-the-art, identify key challenges and opportunities for future studies, and promote community-building among researchers and practitioners.</p> <p>The workshop will be structured around four two-hour sessions on themes relevant to HART. Each session will consist of presentations and questions on selected position papers, followed by a whole-group discussion of the current state-of-the-art and the key challenges and research opportunities relevant to the theme. During the final hour, the workshop organizers will facilitate a discussion to determine next steps. The workshop will be deemed a success when collaborative scientific projects for the coming year are defined, and publication venues are explored. For example, results from the most recent HART workshop (Lorentz Center, Leiden, The Netherlands, December 2010) will be reflected in a special issue of <i>IEEE Intelligent Systems</i> on HART that is slated to appear in January/February 2012.</p></div></span> <a id="expcoll154" href="JavaScript: expandcollapse('expcoll154',154)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157844&CFID=105753441&CFTOKEN=50690582">Advances in tactile sensing and touch based human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100036382&CFID=105753441&CFTOKEN=50690582">Giorgio Cannata</a>, 
                        <a href="author_page.cfm?id=81384594454&CFID=105753441&CFTOKEN=50690582">Fulvio Mastrogiovanni</a>, 
                        <a href="author_page.cfm?id=81309496112&CFID=105753441&CFTOKEN=50690582">Giorgio Metta</a>, 
                        <a href="author_page.cfm?id=81100146701&CFID=105753441&CFTOKEN=50690582">Lorenzo Natale</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 489-490</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157844" title="DOI">10.1145/2157689.2157844</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157844&ftid=1161842&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow155" style="display:inline;"><br /><div style="display:inline">The problem of "providing robots with the sense of touch" is fundamental in order to develop the next generations of robots capable of interacting with humans in different contexts: in daily housekeeping activities, as working partners or as caregivers, ...</div></span>
          <span id="toHide155" style="display:none;"><br /><div style="display:inline"><p>The problem of "providing robots with the sense of touch" is fundamental in order to develop the next generations of robots capable of interacting with humans in different contexts: in daily housekeeping activities, as working partners or as caregivers, just to name a few.</p> <p>In a low-level perspective, through tactile sensing it is possible to measure or estimate physical properties of manipulated or touched objects, whereas feedback from tactile sensors may enable the detection and safe control of the interaction between the robot and objects or humans. In a high-level perspective, touch-based cognitive processes can be entailed by developing robot body self-awareness capabilities and by differentiating the "self" from the "external space", thereby opening new relevant problems in Robotics.</p> <p>The objective of this Workshop is to present and discuss the most recent achievements in the area of tactile sensing starting from the technological aspects, up to the application problems where tactile feedback plays a fundamental role.</p> <p>The Workshop will cover, but will not be limited, to the following three areas: <ol> <li>Technological aspects of robot artificial skin design and implementation including advanced transduction devices, large-scale sensing technologies, embedded electronics, system level solutions, etc.</li> <li>Software and algorithmic aspects related to tactile data processing: software engineering, robot control, touch-based reactive behaviors, touch classification, object recognition, etc.</li> <li>Cognitive issues related, but not limited, to skin-based behaviors and task level control, including: human-robot interaction, learning and assistive technologies, etc.</li></ol></p></div></span> <a id="expcoll155" href="JavaScript: expandcollapse('expcoll155',155)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157845&CFID=105753441&CFTOKEN=50690582">Gaze in HRI: from modeling to communication</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Frank Broz, Hagen Lehmann, Yukiko Nakano, Bilge Mutlu 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 491-492</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157845" title="DOI">10.1145/2157689.2157845</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157845&ftid=1161843&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow156" style="display:inline;"><br /><div style="display:inline">The purpose of this half-day workshop is to explore the role of social gaze in human-robot interaction, both how to measure social gaze behavior by humans and how to implement it in robots that interact with them. Gaze directed at an interaction partner ...</div></span>
          <span id="toHide156" style="display:none;"><br /><div style="display:inline"><p>The purpose of this half-day workshop is to explore the role of social gaze in human-robot interaction, both how to measure social gaze behavior by humans and how to implement it in robots that interact with them. Gaze directed at an interaction partner has become a subject of increased attention in human-robot interaction research. While traditional robotics research has focused work on robot gaze solely on the identification and manipulation of objects, researchers in HRI have come to recognize that gaze is a social behavior in addition to a way of sensing the world. This workshop will approach the problem of understanding the role of social gaze in human-robot interaction from the dual perspectives of investigating human-human gaze for design principles to apply to robots and of experimentally evaluating human-robot gaze interaction in order to assess how humans engage in gaze behavior with robots.</p> <p>Computational modeling of human gaze behavior is useful for human-robot interaction in a number of different ways. Such models can enable a robot to perceive information about the state of the human in the interaction and adjust its behavior accordingly. Additionally, more human-like gaze behavior may make a person more comfortable and engaged during an interaction. It is known the gaze pattern of a social interaction partner has a huge impact on one's own interaction behavior. Therefore, the experimental verification of robot gaze policies is extremely important. Appropriate gaze behavior is critical for establishing joint attention, which enables humans to engage in collaborative activities and gives structure to social interactions. There is still much to be learned about which properties of human-human gaze should be transferred to human-robot gaze and how to model human-robot gaze for autonomous robots. The goal of the workshop is to exchange ideas and develop and improve methodologies for this growing area of research.</p></div></span> <a id="expcoll156" href="JavaScript: expandcollapse('expcoll156',156)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157846&CFID=105753441&CFTOKEN=50690582">ROS and Rosbridge: roboticists out of the loop</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Christopher Crick, Graylin Jay, Sarah Osentoski, Odest Chadwicke Jenkins 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 493-494</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157846" title="DOI">10.1145/2157689.2157846</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157846&ftid=1161844&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow157" style="display:inline;"><br /><div style="display:inline">The advent of ROS, the Robot Operating System, has finally made it possible to implement and use state-of-the-art navigation and manipulation algorithms on widely-available, inexpensive standard robot platforms. With the addition of the Rosbridge application ...</div></span>
          <span id="toHide157" style="display:none;"><br /><div style="display:inline"><p>The advent of ROS, the Robot Operating System, has finally made it possible to implement and use state-of-the-art navigation and manipulation algorithms on widely-available, inexpensive standard robot platforms. With the addition of the Rosbridge application programming interface, interface designers and applications programmers can create robot interfaces and behaviors without venturing into the specialized world of robotics engineers. This tutorial introduces ROS and Rosbridge, and shows how quickly and easily these tools can be used to design and conduct large-scale online HRI experiments, access algorithms for autonomous robot behavior, and leverage the huge ecosystem of general-purpose web-based and application-oriented software engineering for robotics and HRI research. Tutorial attendees will learn the basics of autonomous and teleoperated navigation and manipulation, as well as interface design for online interaction with robots. During the tutorial they will design and write their own remote presence application, as well as develop strategies for incorporating autonomy and dealing with data collection.</p></div></span> <a id="expcoll157" href="JavaScript: expandcollapse('expcoll157',157)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=2157847&CFID=105753441&CFTOKEN=50690582">Cognitive science and socio-cognitive theoryfor the HRI practitioner</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          Jeffrey M. Bradshaw, J. Chris Forsythe 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 495-496</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/2157689.2157847" title="DOI">10.1145/2157689.2157847</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=2157847&ftid=1161863&dwn=1&CFID=105753441&CFTOKEN=50690582" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow158" style="display:inline;"><br /><div style="display:inline">This tutorial provides a synopsis of key findings and theoretical advances from cognitive science and socio-cognitive theory, with examples of how the results of this research can be applied to the design of human-robotic systems. Topics covered will ...</div></span>
          <span id="toHide158" style="display:none;"><br /><div style="display:inline"><p>This tutorial provides a synopsis of key findings and theoretical advances from cognitive science and socio-cognitive theory, with examples of how the results of this research can be applied to the design of human-robotic systems. Topics covered will run the gamut from basic cognitive science (e.g., perception, attention, learning and memory, information processing, multi-tasking, conscious awareness, individual differences) to socio-cognitive issues (e.g., theories of social interaction, dynamic functional allocation, mixed-initiative interaction, human-agent-robot teamwork, coactive design, theory of organizations). Additionally, the tutorial will address new technologies that attempt to leverage the current state of theory (e.g., neuroergonomics, brain-machine interfaces, detection of cognitive states, robotic prostheses and orthotics, cognitive and sensory prostheses). Throughout the tutorial, the presenters will give descriptions and demonstrations of working systems that exemplify the principles being taught. Separately, the presenters have given highly-successful tutorials on relevant subjects at workshops and conferences such as CHI and HCI International, as well as in a variety of industrial and government settings. In this tutorial, they propose to bring together their experience to bear on issues of specific interest to the HRI community.</p></div></span> <a id="expcoll158" href="JavaScript: expandcollapse('expcoll158',158)">expand</a>
          </div>
		  </td>
          </tr>          
          
</table>


</div> 
</div>


 <p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>



 <br />
<div class="footerbody" align="center" >
	

	The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2012 ACM, Inc.<br />
	<a href="http://www.acm.org/publications/policies/usage">Terms of Usage</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;	  
	<a href="http://www.acm.org/about/contact-us">Contact Us</a>

<br /><br />
Useful downloads: 
<a href="http://www.adobe.com/products/acrobat/readstep2.html"><img src="http://dl.acm.org/images/pdf_logo.gif" width="16" height="16" alt="" border="0" /> Adobe Acrobat</a>
&nbsp;&nbsp;
<a href="http://www.apple.com/quicktime/download/" target="_blank"><img src="http://dl.acm.org/images/qtlogo.gif" width="16" height="16" alt="" border="0" /> QuickTime</a>
&nbsp;&nbsp;
<a href="http://www.microsoft.com/windows/windowsmedia/download/default.asp" target="_blank"><img src="http://dl.acm.org/images/wmv.gif" width="16" height="15" alt="" border="0" /> Windows Media Player</a>
&nbsp;&nbsp;
<a href="http://www.real.com/" target="_blank"><img src="http://dl.acm.org/images/realplayer.gif" width="20" height="18" alt="" border="0" /> Real Player</a>

</div> 



<div  id="cf_window1338242675259" class="yuiextdlg">
	
	<div  id="theguide_title" class="x-dlg-hd">
		The ACM Guide to Computing Literature
	 </div>
	<div  id="theguide_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242675262" class="yuiextdlg">
	
	<div  id="thetags_title" class="x-dlg-hd">
		All Tags
	 </div>
	<div  id="thetags_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242675265" class="yuiextdlg">
	
	<div  id="theformats_title" class="x-dlg-hd">
		Export Formats
	 </div>
	<div  id="theformats_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242675267" class="yuiextdlg">
	
	<div  id="theexplaination_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theexplaination_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242675269" class="yuiextdlg">
	
	<div  id="theservices_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theservices_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242675271" class="yuiextdlg">
	
	<div  id="savetobinder_title" class="x-dlg-hd">
		Save to Binder
	 </div>
	<div  id="savetobinder_body" class="x-dlg-bd">
		
		
	 </div>
 </div> 

</body>
</html>