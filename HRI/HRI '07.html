


<!doctype html>


<head><script type="text/javascript">_cf_loadingtexthtml="<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";
_cf_jsonprefix='//';
_cf_clientid='BC81547966A7B857C9A284A14BFDE261';</script><script type="text/javascript" src="/CFIDE/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfform.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/masks.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/FCKeditor/fckeditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/adapter/yui/ext-yui-adapter.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/ext-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/resizable.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dragdrop/dragdrop.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/util.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/build/state/State-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/widget-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dialog/dialogs.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/cf/cf.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />



<title>Proceedings of the ACM/IEEE international conference on Human-robot interaction</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
	
  --></style>
 


<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>


<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  function setDiv() {
	var m = document.getElementById('divmain');
	var mh = m.offsetHeight;
	var t = document.getElementById('divtools');
	var th = t.offsetHeight;
	var tg = document.getElementById('divtags');
	var tgh = tg.offsetHeight;
	var calcheight = mh - th;
	if (tgh > calcheight  ){
	  var x = (th + tgh) - mh;
	  if ( (th + tgh) - mh < 65) {
	  }
	  else {
		 document.getElementById('divtags').innerHTML = ""; 
		 var tg = document.getElementById('divtags');
		 var tgh = tg.offsetHeight;
		 tg.style.height = tgh  + 'px';
	  }
	}
	else {
		tg.style.height = calcheight + 'px';
		document.getElementById('divtags').innerHTML = "";
	}

//  do I need to check after I resize to be sure I didn't go too big?
//	var tg2 = document.getElementById('divtags');
//	var tgh2 = tg.offsetHeight;	
//	if (tgh2 > mh + 65) {
//	  var y = mh + 65;
//	  alert('expanded too much ' + tgh2 + ' should be at most ' + y);
//	  tg.style.height = y + 'px';
//	  document.getElementById('divtags').innerHTML = "";
//	}

  }
</script>

<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>



<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Breazeal, Cynthia; General Chair-Schultz, Alan C.; Program Chair-Fong, Terry; Program Chair-Kiesler, Sara"> <meta name="citation_title" content="Proceedings of the ACM/IEEE international conference on Human-robot interaction"> <meta name="citation_date" content="03/10/2007"> <meta name="citation_isbn" content="978-1-59593-617-2"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1228716"> 



<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFFORM');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFDIV');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFTEXTAREA');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFWINDOW');
</script>

<script type="text/javascript">
	var _cf_window_init_1338242039857=function()
	{
		_cf_bind_init_1338242039858=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide_body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242039858);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window1338242039856', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242039857);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242039860=function()
	{
		_cf_bind_init_1338242039861=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags_body','bindExpr':['showthetags.cfm?id=1228716']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242039861);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1228716',{ modal:false, closable:true, divid:'cf_window1338242039859', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242039860);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242039863=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window1338242039862', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242039863);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242039865=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window1338242039864', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242039865);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242039867=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window1338242039866', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242039867);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242039869=function()
	{
		_cf_bind_init_1338242039870=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder_body','bindExpr':['savetobinder.cfm?id=1228716']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242039870);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1228716',{ modal:false, closable:true, divid:'cf_window1338242039868', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242039869);
</script>
</head>

<body style="text-align:center" onLoad="window.focus();">

<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'http://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
                        
<script src='AC_RunActiveContent.js' type="text/javascript"></script>




<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>



<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
	
    <tr style="vertical-align:top">
		
		<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text"  ><img src="http://dl.acm.org/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
		</td>
        
        <td style="padding-left: 5px; padding-right:10px; padding-bottom:0px;" class="small-link-text">
        	<table style="width:100%; border-collapse:collapse; padding:0px">
			<tr><td style="text-align:center">
				
                            <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
                    
					</td>		
			</tr>
			</table> 
        </td>
		<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text">
			 <p style="margin-top:0px; margin-bottom:10px;">
					
                            <a href="https://dl.acm.org/signin.cfm?cfid=105752387&amp;cftoken=75645419" class="small-link-text" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
                            &nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm?cfid=105752387&amp;cftoken=75645419"  class="small-link-text" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
						
			 </p>
            
			<table style="padding: 5px; border-collapse:collapse; float:right">
				
				
                            
                            <tr>
                            <td class="small-link-text" style="text-align:right">
                            <form name="qiksearch" action="results.cfm?h=1&amp;cfid=105752387&amp;cftoken=75645419" method="post">
                           
                           
                            
                                     
                                    <span style="margin-left:0px"><label><input type="text" name="query" size="34" value=" " /></label>&nbsp;
                                    <input style="vertical-align:top;" type="image" alt="Search" name="Go" src="http://dl.acm.org/images/search_small.jpg" />
                                    
                                    </span>
							  </form>
                                </td>
                            </tr>
                          
				  
			</table>

			
			
		</td>

	</tr>
    
    
    <tr><td colspan="3" class="small-link-text" style="padding-bottom:5px; padding-top:0px; text-align:center">
		<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
         
         </td>
    </tr>
    </table>
	
<map name="port" id="port" > 
  <area shape="rect" coords="1,1,60,50" href="http://www.acm.org/" alt="ACM Home Page" />
  <area shape="rect" coords="65,1,275,68" href="http://dl.acm.org/dl.cfm?CFID=105752387&CFTOKEN=75645419" alt="ACM Digital Library Home Page" />
</map>

<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
	<tr style="vertical-align:top">
		<td style="padding-right:10px; text-align:left" class="small-link-text">
        	<div id="divmain" style="border:1px solid #356b20;">
				 
				<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
					
                    	<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;"><strong>Proceedings of the ACM/IEEE international conference on Human-robot interaction</strong></h1>
                        
                </div>
                
                  

<table class="medium-text" style="border-collapse:collapse; padding:0px;">

<col style="width:540px" />

<tr style="vertical-align:top">
  <td>
    <table style="border-collapse:collapse; padding:2px;" class="medium-text">
      <col style="width:80px;" />
      <col style="width:auto" />
      <tr style="vertical-align:top">
        
      </tr>
    </table>

	
        <table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
            <col style="width:80px" />
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             General Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100258451&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752387&amp;cftoken=75645419" title="Author Profile Page" target="_self">Cynthia Breazeal</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1030230&CFID=105752387&CFTOKEN=75645419" title="Institutional Profile Page"><small>Massachusetts Institute of Technology, USA</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100622960&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752387&amp;cftoken=75645419" title="Author Profile Page" target="_self">Alan C. Schultz</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1016192&CFID=105752387&CFTOKEN=75645419" title="Institutional Profile Page"><small>Naval Research Laboratory, USA</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             Program Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81455605555&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752387&amp;cftoken=75645419" title="Author Profile Page" target="_self">Terry Fong</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1015919&CFID=105752387&CFTOKEN=75645419" title="Institutional Profile Page"><small>NASA Ames Research Center, USA</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100476487&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752387&amp;cftoken=75645419" title="Author Profile Page" target="_self">Sara Kiesler</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1029132&CFID=105752387&CFTOKEN=75645419" title="Institutional Profile Page"><small>Carnegie Mellon University, USA</small></a>
                      	
            </td>
            </tr>
            
        </table>
    

  </td>

  <td rowspan="20" nowrap="nowrap">
	<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
		<tr>
        	<td align="center" style="padding-bottom: 5px;">
			  
               <img src="http://portalparts.acm.org/1230000/1228716/thumb/1228716_thumb.jpg" title="Proceedings of the ACM/IEEE international conference on Human-robot interaction" height="70"  width="79" ALT="Proceedings of the ACM/IEEE international conference on Human-robot interaction" /> 
              </td>
              <td valign="top" align="left" nowrap="nowrap">
	             <img src="images/acm_mini.jpg" title="Published by ACM" alt="Published by ACM" /> 2007 Proceeding<br />
                 
        	 </td>
        </tr>
        
        <tr>
        	<td colspan="2" valign="baseline" style="padding-bottom:5px;">
            <img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
            <a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
            </td>
         </tr>
         <tr>
            <td  class="small-text" colspan="2" valign="top" style="padding-left:30px;">
				
	                    	&middot;&nbsp;Downloads (6 Weeks): 284<br />
    	                    &middot;&nbsp;Downloads (12 Months): 2,147<br />
                          
                        &middot;&nbsp;Citation Count: 292 
			</td>
         </tr>

	</table>
  </td>
</tr>
</table>

<br clear="all" />

<table border="0" class="medium-text" cellpadding="2" cellspacing="0">

<tr valign="top">
    <td colspan="3"><table border="0" class="medium-text" cellpadding="1" cellspacing="0">

<tr valign="top">
    <td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>

	<tr valign="top">
    	<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
	</tr>
    <tr valign="top">
	    <td style="padding-left:10px;">
		   <a href="http://www.hri2007.org" title="Conference Website"  target="_self" class="link-text">HRI'07</a> International Conference on Human Robot Interaction 
        </td>
	</tr>
    
    <tr valign="top">
	    <td style="padding-left:10px; padding-bottom:10px"> Arlington, VA, USA &mdash; March 08 - 10, 2007
                    
                  <br />
                    
                  <a href="http://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text">  &copy;2007</span> 
                  <br />           
                  
      </td>
	</tr>
	 

</table></td>
</tr>

</table>

                  
                 <br clear="all" />
			</div>
			
		</td>
		<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
	
            <div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
              <div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;"><strong>Tools and Resources</strong></h1></div>


<ul title="Tools and Resources" style="list-style: none; list-style-position:inside;
margin-left: 0px;
padding-left: 0em;
text-indent: 5px;
margin-bottom: 0px;">


<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:6px;">
   <span class="small-link-text">TOC Service:</span>
   	  
	  <img src="http://dl.acm.org/images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
      <ul style="margin-left: 0; padding-left: 0; display:inline;">
      	
        <li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
        <li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');"  class="small-link-text">RSS</a></li>
		        
      </ul>
    </span>
</li>

        <li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:6px;">
        <a href="citation.cfm?id=1228716&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
         </span></li>
    




<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Export Formats:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1228716&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1228716&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1228716&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
      </ul>
    </span>
</li>



 
   <li style="list-style-image:url(img/calbullet.jpg);margin-top:15px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Upcoming Conference:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px; margin-left:25px;"><a href="http://humanrobotinteraction.org/2013/" title="ACM/IEEE International Conference on Human-Robot Interaction" class="small-link-text">HRI'13</a></li>
      </ul>
    </span>
	</li>
    


</ul>           

  <!-- ADDTHIS BUTTON BEGIN -->
  
  <!-- ADDTHIS BUTTON END -->

<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>
  <!-- AddThis Button BEGIN -->



<!-- AddThis Button BEGIN -->
<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_slashdot"></a>
<a class="addthis_button_reddit"></a>


<span class="addthis_separator">|</span>
<a href="http://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>
<!-- AddThis Button END -->

  
 

  
  
            </div>
            
		</td>
	</tr>
    
</table>



</div>


<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">




<div id="fback" style="text-align:left; padding-top:10px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="feedback" border="0" /></a>
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design"><strong>Feedback</strong></a>

<span style="padding:10px;">|</span>




<span>Switch to <a href="citation.cfm?id=1228716&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>


</span>

 
<div class="small-text" style="margin-top:10px; margin-bottom:5px;"> 
<br />

    <a href="#abstract"  title="Abstract" style="padding:5px"><span>Abstract</span></a> |
    
    <a href="#formats"  title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
    
    <a href="#authors"  title="Authors" style="padding:5px"><span>Authors</span></a> |
    <a href="#references"  title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
    <a href="#citedby"  title="Cited By" style="padding:5px"><span>Cited By</span></a> |
    <a href="#indexterms"  title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
    <a href="#source"  title="Publication" style="padding:5px"><span>Publication</span></a> |
    <a href="#revs"  title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |               
	<a href="#comments"  title="Comments" style="padding:5px"><span>Comments</span></a>
	
     |               
	<a href="#prox"  title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
    
</div>
    
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;"/>



</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
       	
			<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

			

		
			
           
			
				
				<p>
					<div style="display:inline">It is our great pleasure to welcome you to the 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI 2007). HRI is a highly selective annual conference that seeks to showcase the very best research and thinking in human-robot interaction. Human-robot interaction is inherently inter-disciplinary, and the conference sought papers from researchers in robotics, human-factors, ergonomics, human-computer interaction, cognitive psychology, and other fields. The mission of the conference is to create a common venue for this broad set of researchers.This year's conference theme is "Robot as Team Member". Robots are used in such critical domains as search and rescue, military theater, mine and bomb detection, scientific exploration, law enforcement, and hospital care. Such robots must coordinate their behaviors with human team members; they are more than mere tools but rather quasi-team members whose tasks have to be integrated with those of humans. HRI 2007 is dedicated to these and other issues in human and robot interaction, highlighting the importance of building core science and understanding the social and technical issues in human-robot interaction in the context of teams and groups.Of the 93 submissions, the program committee accepted 22 papers and 26 posters that cover a variety of topics, among them field studies of robots in public spaces, operator-robot rescue teams, attributions of robot behavior, and human-robot dialogue. The program includes paper presentations, a video session, two interactive poster sessions, panels on robots in teams and the future of HRI research, and keynote speeches by human teamwork expert, J. Richard Hackman of Harvard, and by Hiroshi Ishiguro of Osaka University and ATR. We hope that these proceedings will serve as a valuable reference for HRI researchers and students.</div>
				</p>
   				
           	</div>
			
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


  <div class="abstract">
        <SPAN><strong>FRONT MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1230000/1228716/fm/frontmatter.pdf?ip=188.194.239.219&CFID=105752387&CFTOKEN=75645419" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(title page, copyright, foreword, contents, organization) 
  </div>          
  
  <div style="margin-top: 10px;"  class="abstract">
        <SPAN><strong>BACK MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1230000/1228716/bm/backmatter.pdf?ip=188.194.239.219&CFID=105752387&CFTOKEN=75645419" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(author index) 
  </div>          
  
<div style="margin-top: 10px; height: auto; padding: 5px; ">
		
		
        


	</div>

<br clear="all" />
</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<dl title="Authors" style="margin-top:0px">




<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 General Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Cynthia Breazeal" href="author_page.cfm?id=81100258451&CFID=105752387&CFTOKEN=75645419">Cynthia Breazeal</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1999-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">60</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">552</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">28</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">201</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">1,620</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Cynthia Breazeal" href="author_page.cfm?id=81100258451&amp;dsp=coll&amp;trk=1&amp;CFID=105752387&CFTOKEN=75645419" target="_self">View colleagues</a> of Cynthia Breazeal
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Alan C. Schultz" href="author_page.cfm?id=81100622960&CFID=105752387&CFTOKEN=75645419">Alan C. Schultz</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1986-2009</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">32</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">220</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">5</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">54</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">545</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Alan C. Schultz" href="author_page.cfm?id=81100622960&amp;dsp=coll&amp;trk=1&amp;CFID=105752387&CFTOKEN=75645419" target="_self">View colleagues</a> of Alan C. Schultz
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              



<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 Program Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Terry Fong" href="author_page.cfm?id=81455605555&CFID=105752387&CFTOKEN=75645419">Terry Fong</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">2001-2010</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">14</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">102</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">5</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">56</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">554</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Terry Fong" href="author_page.cfm?id=81455605555&amp;dsp=coll&amp;trk=1&amp;CFID=105752387&CFTOKEN=75645419" target="_self">View colleagues</a> of Terry Fong
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="http://portalparts.acm.org/profiles/81100476487/Kiesler2001.jpg" border="0" align="middle" alt="Sara Kiesler" hspace="5" />
		 
		  
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" >
	<span class="small-text"><strong><a title="author page of Sara Kiesler" href="author_page.cfm?id=81100476487&CFID=105752387&CFTOKEN=75645419">Sara Kiesler</a></strong><br /></span>
	
	<span class="small-text"><br /><a href="http://www.cs.cmu.edu/~kiesler/">homepage</a></span>
	
	<div style="margin-top: 6px" class="small-text">kiesler<img src="gifs/at.gif" width="12" height="12" alt="at" />cs.cmu.edu</div>
	
	
	
	<span class="small-text">
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1985-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">90</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">1,142</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">61</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">609</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">4,325</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Sara Kiesler" href="author_page.cfm?id=81100476487&amp;dsp=coll&amp;trk=1&amp;CFID=105752387&CFTOKEN=75645419" target="_self">View colleagues</a> of Sara Kiesler
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              

</dl>
</div>

		  
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	References are not available

</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

			
			
			
			<table cellpadding="5">
			
				<tr valign="top">
				<td valign="top">
					
					&nbsp;
				</td>
				<td>
					<div>
						
						<a href="citation.cfm?id=1348100&CFID=105752387&CFTOKEN=75645419">
						Michael A. Goodrich , Alan C. Schultz, Human-robot interaction: a survey, Foundations and Trends in Human-Computer Interaction, v.1 n.3, p.203-275, January 2007
						</a>
					</div>  
					
				</td>
				</tr>
			
			</table>
			
 </div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

Index Terms are not available


</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">



<table border="0" class="medium-text" cellpadding="0" cellspacing="5">



    <tr valign="top">
    	<td>Title</td> 
	    <td>
		   <a href="http://www.hri2007.org" title="Conference Website"  target="_self" class="link-text">HRI'07</a> International Conference on Human Robot Interaction 
        </td>
	</tr>
    <tr><td></td><td>Arlington, VA, USA &mdash; March 08 - 10, 2007</td></tr> <tr><td>Pages</td><td>382</td></tr> 
                 <tr>
                 
                     <td>Sponsors</td>
                    
                  <td>
                  <a href="sig.cfm?id=SP918&CFID=105752387&CFTOKEN=75645419"> SIGART</a> ACM Special Interest Group on Artificial Intelligence
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="sig.cfm?id=SP923&CFID=105752387&CFTOKEN=75645419"> SIGCHI</a> ACM Special Interest Group on Computer-Human Interaction
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="http://www.acm.org/"> ACM</a> Association for Computing Machinery
                  </td>
                  </tr>
              
                  <tr><td>Publisher</td><td><a href="http://www.acm.org/publications">ACM</a> New York, NY, USA</td>
				  </tr>
             <tr><td>ISBN</td><td> 978-1-59593-617-2</td></tr> <tr><td>Order Number</td><td>609074</td></tr> 
			<tr valign="top">
        	<td>Conference</td>
            <td valign="top" align="left"  style="padding-bottom: 25px;">
	            <strong style="padding-right:10px">HRI</strong><a href="event.cfm?id=RE285&CFID=105752387&CFTOKEN=75645419" title="ACM/IEEE International Conference on Human-Robot Interaction">ACM/IEEE International Conference on Human-Robot Interaction</a>
                
                       
                        <a href="event.cfm?id=RE285&CFID=105752387&CFTOKEN=75645419" title="ACM/IEEE International Conference on Human-Robot Interaction"><img border="0" src="http://portalparts.acm.org/event_logos/677/677.jpg" title="HRI logo" height="62"  width="100" ALT="HRI logo" style="vertical-align:top"></a>
						 

        	 </td>
            </tr>
		    <tr><td colspan="2">Paper Acceptance Rate 22 of 101 submissions, 22%</td></tr> <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 227 of 905 submissions, 25%</td></tr>
                       <tr valign="top">
                        <td colspan="2" style="padding-left:25px;">
                        	<table>
                            	<tr><td>
                                        <!-- WebCharts3D v5.1(2077) -->
<IMG SRC="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=Images/2183229730525688.JPG" id="Images_2183229730525688_JPG" name="Images_2183229730525688_JPG" usemap="#Images_2183229730525688_JPG_map" border="0"/>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAB' id='GP1338242040402AAAB'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>140</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAC' id='GP1338242040402AAAC'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>41</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAD' id='GP1338242040402AAAD'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>101</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAE' id='GP1338242040402AAAE'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>22</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAF' id='GP1338242040402AAAF'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>134</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAG' id='GP1338242040402AAAG'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>48</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAH' id='GP1338242040402AAAH'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>120</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAI' id='GP1338242040402AAAI'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>23</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAJ' id='GP1338242040402AAAJ'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>124</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAK' id='GP1338242040402AAAK'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>26</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAL' id='GP1338242040402AAAL'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>149</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAM' id='GP1338242040402AAAM'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>33</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAN' id='GP1338242040402AAAN'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>137</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242040402AAAO' id='GP1338242040402AAAO'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>34</td></tr></table>
<MAP name='Images_2183229730525688_JPG_map'>
<AREA shape='rect' coords='0,0,1,1'/>
<AREA shape="rect" coords="293,179,309,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAO",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAO",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAO",event)'/>
<AREA shape="rect" coords="277,74,293,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAN",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAN",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAN",event)'/>
<AREA shape="rect" coords="253,180,269,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAM",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAM",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAM",event)'/>
<AREA shape="rect" coords="237,62,253,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAL",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAL",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAL",event)'/>
<AREA shape="rect" coords="213,187,229,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAK",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAK",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAK",event)'/>
<AREA shape="rect" coords="197,87,213,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAJ",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAJ",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAJ",event)'/>
<AREA shape="rect" coords="173,190,189,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAI",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAI",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAI",event)'/>
<AREA shape="rect" coords="157,91,173,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAH",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAH",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAH",event)'/>
<AREA shape="rect" coords="133,165,149,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAG",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAG",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAG",event)'/>
<AREA shape="rect" coords="117,77,133,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAF",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAF",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAF",event)'/>
<AREA shape="rect" coords="93,191,109,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAE",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAE",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAE",event)'/>
<AREA shape="rect" coords="77,111,93,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAD",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAD",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAD",event)'/>
<AREA shape="rect" coords="53,172,69,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAC",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAC",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAC",event)'/>
<AREA shape="rect" coords="37,71,53,213" onMouseover='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAB",event,true)' onMouseout='xx_set_visible("Images_2183229730525688_JPG","GP1338242040402AAAB",event,false)' onMousemove='xx_move_tag("Images_2183229730525688_JPG","GP1338242040402AAAB",event)'/>
<AREA shape="rect" coords="160,13,227,27"/>
<AREA shape="rect" coords="89,13,160,27"/>
</MAP>

<script language="javascript" src="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=script.js"></script>

                                      </td>
                                      <td style="padding-left:20px;">
                                             <table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
                                                <tr bgcolor="#ffffff">
                                                  <th style="width:50%">Year</th>
                                                  <th  align="right" style="width:15%">Submitted</th>
                                                  <th  align="right" style="width:15%">Accepted</th>
                                                  <th  align="center">Rate</th>
                                                </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '06</td>
                                                            <td align="right">140</td>
                                                            <td align="right">41</td>
                                                            <td align="center">29%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '07</td>
                                                            <td align="right">101</td>
                                                            <td align="right">22</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '08</td>
                                                            <td align="right">134</td>
                                                            <td align="right">48</td>
                                                            <td align="center">36%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '09</td>
                                                            <td align="right">120</td>
                                                            <td align="right">23</td>
                                                            <td align="center">19%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '10</td>
                                                            <td align="right">124</td>
                                                            <td align="right">26</td>
                                                            <td align="center">21%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '11</td>
                                                            <td align="right">149</td>
                                                            <td align="right">33</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '12</td>
                                                            <td align="right">137</td>
                                                            <td align="right">34</td>
                                                            <td align="center">25%</td>
                                                         </tr>
                                                
                                                 <tr bgcolor="#ffffff">
                                                    <td><strong>Overall</strong></td>
                                                    <td align="right">905</td>
                                                    <td align="right">227</td>
                                                    <td align="center">25%</td>
                                                  </tr>
                                                </table>
                                       </td>
                                     </tr>
                               </table>
                        </td>
                    </tr>
                     
                     
            
</table>


</table>




</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	<br />Reviews are not available for this item
        
        <div align="left" style="margin-top:30px">
					<a title="Computing Reviews" href="ocr_review_main.cfm?CFID=105752387&CFTOKEN=75645419">
                 <img src="http://dl.acm.org/images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
        
        
       		<ul style="list-style:disc; display:inline-block">
	            <li>Access <a href="ocr_review_main.cfm?CFID=105752387&CFTOKEN=75645419" target="_blank">critical reviews</a> of computing literature.</li>
            	<li><a href="http://www.computingreviews.com/Reviewer/"  target="_blank">Become a reviewer</a> for Computing Reviews</li>
            </ul>
        </div>
        
</div>



<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
         
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<div>
<div>
	
	<p style="margin-left:5px;">
    <strong>Be the first to comment</strong>
    	
          	To Post a comment please <a href="signin.cfm?CFID=105752387&CFTOKEN=75645419">sign in or create</a> a free Web account</a>
        
    
    
	 </p>
	   	
 
</div>


</div>

	
		<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
        
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" >

<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the ACM/IEEE international conference on Human-robot interaction</h5>


<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>

<div style="clear:both">
    
        <div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1121241&picked=prox&CFID=105752387&CFTOKEN=75645419" title="previous: HRI '06"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1349822&picked=prox&CFID=105752387&CFTOKEN=75645419" title="Next: HRI '08">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
        
</div>


 
<table class="text12" border="0">

  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Full papers</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228718&CFID=105752387&CFTOKEN=75645419">Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100537802&CFID=105752387&CFTOKEN=75645419">Guy Hoffman</a>, 
                        <a href="author_page.cfm?id=81100258451&CFID=105752387&CFTOKEN=75645419">Cynthia Breazeal</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 1-8</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228718" title="DOI">10.1145/1228716.1228718</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228718&ftid=407189&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow2" style="display:inline;"><br /><div style="display:inline">A crucial skill for fluent action meshing in human team activity is a learned and calculated selection of anticipatory actions. We believe that the same holds for robotic teammates, if they are to perform in a similarly fluent manner with their human ...</div></span>
          <span id="toHide2" style="display:none;"><br /><div style="display:inline">A crucial skill for fluent action meshing in human team activity is a learned and calculated selection of anticipatory actions. We believe that the same holds for robotic teammates, if they are to perform in a similarly fluent manner with their human counterparts.In this work, we propose an adaptive action selection mechanism for a robotic teammate, making anticipatory decisions based on the confidence of their validity and their relative risk. We predict an improvement in task efficiency and fluency compared to a purely reactive process.We then present results from a study involving untrained human subjects working with a simulated version of a robot using our system. We show a significant improvement in best-case task efficiency when compared to a group of users working with a reactive agent, as well as a significant difference in the perceived commitment of the robot to the team and its contribution to the team's uency and success. By way of explanation, we propose a number of fluency metrics that differ significantly between the two study groups.</div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228719&CFID=105752387&CFTOKEN=75645419">Human control for cooperating robot teams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81450595190&CFID=105752387&CFTOKEN=75645419">Jijun Wang</a>, 
                        <a href="author_page.cfm?id=81100349943&CFID=105752387&CFTOKEN=75645419">Michael Lewis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 9-16</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228719" title="DOI">10.1145/1228716.1228719</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228719&ftid=407190&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow3" style="display:inline;"><br /><div style="display:inline">Human control of multiple robots has been characterized by the average demand of single robots on human attention or the distribution of demands from multiple robots. When robots are allowed to cooperate autonomously, however, demands on the operator ...</div></span>
          <span id="toHide3" style="display:none;"><br /><div style="display:inline">Human control of multiple robots has been characterized by the average demand of single robots on human attention or the distribution of demands from multiple robots. When robots are allowed to cooperate autonomously, however, demands on the operator should be reduced by the amount previously required to coordinate their actions. The present experiment compares control of small robot teams in which cooperating robots explored autonomously, were controlled independently by an operator or through mixed initiative as a cooperating team. Mixed initiative teams found more victims and searched wider areas than either fully autonomous or manually controlled teams. Operators who switched attention between robots more frequently were found to perform better in both manual and mixed initiative conditions.</div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228720&CFID=105752387&CFTOKEN=75645419">Natural person-following behavior for social robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81311485201&CFID=105752387&CFTOKEN=75645419">Rachel Gockley</a>, 
                        <a href="author_page.cfm?id=81100492013&CFID=105752387&CFTOKEN=75645419">Jodi Forlizzi</a>, 
                        <a href="author_page.cfm?id=81332527865&CFID=105752387&CFTOKEN=75645419">Reid Simmons</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 17-24</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228720" title="DOI">10.1145/1228716.1228720</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228720&ftid=407191&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow4" style="display:inline;"><br /><div style="display:inline">We are developing robots with socially appropriate spatial skills not only to travel around or near people, but also to accompany people side-by-side. As a step toward this goal, we are investigating the social perceptions of a robot's movement as it ...</div></span>
          <span id="toHide4" style="display:none;"><br /><div style="display:inline">We are developing robots with socially appropriate spatial skills not only to travel around or near people, but also to accompany people side-by-side. As a step toward this goal, we are investigating the social perceptions of a robot's movement as it follows behind a person. This paper discusses our laser-based person-tracking method and two different approaches to person-following: direction-following and path-following. While both algorithms have similar characteristics in terms of tracking performance and following distances, participants in a pilot study rated the direction-following behavior as significantly more human-like and natural than the path-following behavior. We argue that the path-following method may still be more appropriate in some situations, and we propose that the ideal person-following behavior may be a hybrid approach, with the robot automatically selecting which method to use.</div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228721&CFID=105752387&CFTOKEN=75645419">Managing autonomy in robot teams: observations from four experiments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350575550&CFID=105752387&CFTOKEN=75645419">Michael A. Goodrich</a>, 
                        <a href="author_page.cfm?id=81100282434&CFID=105752387&CFTOKEN=75645419">Timothy W. McLain</a>, 
                        <a href="author_page.cfm?id=81325487662&CFID=105752387&CFTOKEN=75645419">Jeffrey D. Anderson</a>, 
                        <a href="author_page.cfm?id=81452594332&CFID=105752387&CFTOKEN=75645419">Jisang Sun</a>, 
                        <a href="author_page.cfm?id=81309483016&CFID=105752387&CFTOKEN=75645419">Jacob W. Crandall</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 25-32</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228721" title="DOI">10.1145/1228716.1228721</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228721&ftid=407192&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow5" style="display:inline;"><br /><div style="display:inline">It is often desirable for a human to manage multiple robots. Autonomy is required to keep workload within tolerable ranges, and dynamically adapting the type of autonomy may be useful for responding to environment and workload changes. We identify two ...</div></span>
          <span id="toHide5" style="display:none;"><br /><div style="display:inline">It is often desirable for a human to manage multiple robots. Autonomy is required to keep workload within tolerable ranges, and dynamically adapting the type of autonomy may be useful for responding to environment and workload changes. We identify two management styles for managing multiple robots and present results from four experiments that have relevance to dynamic autonomy within these two management styles. These experiments, which involved 80 subjects, suggest that individual and team autonomy benefit from attention management aids, adaptive autonomy, and proper information abstraction.</div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228722&CFID=105752387&CFTOKEN=75645419">Developing performance metrics for the supervisory control of multiple robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309483016&CFID=105752387&CFTOKEN=75645419">Jacob W. Crandall</a>, 
                        <a href="author_page.cfm?id=81325487898&CFID=105752387&CFTOKEN=75645419">M. L. Cummings</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 33-40</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228722" title="DOI">10.1145/1228716.1228722</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228722&ftid=407193&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow6" style="display:inline;"><br /><div style="display:inline">Efforts are underway to make it possible for a single operator to effectively control multiple robots. In these high workload situations, many questions arise including how many robots should be in the team (Fan-out), what level of autonomy should the ...</div></span>
          <span id="toHide6" style="display:none;"><br /><div style="display:inline">Efforts are underway to make it possible for a single operator to effectively control multiple robots. In these high workload situations, many questions arise including how many robots should be in the team (Fan-out), what level of autonomy should the robots have, and when should this level of autonomy change (i.e., dynamic autonomy). We propose that a set of metric classes should be identified that can adequately answer these questions. Toward this end, we present a potential set of metric classes for human-robot teams consisting of a single human operator and multiple robots. To test the usefulness and appropriateness of this set of metric classes, we conducted a user study with simulated robots. Using the data obtained from this study, we explore the ability of this set of metric classes to answer these questions.</div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228723&CFID=105752387&CFTOKEN=75645419">Adapting GOMS to model human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100059315&CFID=105752387&CFTOKEN=75645419">Jill L. Drury</a>, 
                        <a href="author_page.cfm?id=81100426546&CFID=105752387&CFTOKEN=75645419">Jean Scholtz</a>, 
                        <a href="author_page.cfm?id=81100254452&CFID=105752387&CFTOKEN=75645419">David Kieras</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 41-48</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228723" title="DOI">10.1145/1228716.1228723</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228723&ftid=407194&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow7" style="display:inline;"><br /><div style="display:inline">A formal interaction modeling technique known as Goals, Operators, Methods, and Selection rules (GOMS) is well-established in human-computer interaction as a cost-effective way of evaluating designs without the participation of end users. This paper ...</div></span>
          <span id="toHide7" style="display:none;"><br /><div style="display:inline">A formal interaction modeling technique known as Goals, Operators, Methods, and Selection rules (GOMS) is well-established in human-computer interaction as a cost-effective way of evaluating designs without the participation of end users. This paper explores the use of GOMS for evaluating human-robot interaction. We provide a case study in the urban search-and-rescue domain and raise issues for developing GOMS models that have not been previously addressed. Further, we provide rationale for selecting different types of GOMS modeling techniques to help the analyst model human-robot interfaces.</div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228724&CFID=105752387&CFTOKEN=75645419">Interactive robot task training through dialog and demonstration</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100077697&CFID=105752387&CFTOKEN=75645419">Paul E. Rybski</a>, 
                        <a href="author_page.cfm?id=81325490382&CFID=105752387&CFTOKEN=75645419">Kevin Yoon</a>, 
                        <a href="author_page.cfm?id=81325490183&CFID=105752387&CFTOKEN=75645419">Jeremy Stolarz</a>, 
                        <a href="author_page.cfm?id=81100032034&CFID=105752387&CFTOKEN=75645419">Manuela M Veloso</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 49-56</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228724" title="DOI">10.1145/1228716.1228724</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228724&ftid=407195&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow8" style="display:inline;"><br /><div style="display:inline">Effective human/robot interfaces which mimic how humans interact with one another could ultimately lead to robots being accepted in a wider domain of applications. We present a framework for interactive task training of a mobile robot where the robot ...</div></span>
          <span id="toHide8" style="display:none;"><br /><div style="display:inline">Effective human/robot interfaces which mimic how humans interact with one another could ultimately lead to robots being accepted in a wider domain of applications. We present a framework for interactive task training of a mobile robot where the robot learns how to do various tasks while observing a human. In addition to observation, the robot listens to the human's speech and interprets the speech as behaviors that are required to be executed. This is especially important where individual steps of a given task may have contingencies that have to be dealt with depending on the situation. Finally, the context of the location where the task takes place and the people present factor heavily into the robot's interpretation of how to execute the task. In this paper, we describe the task training framework, describe how environmental context and communicative dialog with the human help the robot learn the task, and illustrate the utility of this approach with several experimental case studies.</div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228725&CFID=105752387&CFTOKEN=75645419">Learning by demonstration with critique from a human teacher</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310499255&CFID=105752387&CFTOKEN=75645419">Brenna Argall</a>, 
                        <a href="author_page.cfm?id=81100556983&CFID=105752387&CFTOKEN=75645419">Brett Browning</a>, 
                        <a href="author_page.cfm?id=81100032034&CFID=105752387&CFTOKEN=75645419">Manuela Veloso</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 57-64</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228725" title="DOI">10.1145/1228716.1228725</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228725&ftid=407196&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow9" style="display:inline;"><br /><div style="display:inline">Learning by demonstration can be a powerful and natural tool for developing robot control policies. That is, instead of tedious hand-coding, a robot may learn a control policy by interacting with a teacher. In this work we present an algorithm for learning ...</div></span>
          <span id="toHide9" style="display:none;"><br /><div style="display:inline">Learning by demonstration can be a powerful and natural tool for developing robot control policies. That is, instead of tedious hand-coding, a robot may learn a control policy by interacting with a teacher. In this work we present an algorithm for learning by demonstration in which the teacher operates in two phases. The teacher first demonstrates the task to the learner. The teacher next critiques learner performance of the task. This critique is used by the learner to update its control policy. In our implementation we utilize a 1-Nearest Neighbor technique which incorporates both training dataset and teacher critique. Since the teacher critiques performance only, they do not need to guess at an effective critique for the underlying algorithm. We argue that this method is particularly well-suited to human teachers, who are generally better at assigning credit to performances than to algorithms. We have applied this algorithm to the simulated task of a robot intercepting a ball. Our results demonstrate improved performance with teacher critiquing, where performance is measured by both execution success and efficiency.</div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228726&CFID=105752387&CFTOKEN=75645419">Efficient model learning for dialog management</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325487917&CFID=105752387&CFTOKEN=75645419">Finale Doshi</a>, 
                        <a href="author_page.cfm?id=81339525411&CFID=105752387&CFTOKEN=75645419">Nicholas Roy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 65-72</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228726" title="DOI">10.1145/1228716.1228726</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228726&ftid=407197&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow10" style="display:inline;"><br /><div style="display:inline">Intelligent planning algorithms such as the Partially Observable Markov Decision Process (POMDP) have succeeded in dialog management applications [10, 11, 12] because they are robust to the inherent uncertainty of human interaction. Like all dialog planning ...</div></span>
          <span id="toHide10" style="display:none;"><br /><div style="display:inline">Intelligent planning algorithms such as the Partially Observable Markov Decision Process (POMDP) have succeeded in dialog management applications [10, 11, 12] because they are robust to the inherent uncertainty of human interaction. Like all dialog planning systems, however, POMDPs require an accurate model of the user (e.g., what the user might say or want). POMDPs are generally specified using a large probabilistic model with many parameters. These parameters are difficult to specify from domain knowledge, and gathering enough data to estimate the parameters accurately a <i>priori</i> is expensive.In this paper, we take a Bayesian approach to learning the user model simultaneously with dialog manager policy. At the heart of our approach is an efficient incremental update algorithm that allows the dialog manager to replan just long enough to improve the current dialog policy given data from recent interactions. The update process has a relatively small computational cost, preventing long delays in the interaction. We are able to demonstrate a robust dialog manager that learns from interaction data, out-performing a hand-coded model in simulation and in a robotic wheelchair application.</div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228727&CFID=105752387&CFTOKEN=75645419">Using vision, acoustics, and natural language for disambiguation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325488400&CFID=105752387&CFTOKEN=75645419">Benjamin Fransen</a>, 
                        <a href="author_page.cfm?id=81317498196&CFID=105752387&CFTOKEN=75645419">Vlad Morariu</a>, 
                        <a href="author_page.cfm?id=81310501807&CFID=105752387&CFTOKEN=75645419">Eric Martinson</a>, 
                        <a href="author_page.cfm?id=81310500412&CFID=105752387&CFTOKEN=75645419">Samuel Blisard</a>, 
                        <a href="author_page.cfm?id=81310500818&CFID=105752387&CFTOKEN=75645419">Matthew Marge</a>, 
                        <a href="author_page.cfm?id=81325490675&CFID=105752387&CFTOKEN=75645419">Scott Thomas</a>, 
                        <a href="author_page.cfm?id=81100622960&CFID=105752387&CFTOKEN=75645419">Alan Schultz</a>, 
                        <a href="author_page.cfm?id=81100314905&CFID=105752387&CFTOKEN=75645419">Dennis Perzanowski</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 73-80</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228727" title="DOI">10.1145/1228716.1228727</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228727&ftid=407198&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow11" style="display:inline;"><br /><div style="display:inline">Creating a human-robot interface is a daunting experience. Capabilities and functionalities of the interface are dependent on the robustness of many different sensor and input modalities. For example, object recognition poses problems for state-of-the-art ...</div></span>
          <span id="toHide11" style="display:none;"><br /><div style="display:inline">Creating a human-robot interface is a daunting experience. Capabilities and functionalities of the interface are dependent on the robustness of many different sensor and input modalities. For example, object recognition poses problems for state-of-the-art vision systems. Speech recognition in noisy environments remains problematic for acoustic systems. Natural language understanding and dialog are often limited to specific domains and baffled by ambiguous or novel utterances. Plans based on domain-specific tasks limit the applicability of dialog managers. The types of sensors used limit spatial knowledge and understanding, and constrain cognitive issues, such as perspective-taking.In this research, we are integrating several modalities, such as vision, audition, and natural language understanding to leverage the existing strengths of each modality and overcome individual weaknesses. We are using visual, acoustic, and linguistic inputs in various combinations to solve such problems as the disambiguation of referents (objects in the environment), localization of human speakers, and determination of the source of utterances and appropriateness of responses when humans and robots interact. For this research, we limit our consideration to the interaction of two humans and one robot in a retrieval scenario. This paper will describe the system and integration of the various modules prior to future testing.</div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228728&CFID=105752387&CFTOKEN=75645419">To kill a mockingbird robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100461702&CFID=105752387&CFTOKEN=75645419">Christoph Bartneck</a>, 
                        <a href="author_page.cfm?id=81325490312&CFID=105752387&CFTOKEN=75645419">Marcel Verbunt</a>, 
                        <a href="author_page.cfm?id=81325489083&CFID=105752387&CFTOKEN=75645419">Omar Mubin</a>, 
                        <a href="author_page.cfm?id=81325487779&CFID=105752387&CFTOKEN=75645419">Abdullah Al Mahmud</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 81-87</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228728" title="DOI">10.1145/1228716.1228728</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228728&ftid=407199&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow12" style="display:inline;"><br /><div style="display:inline">Robots are being introduced in our society but their social status is still unclear. A critical issue is if the robot's exhibition of intelligent life-like behavior leads to the users' perception of animacy. The ultimate test for the life-likeness of ...</div></span>
          <span id="toHide12" style="display:none;"><br /><div style="display:inline">Robots are being introduced in our society but their social status is still unclear. A critical issue is if the robot's exhibition of intelligent life-like behavior leads to the users' perception of animacy. The ultimate test for the life-likeness of a robot is to kill it. We therefore conducted an experiment in which the robot's intelligence and the participants' gender were the independent variables and the users' destructive behavior of the robot the dependent variables. Several practical and methodological problems compromised the acquired data, but we can conclude that the robot's intelligence had a significant influence on the users' destructive behavior. We discuss the encountered problems and the possible application of this animacy measuring method.</div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228729&CFID=105752387&CFTOKEN=75645419">A dancing robot for rhythmic social interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310502486&CFID=105752387&CFTOKEN=75645419">Marek P. Michalowski</a>, 
                        <a href="author_page.cfm?id=81310500639&CFID=105752387&CFTOKEN=75645419">Selma Sabanovic</a>, 
                        <a href="author_page.cfm?id=81100620208&CFID=105752387&CFTOKEN=75645419">Hideki Kozima</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 89-96</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228729" title="DOI">10.1145/1228716.1228729</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228729&ftid=407200&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow13" style="display:inline;"><br /><div style="display:inline">This paper describes a robotic system that uses dance as a form of social interaction to explore the properties and importance of rhythmic movement in general social interaction. The system consists of a small creature-like robot whose movement is controlled ...</div></span>
          <span id="toHide13" style="display:none;"><br /><div style="display:inline">This paper describes a robotic system that uses dance as a form of social interaction to explore the properties and importance of rhythmic movement in general social interaction. The system consists of a small creature-like robot whose movement is controlled by a rhythm-based software system. Environmental rhythms can be extracted from auditory or visual sensory stimuli, and the robot synchronizes its movement to a dominant rhythm. The system was demonstrated, and an exploratory study conducted, with children interacting with the robot in a generalized dance task. Through a behavioral analysis of videotaped interactions, we found that the robot's synchronization with the background music had an effect on children's interactive involvement with the robot. Furthermore, we observed a number of expected and unexpected styles and modalities of interactive exploration and play that inform our discussion on the next steps in the design of a socially rhythmic robotic system.</div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228730&CFID=105752387&CFTOKEN=75645419">The interactive robotic percussionist: new developments in form, mechanics, perception and interaction design</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100638548&CFID=105752387&CFTOKEN=75645419">Gil Weinberg</a>, 
                        <a href="author_page.cfm?id=81100355389&CFID=105752387&CFTOKEN=75645419">Scott Driscoll</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 97-104</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228730" title="DOI">10.1145/1228716.1228730</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228730&ftid=407201&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow14" style="display:inline;"><br /><div style="display:inline">We present new developments in the improvisational robotic percussionist project, aimed at improving human-robot interaction through design, mechanics, and perceptual modeling. Our robot, named Haile, listens to live human players, analyzes perceptual ...</div></span>
          <span id="toHide14" style="display:none;"><br /><div style="display:inline">We present new developments in the improvisational robotic percussionist project, aimed at improving human-robot interaction through design, mechanics, and perceptual modeling. Our robot, named Haile, listens to live human players, analyzes perceptual aspects in their playing in real-time, and uses the product of this analysis to play along in a collaborative and improvisatory manner. It is designed to combine the benefits of computational power in algorithmic music with the expression and visual interactivity of acoustic playing. Haile's new features include an anthropomorphic form, a linear-motor based robotic arm, a novel perceptual modeling implementation, and a number of new interaction schemes. The paper begins with an overview of related work and a presentation of goals and challenges based on Haile's original design. We then describe new developments in physical design, mechanics, perceptual implementation, and interaction design, aimed at improving human-robot interactions with Haile. The paper concludes with a description of a user study, conducted in an effort to evaluate the new functionalities and their effectiveness in facilitating expressive musical human-robot interaction. The results of the study show correlation between human's and Haile's rhythmic perception as well as user satisfaction regarding Haile's perceptual and mechanical abilties. The study also indicates areas for improvement such as the need for better timbre and loudness control and more advance and responsive interaction schemes.</div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228731&CFID=105752387&CFTOKEN=75645419">Using proprioceptive sensors for categorizing human-robot interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414610058&CFID=105752387&CFTOKEN=75645419">T. Salter</a>, 
                        <a href="author_page.cfm?id=81100593688&CFID=105752387&CFTOKEN=75645419">F. Michaud</a>, 
                        <a href="author_page.cfm?id=81100185093&CFID=105752387&CFTOKEN=75645419">D. L&#233;tourneau</a>, 
                        <a href="author_page.cfm?id=81325488676&CFID=105752387&CFTOKEN=75645419">D. C. Lee</a>, 
                        <a href="author_page.cfm?id=81325490629&CFID=105752387&CFTOKEN=75645419">I. P. Werry</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 105-112</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228731" title="DOI">10.1145/1228716.1228731</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228731&ftid=407202&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow15" style="display:inline;"><br /><div style="display:inline">Increasingly researchers are looking outside of normal communication channels (such as video and audio) to provide additional forms of communication or interaction between a human and a robot, or a robot and its environment. Amongst the new channels ...</div></span>
          <span id="toHide15" style="display:none;"><br /><div style="display:inline">Increasingly researchers are looking outside of normal communication channels (such as video and audio) to provide additional forms of communication or interaction between a human and a robot, or a robot and its environment. Amongst the new channels being investigated is the detection of touch using infrared, proprioceptive and temperature sensors. Our work aims at developing a system that can detect natural touch or interaction coming from children playing with a robot, and adapt to this interaction. This paper reports trials carried out using Roball, a spherical mobile robot, demonstrating how sensory data patterns can be identified in human-robot interaction, and exploited for achieving behavioral adaptation. The experimental methodology used for these trials is reported, which validated the hypothesis that human interaction can not only be perceived from proprioceptive sensors on-board a robotic platform, but that this perception has the ability to lead to adaptation.</div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228732&CFID=105752387&CFTOKEN=75645419">Improving human-robot interaction through adaptation to the auditory scene</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310501807&CFID=105752387&CFTOKEN=75645419">Eric Martinson</a>, 
                        <a href="author_page.cfm?id=81100455302&CFID=105752387&CFTOKEN=75645419">Derek Brock</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 113-120</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228732" title="DOI">10.1145/1228716.1228732</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228732&ftid=407203&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow16" style="display:inline;"><br /><div style="display:inline">Effective communication with a mobile robot using speech is a difficult problem even when you can control the auditory scene. Robot ego-noise, echoes, and human interference are all common sources of decreased intelligibility. In real-world environments, ...</div></span>
          <span id="toHide16" style="display:none;"><br /><div style="display:inline">Effective communication with a mobile robot using speech is a difficult problem even when you can control the auditory scene. Robot ego-noise, echoes, and human interference are all common sources of decreased intelligibility. In real-world environments, however, these common problems are supplemented with many different types of background noise sources. For instance, military scenarios might be punctuated by high decibel plane noise and bursts from weaponry that mask parts of the speech output from the robot. Even in non-military settings, however, fans, computers, alarms, and transportation noise can cause enough interference that they might render a traditional speech interface unintelligible. In this work, we seek to overcome these problems by applying robotic advantages of sensing and mobility to a text-to-speech interface. Using perspective taking skills to predict how the human user is being affected by new sound sources, a robot can adjust its speaking patterns and/or reposition itself within the environment to limit the negative impact on intelligibility, making a speech interface easier to use.</div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228733&CFID=105752387&CFTOKEN=75645419">Group attention control for communication robots with wizard of OZ approach</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310499910&CFID=105752387&CFTOKEN=75645419">Masahiro Shiomi</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752387&CFTOKEN=75645419">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81325488901&CFID=105752387&CFTOKEN=75645419">Satoshi Koizumi</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752387&CFTOKEN=75645419">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752387&CFTOKEN=75645419">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 121-128</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228733" title="DOI">10.1145/1228716.1228733</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228733&ftid=407204&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow17" style="display:inline;"><br /><div style="display:inline">This paper describes a group attention control (GAC) system that enables a communication robot to simultaneously interact with many people. GAC is based on controlling social situations and indicating explicit control to unify all purposes of attention. ...</div></span>
          <span id="toHide17" style="display:none;"><br /><div style="display:inline">This paper describes a group attention control (GAC) system that enables a communication robot to simultaneously interact with many people. GAC is based on controlling social situations and indicating explicit control to unify all purposes of attention. We implemented a semi-autonomous GAC system into a communication robot that guides visitors to exhibits in a science museum and engages in free-play interactions with them. The GAC system's effectiveness was demonstrated in a two-week experiment in the museum. We believe these results will allow us to develope interactive humanoid robots that can interact effectively with groups of people.</div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228734&CFID=105752387&CFTOKEN=75645419">How robotic products become social products: an ethnographic study of cleaning in the home</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100492013&CFID=105752387&CFTOKEN=75645419">Jodi Forlizzi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 129-136</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228734" title="DOI">10.1145/1228716.1228734</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228734&ftid=407205&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow18" style="display:inline;"><br /><div style="display:inline">Robots that work with people foster social relationships between people and systems. The home is an interesting place to study the adoption and use of these systems. The home provides challenges from both technical and interaction perspectives. In addition, ...</div></span>
          <span id="toHide18" style="display:none;"><br /><div style="display:inline">Robots that work with people foster social relationships between people and systems. The home is an interesting place to study the adoption and use of these systems. The home provides challenges from both technical and interaction perspectives. In addition, the home is a seat for many specialized human behaviors and needs, and has a long history of what is collected and used to functionally, aesthetically, and symbolically fit the home. To understand the social impact of robotic technologies, this paper presents an ethnographic study of consumer robots in the home. Six families' experience of floor cleaning after receiving a new vacuum (a Roomba robotic vacuum or the Flair, a handheld upright) was studied. While the Flair had little impact, the Roomba changed people, cleaning activities, and other product use. In addition, people described the Roomba in aesthetic and social terms. The results of this study, while initial, generate implications for how robots should be designed for the home.</div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228735&CFID=105752387&CFTOKEN=75645419">Humanoid robots as a passive-social medium: a field experiment at a train station</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325488445&CFID=105752387&CFTOKEN=75645419">Kotaro Hayashi</a>, 
                        <a href="author_page.cfm?id=81329491765&CFID=105752387&CFTOKEN=75645419">Daisuke Sakamoto</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752387&CFTOKEN=75645419">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81310499910&CFID=105752387&CFTOKEN=75645419">Masahiro Shiomi</a>, 
                        <a href="author_page.cfm?id=81325488901&CFID=105752387&CFTOKEN=75645419">Satoshi Koizumi</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752387&CFTOKEN=75645419">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100012453&CFID=105752387&CFTOKEN=75645419">Tsukasa Ogasawara</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752387&CFTOKEN=75645419">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 137-144</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228735" title="DOI">10.1145/1228716.1228735</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228735&ftid=407206&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow19" style="display:inline;"><br /><div style="display:inline">This paper reports a method that uses humanoid robots as a communication medium. There are many interactive robots under development, but due to their limited perception, their interactivity is still far poorer than that of humans. Our approach in this ...</div></span>
          <span id="toHide19" style="display:none;"><br /><div style="display:inline">This paper reports a method that uses humanoid robots as a communication medium. There are many interactive robots under development, but due to their limited perception, their interactivity is still far poorer than that of humans. Our approach in this paper is to limit robots' purpose to a non-interactive medium and to look for a way to attract people's interest in the information that robots convey. We propose using robots as a passive-social medium, in which multiple robots converse with each other. We conducted a field experiment at a train station for eight days to investigate the effects of a passive-social medium.</div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228736&CFID=105752387&CFTOKEN=75645419">Comparing a computer agent with a humanoid robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310502228&CFID=105752387&CFTOKEN=75645419">Aaron Powers</a>, 
                        <a href="author_page.cfm?id=81100476487&CFID=105752387&CFTOKEN=75645419">Sara Kiesler</a>, 
                        <a href="author_page.cfm?id=81100583820&CFID=105752387&CFTOKEN=75645419">Susan Fussell</a>, 
                        <a href="author_page.cfm?id=81310500984&CFID=105752387&CFTOKEN=75645419">Cristen Torrey</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 145-152</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228736" title="DOI">10.1145/1228716.1228736</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228736&ftid=407207&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow20" style="display:inline;"><br /><div style="display:inline">HRI researchers interested in social robots have made large investments in humanoid robots. There is still sparse evidence that peoples' responses to robots differ from their responses to computer agents, suggesting that agent studies might serve to ...</div></span>
          <span id="toHide20" style="display:none;"><br /><div style="display:inline">HRI researchers interested in social robots have made large investments in humanoid robots. There is still sparse evidence that peoples' responses to robots differ from their responses to computer agents, suggesting that agent studies might serve to test HRI hypotheses. To help us understand the difference between people's social interactions with an agent and a robot, we experimentally compared people's responses in a health interview with (a) a computer agent projected either on a computer monitor or life-size on a screen, (b) a remote robot projected life-size on a screen, or (c) a collocated robot in the same room. We found a few behavioral and large attitude differences across these conditions. Participants forgot more and disclosed least with the collocated robot, next with the projected remote robot, and then with the agent. They spent more time with the collocated robot and their attitudes were most positive toward that robot. We discuss tradeoffs for HRI research of using collocated robots, remote robots, and computer agents as proxies of robots.</div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228737&CFID=105752387&CFTOKEN=75645419">Experiments with a robotic computer: body, affect and cognition interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100258451&CFID=105752387&CFTOKEN=75645419">Cynthia Breazeal</a>, 
                        <a href="author_page.cfm?id=81408600243&CFID=105752387&CFTOKEN=75645419">Andrew Wang</a>, 
                        <a href="author_page.cfm?id=81100496593&CFID=105752387&CFTOKEN=75645419">Rosalind Picard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 153-160</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228737" title="DOI">10.1145/1228716.1228737</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228737&ftid=407208&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow21" style="display:inline;"><br /><div style="display:inline">We present RoCo, the first robotic computer designed with the ability to move its monitor in subtly expressive ways that respond to and encourage its users own postural movement. We use RoCo in a novel user study to explore whether a computers "posture" ...</div></span>
          <span id="toHide21" style="display:none;"><br /><div style="display:inline">We present RoCo, the first robotic computer designed with the ability to move its monitor in subtly expressive ways that respond to and encourage its users own postural movement. We use RoCo in a novel user study to explore whether a computers "posture" can in fluence its use''s subsequent posture, and if the interaction of the user's body state with their affective state during a task leads to improved task measures such as persistence in problem solving. We believe this is possible in light of new theories that link physical posture and its in uence on affect and cognition. Initial results with 71 subjects support the hypothesis that RoCo's posture not only manipulates the users posture, but also is associated with hypothesized posture-affect interactions. Specifically, we found effects on increased persistence on a subsequent cognitive task, and effects on perceived level of comfort.</div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228738&CFID=105752387&CFTOKEN=75645419">RSVP: an investigation of remote shared visual presence as common ground for human-robot teams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81320488460&CFID=105752387&CFTOKEN=75645419">Jenny Burke</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=105752387&CFTOKEN=75645419">Robin Murphy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 161-168</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228738" title="DOI">10.1145/1228716.1228738</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228738&ftid=407209&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow22" style="display:inline;"><br /><div style="display:inline">This study presents mobile robots as a way of augmenting communication in distributed teams through a remote shared visual presence (RSVP) consisting of the robot's view. By giving all team members access to the shared visual display provided by a robot ...</div></span>
          <span id="toHide22" style="display:none;"><br /><div style="display:inline">This study presents mobile robots as a way of augmenting communication in distributed teams through a remote shared visual presence (RSVP) consisting of the robot's view. By giving all team members access to the shared visual display provided by a robot situated in a remote workspace, the robot can serve as a source of common ground for the distributed team. In a field study examining the effects of remote shared visual presence on team performance in collocated and distributed Urban Search & Rescue technical search teams, data were collected from 25 dyadic teams comprised of US&R task force personnel drawn from high-fidelity training exercises held in California (2004) and New Jersey (2005). They performed a 2 x 2 repeated measures search task entailing robot-assisted search in a confined space rubble pile. Multilevel regression analyses were used to predict team performance based upon use of <i>RSVP</i> (<i>RSVP</i> or no-<i>RSVP</i>) and whether or not team members had visual access to other team members. Results indicated that the use of <i>RSVP</i> technology predicted team performance ( &#223;= -1.24, <i>p</i>&lt;.05). No significant differences emerged in performance between teams with and without visual access to their team members. Findings suggest RSVP may enable distributed teams to perform as effectively as collocated teams. However, differences detected between sites suggest efficiency of <i>RSVP</i> may depend on the user's domain experience and team cohesion.</div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228739&CFID=105752387&CFTOKEN=75645419">A field experiment of autonomous mobility: operator workload for one and two robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325488449&CFID=105752387&CFTOKEN=75645419">Susan G. Hill</a>, 
                        <a href="author_page.cfm?id=81325487723&CFID=105752387&CFTOKEN=75645419">Barry Bodt</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 169-176</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228739" title="DOI">10.1145/1228716.1228739</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228739&ftid=407210&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow23" style="display:inline;"><br /><div style="display:inline">An experiment was conducted on aspects of human-robot interaction in a field environment using the U.S. Army's Experimental Unmanned Vehicle (XUV). Goals of this experiment were to examine the use of scalable interfaces and to examine operator span of ...</div></span>
          <span id="toHide23" style="display:none;"><br /><div style="display:inline">An experiment was conducted on aspects of human-robot interaction in a field environment using the U.S. Army's Experimental Unmanned Vehicle (XUV). Goals of this experiment were to examine the use of scalable interfaces and to examine operator span of control when controlling one versus two autonomous unmanned ground vehicles. We collected workload ratings from two Soldiers after they had performed missions that included monitoring, downloading and reporting on simulated reconnaissance, surveillance, and target acquisition (RSTA) images, and responding to unplanned operator intervention requests from the XUV. Several observations are made based on workload data, experimenter notes, and informal interviews with operators.</div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228740&CFID=105752387&CFTOKEN=75645419">HRI caught on film</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100461702&CFID=105752387&CFTOKEN=75645419">Christoph Bartneck</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752387&CFTOKEN=75645419">Takayuki Kanda</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 177-183</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228740" title="DOI">10.1145/1228716.1228740</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228740&ftid=407211&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow24" style="display:inline;"><br /><div style="display:inline">The Human Robot Interaction 2007 conference hosted a video session, in which movies of interesting, important, illustrative, or humorous HRI research moments are shown. This paper summarizes the abstracts of the presented videos. Robots and humans do ...</div></span>
          <span id="toHide24" style="display:none;"><br /><div style="display:inline">The Human Robot Interaction 2007 conference hosted a video session, in which movies of interesting, important, illustrative, or humorous HRI research moments are shown. This paper summarizes the abstracts of the presented videos. Robots and humans do not always behave as expected and the results can be entertaining and even enlightening - therefore instances of failures have also been considered in the video session. Besides the importance of the lessons learned and the novelty of the situation, the videos have also an entertaining value.</div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">POSTER SESSION: <strong>Posters</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228742&CFID=105752387&CFTOKEN=75645419">A cognitive robotics approach to comprehending human language and behaviors</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100264377&CFID=105752387&CFTOKEN=75645419">D. Paul Benjamin</a>, 
                        <a href="author_page.cfm?id=81100016454&CFID=105752387&CFTOKEN=75645419">Deryle Lonsdale</a>, 
                        <a href="author_page.cfm?id=81100493624&CFID=105752387&CFTOKEN=75645419">Damian Lyons</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 185-192</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228742" title="DOI">10.1145/1228716.1228742</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228742&ftid=407212&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow26" style="display:inline;"><br /><div style="display:inline">The ADAPT project is a collaboration of researchers in linguistics, robotics and artificial intelligence at three universities. We are building a complete robotic cognitive architecture for a mobile robot designed to interact with humans in a range of ...</div></span>
          <span id="toHide26" style="display:none;"><br /><div style="display:inline">The ADAPT project is a collaboration of researchers in linguistics, robotics and artificial intelligence at three universities. We are building a complete robotic cognitive architecture for a mobile robot designed to interact with humans in a range of environments, and which uses natural language and models human behavior. This paper concentrates on the HRI aspects of ADAPT, and especially on how ADAPT models and interacts with humans.</div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228743&CFID=105752387&CFTOKEN=75645419">Android as a telecommunication medium with a human-like presence</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81329491765&CFID=105752387&CFTOKEN=75645419">Daisuke Sakamoto</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752387&CFTOKEN=75645419">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100261578&CFID=105752387&CFTOKEN=75645419">Tetsuo Ono</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752387&CFTOKEN=75645419">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752387&CFTOKEN=75645419">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 193-200</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228743" title="DOI">10.1145/1228716.1228743</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228743&ftid=407213&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow27" style="display:inline;"><br /><div style="display:inline">In this research, we realize human telepresence by developing a remote-controlled android system called Geminoid HI-1. Experimental results confirm that participants felt stronger presence of the operator when he talked through the android than when ...</div></span>
          <span id="toHide27" style="display:none;"><br /><div style="display:inline">In this research, we realize human telepresence by developing a remote-controlled android system called Geminoid HI-1. Experimental results confirm that participants felt stronger presence of the operator when he talked through the android than when he appeared on a video monitor in a video conference system. In addition, participants talked with the robot naturally and evaluated its human likeness as equal to a man on a video monitor. At this paper's conclusion, we will discuss a remote-control system for telepresence that uses a human-like android robot as a new telecommunication medium.</div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228744&CFID=105752387&CFTOKEN=75645419">Autonomous behavior design for robotic appliance</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325488673&CFID=105752387&CFTOKEN=75645419">Hyunjeong Lee</a>, 
                        <a href="author_page.cfm?id=81100661492&CFID=105752387&CFTOKEN=75645419">Hyun Jin Kim</a>, 
                        <a href="author_page.cfm?id=81325488863&CFID=105752387&CFTOKEN=75645419">Changsu Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 201-208</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228744" title="DOI">10.1145/1228716.1228744</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228744&ftid=407214&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow28" style="display:inline;"><br /><div style="display:inline">We consider robotic appliances, mobile and intelligent robots that perform household tasks, and study design method for autonomous behaviors of robotic appliances, a key component in human-robot and robot-environment interactions. Specifically, we develop ...</div></span>
          <span id="toHide28" style="display:none;"><br /><div style="display:inline">We consider robotic appliances, mobile and intelligent robots that perform household tasks, and study design method for autonomous behaviors of robotic appliances, a key component in human-robot and robot-environment interactions. Specifically, we develop robot behavior diagram, a visualized, robot-centric, chart-based behavior design method that is useful in describing robotic behaviors and transitions among them. This tool enables systematic construction of exhaustive set of autonomous behaviors, within the context of given set of robotic functionalities. First, the set of functionalities for a given robot is defined. Then, employing the defined functionalities, a diagram is constructed with behavioral states and events that can trigger transitions to other behaviors. For any given behavioral state, all plausible events are considered, and this is repeated until the diagram is complete with all the possible behaviors and events. We demonstrate that, via this method, a complete and robust behavioral scenario design is indeed attainable, by applying robot behavior diagram approach to an example household cleaning robot platform.</div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228745&CFID=105752387&CFTOKEN=75645419">Combining ubiquitous and on-board audio sensing for human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81314485902&CFID=105752387&CFTOKEN=75645419">Simon Thompson</a>, 
                        <a href="author_page.cfm?id=81100481894&CFID=105752387&CFTOKEN=75645419">Satoshi Kagami</a>, 
                        <a href="author_page.cfm?id=81325490024&CFID=105752387&CFTOKEN=75645419">Yoko Sasaki</a>, 
                        <a href="author_page.cfm?id=81100382157&CFID=105752387&CFTOKEN=75645419">Yoshifumi Nishida</a>, 
                        <a href="author_page.cfm?id=81325487980&CFID=105752387&CFTOKEN=75645419">Tadashi Enomoto</a>, 
                        <a href="author_page.cfm?id=81100110304&CFID=105752387&CFTOKEN=75645419">Hiroshi Mizoguchi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 209-216</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228745" title="DOI">10.1145/1228716.1228745</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228745&ftid=407215&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow29" style="display:inline;"><br /><div style="display:inline">This paper reports on the development of a mobile robot system for operation within a house equipped with a ubiquitous sensor network.Human robot interaction is achieved through the combination of on-robot audio and laser range sensing and additional ...</div></span>
          <span id="toHide29" style="display:none;"><br /><div style="display:inline">This paper reports on the development of a mobile robot system for operation within a house equipped with a ubiquitous sensor network.Human robot interaction is achieved through the combination of on-robot audio and laser range sensing and additional audio sensors mounted in the ceiling of the ubiquitous environment. The ceiling mounted microphone arrays can be used to summon a mobile robot from a location outside the robot's range of hearing. After the robot autonomously navigates to the desired location, the on-board microphone array can be used to locate the sound source and to recognise a series of greetings and commands.</div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228746&CFID=105752387&CFTOKEN=75645419">"Daisy, Daisy, give me your answer do!": switching off a robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100461702&CFID=105752387&CFTOKEN=75645419">Christoph Bartneck</a>, 
                        <a href="author_page.cfm?id=81325490623&CFID=105752387&CFTOKEN=75645419">Michel van der Hoek</a>, 
                        <a href="author_page.cfm?id=81325489083&CFID=105752387&CFTOKEN=75645419">Omar Mubin</a>, 
                        <a href="author_page.cfm?id=81325487779&CFID=105752387&CFTOKEN=75645419">Abdullah Al Mahmud</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 217-222</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228746" title="DOI">10.1145/1228716.1228746</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228746&ftid=407216&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow30" style="display:inline;"><br /><div style="display:inline">Robots can exhibit life like behavior, but are according to traditional definitions not alive. Current robot users are confronted with an ambiguous entity and it is important to understand the users perception of these robots. This study analyses if ...</div></span>
          <span id="toHide30" style="display:none;"><br /><div style="display:inline">Robots can exhibit life like behavior, but are according to traditional definitions not alive. Current robot users are confronted with an ambiguous entity and it is important to understand the users perception of these robots. This study analyses if a robot's intelligence and its agreeableness influence its perceived animacy. The robot's animacy was measured, amongst other measurements, by the users' hesitation to switch it off. The results show that participants hesitated three times as long to switch off an agreeable and intelligent robot as compared to a non agreeable and unintelligent robot. The robots' intelligence had a significant influence on its perceived animacy. Our results suggest that interactive robots should be intelligent and exhibit an agreeable attitude to maximize its perceived animacy.</div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228747&CFID=105752387&CFTOKEN=75645419">Directed stigmergy-based control for multi-robot systems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325489923&CFID=105752387&CFTOKEN=75645419">Fitzgerald Steele, Jr.</a>, 
                        <a href="author_page.cfm?id=81100409807&CFID=105752387&CFTOKEN=75645419">Geb Thomas</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 223-230</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228747" title="DOI">10.1145/1228716.1228747</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228747&ftid=407217&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow31" style="display:inline;"><br /><div style="display:inline">Multi-robot systems are particularly useful in tasks that require searching large areas such as planetary science exploration, urban search and rescue, or landmine remediation. In order to overcome the inherent complexity of controlling multiple robots, ...</div></span>
          <span id="toHide31" style="display:none;"><br /><div style="display:inline">Multi-robot systems are particularly useful in tasks that require searching large areas such as planetary science exploration, urban search and rescue, or landmine remediation. In order to overcome the inherent complexity of controlling multiple robots, the user must be able to give high-level, goal driven direction to the robot team. Since human robot interaction is a relatively new discipline, it is helpful to look to existing systems for concepts, analogies, or metaphors that might be utilized in building useful systems. Inspiration from natural decentralized systems guides the development of a computer simulation for stigmergy-based control of multi-robot system, and the interface with which an operator can interact and control mobile robots. In-depth description of the design process includes a description of a basic stigmergy-based control system and an innovative Directed Stigmergy control system that facilitates operator control of the robot team in an interesting and surprisingly effective way.</div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228748&CFID=105752387&CFTOKEN=75645419">Elements of a spoken language programming interface for robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81407592901&CFID=105752387&CFTOKEN=75645419">Tim Miller</a>, 
                        <a href="author_page.cfm?id=81414615359&CFID=105752387&CFTOKEN=75645419">Andy Exley</a>, 
                        <a href="author_page.cfm?id=81100003457&CFID=105752387&CFTOKEN=75645419">William Schuler</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 231-237</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228748" title="DOI">10.1145/1228716.1228748</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228748&ftid=407218&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow32" style="display:inline;"><br /><div style="display:inline">In many settings, such as home care or mobile environments, demands on users' attention, or users' anticipated level of formal training, or other on-site conditions will make standard keyboard-and monitor-based robot programming interfaces impractical. ...</div></span>
          <span id="toHide32" style="display:none;"><br /><div style="display:inline">In many settings, such as home care or mobile environments, demands on users' attention, or users' anticipated level of formal training, or other on-site conditions will make standard keyboard-and monitor-based robot programming interfaces impractical. In such cases, a spoken language interface may be preferable. However, the open-ended task of programming a machine is very different from the sort of closed-vocabulary, data-rich applications (e.g. call routing) for which most speaker-independent spoken language interfaces are designed. This paper will describe some of the challenges of designing a spoken language programming interface for robots, and will present an approach that uses these semantic-level resources as extensively as possible in order to address these challenges.</div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228749&CFID=105752387&CFTOKEN=75645419">Assessing the scalability of a multiple robot interface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325488567&CFID=105752387&CFTOKEN=75645419">Curtis M. Humphrey</a>, 
                        <a href="author_page.cfm?id=81325488595&CFID=105752387&CFTOKEN=75645419">Christopher Henk</a>, 
                        <a href="author_page.cfm?id=81325490187&CFID=105752387&CFTOKEN=75645419">George Sewell</a>, 
                        <a href="author_page.cfm?id=81320496360&CFID=105752387&CFTOKEN=75645419">Brian W. Williams</a>, 
                        <a href="author_page.cfm?id=81100313646&CFID=105752387&CFTOKEN=75645419">Julie A. Adams</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 239-246</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228749" title="DOI">10.1145/1228716.1228749</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228749&ftid=407219&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow33" style="display:inline;"><br /><div style="display:inline">As multiple robot systems become more common, it is necessary to develop scalable human-robot interfaces that permit the inclusion of additional robots without reducing the overall system performance. Workload and situational awareness play key roles ...</div></span>
          <span id="toHide33" style="display:none;"><br /><div style="display:inline">As multiple robot systems become more common, it is necessary to develop scalable human-robot interfaces that permit the inclusion of additional robots without reducing the overall system performance. Workload and situational awareness play key roles in determining the ratio of <i>m</i> operators to <i>n</i> robots. A scalable interface, where <i>m</i> is much smaller than <i>n</i>, will have to manage the operator's workload and promote a high level of situation awareness. This work focused on the development of a scalable interface for a single human-multiple robot system. This interface introduces a relational "halo display that augments a camera view to promote situational awareness and the management of multiple robots by providing information regarding the robots' relative locations with respect to a selected robot. An evaluation was conducted to determine the scalability of the interface focusing on the effects of increasing the number of robots on workload, situation awareness, and robot usage. Twenty participants completed two bomb defusing tasks: one employing six robots, the other nine. The results indicated that increasing the number of robots increased overall workload and the operator's situation awareness.</div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228750&CFID=105752387&CFTOKEN=75645419">Exploring adaptive dialogue based on a robot's awareness of human gaze and task progress</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310500984&CFID=105752387&CFTOKEN=75645419">Cristen Torrey</a>, 
                        <a href="author_page.cfm?id=81310502228&CFID=105752387&CFTOKEN=75645419">Aaron Powers</a>, 
                        <a href="author_page.cfm?id=81100583820&CFID=105752387&CFTOKEN=75645419">Susan R. Fussell</a>, 
                        <a href="author_page.cfm?id=81100476487&CFID=105752387&CFTOKEN=75645419">Sara Kiesler</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 247-254</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228750" title="DOI">10.1145/1228716.1228750</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228750&ftid=407220&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow34" style="display:inline;"><br /><div style="display:inline">When a robot provides direction--as a guide, an assistant, or as an instructor--the robot may have to interact with people of different backgrounds and skill sets. Different people require informat on adapted to their level of understanding. In this ...</div></span>
          <span id="toHide34" style="display:none;"><br /><div style="display:inline">When a robot provides direction--as a guide, an assistant, or as an instructor--the robot may have to interact with people of different backgrounds and skill sets. Different people require informat on adapted to their level of understanding. In this paper, we explore the use of two simple forms of awareness that a robot might use to infer that a person needs further verbal elaboration during a tool select on task. First, the robot could use an eye tracker for inferring whether the person is looking at the robot and thus in need of further elaboration. Second, the robot could monitor delays in the individual's task progress, indicating that he or she could use further elaboration. We investigated the effects of these two types of awareness on performance time, selection mistakes, and the number of questions people asked the robot. We did not observe any obvious benefits of our gaze awareness manipulation. Awareness of task delays did reduce the number of questions participants' asked compared to our control condition but did not significantly reduce the number of select on mistakes. The mixed results of our investigation suggest that more research is necessary before we can understand how awareness of gaze and awareness of task delay can be successfully implemented in human-robot dialogue.</div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228751&CFID=105752387&CFTOKEN=75645419">Incremental learning of gestures by imitation in a humanoid robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309487873&CFID=105752387&CFTOKEN=75645419">Sylvain Calinon</a>, 
                        <a href="author_page.cfm?id=81100342762&CFID=105752387&CFTOKEN=75645419">Aude Billard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 255-262</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228751" title="DOI">10.1145/1228716.1228751</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228751&ftid=407221&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow35" style="display:inline;"><br /><div style="display:inline">We present an approach to teach incrementally human gestures to a humanoid robot. By using active teaching methods that puts the human teacher "in the loop" of the robot's learning, we show that the essential characteristics of a gesture can be efficiently ...</div></span>
          <span id="toHide35" style="display:none;"><br /><div style="display:inline">We present an approach to teach incrementally human gestures to a humanoid robot. By using active teaching methods that puts the human teacher "in the loop" of the robot's learning, we show that the essential characteristics of a gesture can be efficiently transferred by interacting socially with the robot. In a first phase, the robot observes the user demonstrating the skill while wearing motion sensors. The motion of his/her two arms and head are recorded by the robot, projected in a latent space of motion and encoded bprobabilistically in a Gaussian Mixture Model (GMM). In a second phase, the user helps the robot refine its gesture by kinesthetic teaching, i.e. by grabbing and moving its arms throughout the movement to provide the appropriate scaffolds. To update the model of the gesture, we compare the performance of two incremental training procedures against a batch training procedure. We present experiments to show that different modalities can be combined efficiently to teach incrementally basketball officials' signals to a HOAP-3 humanoid robot.</div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228752&CFID=105752387&CFTOKEN=75645419">Incremental natural language processing for HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325487408&CFID=105752387&CFTOKEN=75645419">Timothy Brick</a>, 
                        <a href="author_page.cfm?id=81100029514&CFID=105752387&CFTOKEN=75645419">Matthias Scheutz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 263-270</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228752" title="DOI">10.1145/1228716.1228752</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228752&ftid=407222&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow36" style="display:inline;"><br /><div style="display:inline">Robots that interact with humans face-to-face using natural language need to be responsive to the way humans use language in those situations. We propose a psychologically-inspired natural language processing system for robots which performs incremental ...</div></span>
          <span id="toHide36" style="display:none;"><br /><div style="display:inline">Robots that interact with humans face-to-face using natural language need to be responsive to the way humans use language in those situations. We propose a psychologically-inspired natural language processing system for robots which performs incremental semantic interpretation of spoken utterances, integrating tightly with the robot's perceptual and motor systems.</div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228753&CFID=105752387&CFTOKEN=75645419">Influence of perspective-taking and mental rotation abilities in space teleoperation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325489154&CFID=105752387&CFTOKEN=75645419">M. Alejandra Menchaca-Brandan</a>, 
                        <a href="author_page.cfm?id=81339513989&CFID=105752387&CFTOKEN=75645419">Andrew M. Liu</a>, 
                        <a href="author_page.cfm?id=81100300177&CFID=105752387&CFTOKEN=75645419">Charles M. Oman</a>, 
                        <a href="author_page.cfm?id=81100207992&CFID=105752387&CFTOKEN=75645419">Alan Natapoff</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 271-278</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228753" title="DOI">10.1145/1228716.1228753</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228753&ftid=407223&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow37" style="display:inline;"><br /><div style="display:inline">Operator performance during Space Shuttle and International Space Station robotic arm training can differ dramatically among astronauts. The difficulty making appropriate camera selections and accurate use of hand controllers, two of the more important ...</div></span>
          <span id="toHide37" style="display:none;"><br /><div style="display:inline">Operator performance during Space Shuttle and International Space Station robotic arm training can differ dramatically among astronauts. The difficulty making appropriate camera selections and accurate use of hand controllers, two of the more important aspects for performance, may be rooted in a problem mentally relating the various reference frames used by the displays, hand controllers and robot arm. In this paper, we examine whether the origin of such individual differences can be found in certain components of spatial ability. We have developed a virtual reality simulation of the Space Station Robotic Workstation to investigate whether performance differences can be correlated with subjects' perspective-taking and mental rotation abilities. Spatial test scores were measured and correlated to their performance in a docking robotic task. The preliminary results show that both mental rotation strategies and perspective-taking strategies are used by the operator to move the robot arm around the workspace. Further studies must be performed to confirm such findings. If important correlations between performance and spatial abilities are found, astronaut training could be designed in order to fulfill each operator's needs, reducing both training time and cost.</div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228754&CFID=105752387&CFTOKEN=75645419">LASSOing HRI: analyzing situation awareness in map-centric and video-centric interfaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100059315&CFID=105752387&CFTOKEN=75645419">Jill L. Drury</a>, 
                        <a href="author_page.cfm?id=81325488657&CFID=105752387&CFTOKEN=75645419">Brenden Keyes</a>, 
                        <a href="author_page.cfm?id=81100443257&CFID=105752387&CFTOKEN=75645419">Holly A. Yanco</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 279-286</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228754" title="DOI">10.1145/1228716.1228754</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228754&ftid=407224&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow38" style="display:inline;"><br /><div style="display:inline">Good situation awareness (SA) is especially necessary when robots and their operators are not collocated, such as in urban search and rescue (USAR). This paper compares how SA is attained in two systems: one that has an emphasis on video and another ...</div></span>
          <span id="toHide38" style="display:none;"><br /><div style="display:inline">Good situation awareness (SA) is especially necessary when robots and their operators are not collocated, such as in urban search and rescue (USAR). This paper compares how SA is attained in two systems: one that has an emphasis on video and another that has an emphasis on a three-dimensional map. We performed a within-subjects study with eight USAR domain experts. To analyze the utterances made by the participants, we developed a SA analysis technique, called LASSO, which includes five awareness categories: location, activities, surroundings, status, and overall mission. Using our analysis technique, we show that a map-centric interface is more effective in providing good location and status awareness while a video-centric interface is more effective in providing good surroundings and activities awareness.</div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228755&CFID=105752387&CFTOKEN=75645419">Non-facial/non-verbal methods of affective expression as applied to robot-assisted victim assessment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310503105&CFID=105752387&CFTOKEN=75645419">Cindy L. Bethel</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=105752387&CFTOKEN=75645419">Robin R. Murphy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 287-294</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228755" title="DOI">10.1145/1228716.1228755</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228755&ftid=407225&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow39" style="display:inline;"><br /><div style="display:inline">This work applies a previously developed set of heuristics for determining when to use non-facial/non-verbal methods of affective expression to the domain of a robot being used for victim assessment in the aftermath of a disaster. Robot-assisted victim ...</div></span>
          <span id="toHide39" style="display:none;"><br /><div style="display:inline">This work applies a previously developed set of heuristics for determining when to use non-facial/non-verbal methods of affective expression to the domain of a robot being used for victim assessment in the aftermath of a disaster. Robot-assisted victim assessment places a robot approximately three meters or less from a victim, and the path of the robot traverses three proximity zones (intimate (contact -- 0.46m), personal (0.46 -- 1.22 m), and social (1.22 -- 3.66 m)). Robot- and victim-eye views of an Inuktun robot were collected as it followed a path around the victim. The path was derived from observations of a prior robot-assisted medical reachback study. The victim's-eye views of the robot from seven points of interest on the path illustrate the appropriateness of each of the five primary non-facial/non-verbal methods of affective expression: (body movement, posture, orientation, illuminated color, and sound), offering support for the heuristics as a design aid. In addition to supporting the heuristics, the investigation identified three open research questions on acceptable motions and impact of the surroundings on robot affect.</div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228756&CFID=105752387&CFTOKEN=75645419">On-line behaviour classification and adaptation to human-robot interaction styles</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325488289&CFID=105752387&CFTOKEN=75645419">Doroth&#233;e Fran&#231;ois</a>, 
                        <a href="author_page.cfm?id=81100329934&CFID=105752387&CFTOKEN=75645419">Daniel Polani</a>, 
                        <a href="author_page.cfm?id=81100447769&CFID=105752387&CFTOKEN=75645419">Kerstin Dautenhahn</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 295-302</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228756" title="DOI">10.1145/1228716.1228756</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228756&ftid=407226&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow40" style="display:inline;"><br /><div style="display:inline">This paper presents a proof-of-concept of a robot that is adapting its behaviour on-line, during interactions with a human according to detected play styles. The study is part of the AuRoRa project which investigates how robots may be used to help children ...</div></span>
          <span id="toHide40" style="display:none;"><br /><div style="display:inline">This paper presents a proof-of-concept of a robot that is adapting its behaviour on-line, during interactions with a human according to detected play styles. The study is part of the AuRoRa project which investigates how robots may be used to help children with autism overcome some of their impairments in social interactions. The paper motivates why adaptation is a very desirable feature of autonomous robots in human-robot interaction scenarios in general, and in autism therapy in particular. Two different play styles namely 'strong' and 'gentle', which refer to the user, are investigated experimentally. The model relies on Self-Organizing Maps, used as a classifier, and on Fast Fourier Transform to preprocess the sensor data. First experiments were carried out which discuss the performance of the model. Related work on adaptation in socially assistive and therapeutic work are surveyed. In future work, with typically developing and autistic children, the concrete choice of the robot's behaviours will be tailored towards the children's interests and abilities.</div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228757&CFID=105752387&CFTOKEN=75645419">Realizing Hinokio: candidate requirements for physical avatar aystems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325489801&CFID=105752387&CFTOKEN=75645419">Laurel D. Riek</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 303-308</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228757" title="DOI">10.1145/1228716.1228757</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228757&ftid=407227&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow41" style="display:inline;"><br /><div style="display:inline">This paper presents a set of candidate requirements and survey questions for physical avatar systems as derived from the literature. These requirements will be applied to analyze a fictional, yet well-envisioned, physical avatar system depicted in the ...</div></span>
          <span id="toHide41" style="display:none;"><br /><div style="display:inline">This paper presents a set of candidate requirements and survey questions for physical avatar systems as derived from the literature. These requirements will be applied to analyze a fictional, yet well-envisioned, physical avatar system depicted in the film Hinokio. It is hoped that these requirements and survey questions can be used by other researchers as a guide when performing formal engineering tradeoff analysis during the design phase of new physical avatar systems, or during evaluation of existing systems.</div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228758&CFID=105752387&CFTOKEN=75645419">Robot expressionism through cartooning</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81416601902&CFID=105752387&CFTOKEN=75645419">James E. Young</a>, 
                        <a href="author_page.cfm?id=81311482442&CFID=105752387&CFTOKEN=75645419">Min Xin</a>, 
                        <a href="author_page.cfm?id=81339528031&CFID=105752387&CFTOKEN=75645419">Ehud Sharlin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 309-316</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228758" title="DOI">10.1145/1228716.1228758</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228758&ftid=407228&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow42" style="display:inline;"><br /><div style="display:inline">We present a new technique for human-robot interaction called robot expressionism through cartooning. We suggest that robots utilise cartoon-art techniques such as simplified and exaggerated facial expressions, stylised text, and icons for intuitive ...</div></span>
          <span id="toHide42" style="display:none;"><br /><div style="display:inline">We present a new technique for human-robot interaction called robot expressionism through cartooning. We suggest that robots utilise cartoon-art techniques such as simplified and exaggerated facial expressions, stylised text, and icons for intuitive social interaction with humans. We discuss practical mixed reality solutions that allow robots to augment themselves or their surroundings with cartoon art content. Our effort is part of what we call robot expressionism, a conceptual approach to the design and analysis of robotic interfaces that focuses on providing intuitive insight into robotic states as well as the artistic quality of interaction. Our paper discusses a variety of ways that allow robots to use cartoon art and details a test bed design, implementation, and exploratory evaluation. We describe our test bed, Jeeves, which uses a Roomba, an iRobot vacuum cleaner robot, and a mixed-reality system as a platform for rapid prototyping of cartoon-art interfaces. Finally, we present a set of interaction content scenarios which use the Jeeves prototype: trash Roomba, the recycle police, and clean tracks, as well as initial exploratory evaluation of our approach.</div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228759&CFID=105752387&CFTOKEN=75645419">Robotic etiquette: results from user studies involving a fetch and carry task</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350572990&CFID=105752387&CFTOKEN=75645419">Michael L. Walters</a>, 
                        <a href="author_page.cfm?id=81100447769&CFID=105752387&CFTOKEN=75645419">Kerstin Dautenhahn</a>, 
                        <a href="author_page.cfm?id=81100365903&CFID=105752387&CFTOKEN=75645419">Sarah N. Woods</a>, 
                        <a href="author_page.cfm?id=81350568813&CFID=105752387&CFTOKEN=75645419">Kheng Lee Koay</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 317-324</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228759" title="DOI">10.1145/1228716.1228759</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228759&ftid=407229&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow43" style="display:inline;"><br /><div style="display:inline">This paper presents results, outcomes and conclusions from a series of Human Robot Interaction (HRI) trials which investigated how a robot should approach a human in a fetch and carry task. Two pilot trials were carried out, aiding the development of ...</div></span>
          <span id="toHide43" style="display:none;"><br /><div style="display:inline">This paper presents results, outcomes and conclusions from a series of Human Robot Interaction (HRI) trials which investigated how a robot should approach a human in a fetch and carry task. Two pilot trials were carried out, aiding the development of a main HRI trial with four different approach contexts under controlled experimental conditions. The findings from the pilot trials were confirmed and expanded upon. Most subjects disliked a frontal approach when seated. In general, seated humans do not like to be approached by a robot directly from the front even when seated behind a table. A frontal approach is more acceptable when a human is standing in an open area. Most subjects preferred to be approached from either the left or right side, with a small overall preference for a right approach by the robot. However, this is not a strong preference and it may be disregarded if it is more physically convenient to approach from a left front direction. Handedness and occupation were not related to these preferences. Subjects do not usually like the robot to move or approach from directly behind them, preferring the robot to be in view even if this means the robot taking a physically non-optimum path. The subjects for the main HRI trials had no previous experience of interacting with robots. Future research aims are outlined and include the necessity of carrying out longitudinal trials to see if these findings hold over a longer period of exposure to robots.</div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228760&CFID=105752387&CFTOKEN=75645419">Robots as interfaces to haptic and locomotor spaces</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100299510&CFID=105752387&CFTOKEN=75645419">Vladimir Kulyukin</a>, 
                        <a href="author_page.cfm?id=81310501339&CFID=105752387&CFTOKEN=75645419">Chaitanya Gharpure</a>, 
                        <a href="author_page.cfm?id=81325489513&CFID=105752387&CFTOKEN=75645419">Cassidy Pentico</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 325-331</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228760" title="DOI">10.1145/1228716.1228760</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228760&ftid=407230&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow44" style="display:inline;"><br /><div style="display:inline">Research on spatial cognition and navigation of the visually impaired suggests that vision may be a primary sensory modality that enables humans to align the egocentric (self to object) and allocentric (object to object) frames of reference in space. ...</div></span>
          <span id="toHide44" style="display:none;"><br /><div style="display:inline">Research on spatial cognition and navigation of the visually impaired suggests that vision may be a primary sensory modality that enables humans to align the egocentric (self to object) and allocentric (object to object) frames of reference in space. In the absence of vision, the frames align best in the haptic space. In the locomotor space, as the haptic space translates with the body, lack of vision causes the frames to misalign, which negatively affects action reliability. In this paper, we argue that robots can function as interfaces to the haptic and locomotor spaces in supermarkets. In the locomotor space, the robot eliminates the necessity of frame alignment and, in or near the haptic space, it cues the shopper to the salient features of the environment sufficient for product retrieval. We present a trichotomous ontology of spaces in a supermarket induced by the presence of a robotic shopping assistant and analyze the results of robot-assisted shopping experiments with ten visually impaired participants conducted in a real supermarket.</div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228761&CFID=105752387&CFTOKEN=75645419">The RUBI project: a progress report</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100053278&CFID=105752387&CFTOKEN=75645419">Javier R. Movellan</a>, 
                        <a href="author_page.cfm?id=81310501100&CFID=105752387&CFTOKEN=75645419">Fumihide Tanaka</a>, 
                        <a href="author_page.cfm?id=81100641606&CFID=105752387&CFTOKEN=75645419">Ian R. Fasel</a>, 
                        <a href="author_page.cfm?id=81325490041&CFID=105752387&CFTOKEN=75645419">Cynthia Taylor</a>, 
                        <a href="author_page.cfm?id=81325489950&CFID=105752387&CFTOKEN=75645419">Paul Ruvolo</a>, 
                        <a href="author_page.cfm?id=81414600315&CFID=105752387&CFTOKEN=75645419">Micah Eckhardt</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 333-339</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228761" title="DOI">10.1145/1228716.1228761</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228761&ftid=407231&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow45" style="display:inline;"><br /><div style="display:inline">The goal of the RUBI project is to accelerate progress in the development of social robots by addressing the problem at multiple levels, including the development of a scientific agenda, research methods, formal approaches, software, and hardware. The ...</div></span>
          <span id="toHide45" style="display:none;"><br /><div style="display:inline">The goal of the RUBI project is to accelerate progress in the development of social robots by addressing the problem at multiple levels, including the development of a scientific agenda, research methods, formal approaches, software, and hardware. The project is based on the idea that progress will go hand-in-hand with the emergence of a new scientific discipline that focuses on understanding the organization of adaptive behavior in real-time within the environments in which organisms operate. As such, the RUBI project emphasizes the process of design by immersion, i.e., embedding scientists, engineers and robots in everyday life environments so as to have these environments shape the hardware, software, and scientific questions as early as possible in the development process. The focus of the project so far has been on social robots that interact with 18 to 24 month old toddlers as part of their daily activities at the Early Childhood Education Center at the University of California, San Diego. In this document we present an overall assessment of the lessons and progress through year two of the project.</div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228762&CFID=105752387&CFTOKEN=75645419">Spatial dialog for space system autonomy</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81343493915&CFID=105752387&CFTOKEN=75645419">Scott Green</a>, 
                        <a href="author_page.cfm?id=81343503874&CFID=105752387&CFTOKEN=75645419">Scott Richardson</a>, 
                        <a href="author_page.cfm?id=81325490033&CFID=105752387&CFTOKEN=75645419">Vadim Slavin</a>, 
                        <a href="author_page.cfm?id=81100172423&CFID=105752387&CFTOKEN=75645419">Randy Stiles</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 341-348</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228762" title="DOI">10.1145/1228716.1228762</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228762&ftid=407232&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow46" style="display:inline;"><br /><div style="display:inline">Future space operations will increasingly demand cooperation between humans and autonomous space systems such as robots, observer satellites, and distributed components. Human team members use a combination of gestures, gaze, posture, deictic references ...</div></span>
          <span id="toHide46" style="display:none;"><br /><div style="display:inline">Future space operations will increasingly demand cooperation between humans and autonomous space systems such as robots, observer satellites, and distributed components. Human team members use a combination of gestures, gaze, posture, deictic references and speech to communicate effectively. When a human team collaborates on a given task, they discuss the task, create a plan and then review this plan prior to execution to ensure success. This is exactly the process we envision for effective human-autonomous agent collaborative teamwork. Visual spatial information is a common reference for increased shared situation awareness between humans and autonomous systems. We use a spatial dialog approach to enable multiple humans to naturally and effectively communicate and work with multiple autonomous space systems. With this in mind, we have created a prototype spatial dialog system to support teamwork between humans and autonomous robotic agents. We combine augmented reality gesture interaction and visualization with speech to realize a spatial dialog capability. This paper describes our prototype, general approach, and related issues for team-oriented spatial dialog interaction with autonomous space systems.</div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228763&CFID=105752387&CFTOKEN=75645419">Speed adaptation for a robot walking with a human</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325490079&CFID=105752387&CFTOKEN=75645419">Emma Sviestins</a>, 
                        <a href="author_page.cfm?id=81100380195&CFID=105752387&CFTOKEN=75645419">Noriaki Mitsunaga</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752387&CFTOKEN=75645419">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752387&CFTOKEN=75645419">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752387&CFTOKEN=75645419">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 349-356</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228763" title="DOI">10.1145/1228716.1228763</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228763&ftid=407233&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow47" style="display:inline;"><br /><div style="display:inline">We have taken steps towards developing a method that enables an interactive humanoid robot to adapt its speed to a walking human that it is moving together with. This is difficult because the human is simultaneously adapting to the robot. From a case ...</div></span>
          <span id="toHide47" style="display:none;"><br /><div style="display:inline">We have taken steps towards developing a method that enables an interactive humanoid robot to adapt its speed to a walking human that it is moving together with. This is difficult because the human is simultaneously adapting to the robot. From a case study in human-human walking interaction we established a hypothesis about how to read a human's speed preference based on a relationship between humans' walking speed and their relative position in the direction of walking. We conducted two experiments to verify this hypothesis: one with two humans walking together, and one with a human subject walking with a humanoid robot, Robovie-IV. For 11 out of 15 subjects who walked with the robot, the results were consistent with the speed-position relationship of the hypothesis. We also conducted a preferred speed estimation experiment for six of the subjects. All of them were satisfied with one or more of the speeds that our algorithm estimated and four of them answered one of the speeds as the best one if the algorithm was allowed to give three options. In the paper, we also discuss the difficulties and possibilities that we learned from this preliminary trial.</div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228764&CFID=105752387&CFTOKEN=75645419">Young researchers' views on the current and future state of HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310499420&CFID=105752387&CFTOKEN=75645419">Kevin Gold</a>, 
                        <a href="author_page.cfm?id=81100641606&CFID=105752387&CFTOKEN=75645419">Ian Fasel</a>, 
                        <a href="author_page.cfm?id=81100563869&CFID=105752387&CFTOKEN=75645419">Nathan G. Freier</a>, 
                        <a href="author_page.cfm?id=81310500984&CFID=105752387&CFTOKEN=75645419">Cristen Torrey</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 357-364</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228764" title="DOI">10.1145/1228716.1228764</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228764&ftid=407234&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow48" style="display:inline;"><br /><div style="display:inline">This paper presents the results of a panel discussion titled "The Future of HRI," held during an NSF workshop for graduate students on human-robot interaction in August 2006. The panel divided the workshop into groups tasked with inventing models of ...</div></span>
          <span id="toHide48" style="display:none;"><br /><div style="display:inline">This paper presents the results of a panel discussion titled "The Future of HRI," held during an NSF workshop for graduate students on human-robot interaction in August 2006. The panel divided the workshop into groups tasked with inventing models of the field, and then asked these groups their opinions on the future of the field. In general, the workshop participants shared the belief that HRI can and should be seen as a single scientific discipline, despite the fact that it encompasses a variety of beliefs, methods, and philosophies drawn from several "core" disciplines in traditional areas of study. HRI researchers share many interrelated goals, participants felt, and enhancing the lines of communication between different areas would help speed up progress in the field. Common concerns included the unavailability of common robust platforms, the emphasis on human perception over robot perception, and the paucity of longitudinal real-world studies. The authors point to the current lack of consensus on research paradigms and platforms to argue that the field is not yet in the phase that philosopher Thomas Kuhn would call "normal science," but believe the field shows signs of approaching that phase.</div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228765&CFID=105752387&CFTOKEN=75645419">Tracking human motion and actions for interactive robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100598466&CFID=105752387&CFTOKEN=75645419">Odest Chadwicke Jenkins</a>, 
                        <a href="author_page.cfm?id=81317493875&CFID=105752387&CFTOKEN=75645419">German Gonz&#225;lez</a>, 
                        <a href="author_page.cfm?id=81335494161&CFID=105752387&CFTOKEN=75645419">Matthew Maverick Loper</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 365-372</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228765" title="DOI">10.1145/1228716.1228765</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228765&ftid=407235&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow49" style="display:inline;"><br /><div style="display:inline">A method is presented for kinematic pose estimation and action recognition from monocular robot vision through the use of dynamical human motion vocabularies. We propose the utilization of dynamical motion vocabularies towards bridging the decision making ...</div></span>
          <span id="toHide49" style="display:none;"><br /><div style="display:inline">A method is presented for kinematic pose estimation and action recognition from monocular robot vision through the use of dynamical human motion vocabularies. We propose the utilization of dynamical motion vocabularies towards bridging the decision making of observed humans and information from robot sensing. Our motion vocabulary is comprised of learned primitives that structure the action space for decision making and describe human movement dynamics. Given image observations over time, each primitive infers on pose independently using its prediction density on movement dynamics in the context of a particle filter. Pose estimates from a set of primitives inferencing in parallel are arbitrated to estimate the action being performed. The efficacy of our approach is demonstrated through tracking and action recognition over extended motion trials. Results evidence the robustness of the algorithm with respect to unsegmented multi-action movement, movement speed, and camera viewpoint.</div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1228766&CFID=105752387&CFTOKEN=75645419">User-centered approach to path planning of cleaning robots: analyzing user's cleaning behavior</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100661492&CFID=105752387&CFTOKEN=75645419">Hyunjin Kim</a>, 
                        <a href="author_page.cfm?id=81325488673&CFID=105752387&CFTOKEN=75645419">Hyunjeong Lee</a>, 
                        <a href="author_page.cfm?id=81325488160&CFID=105752387&CFTOKEN=75645419">Stanley Chung</a>, 
                        <a href="author_page.cfm?id=81325488863&CFID=105752387&CFTOKEN=75645419">Changsu Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 373-380</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1228716.1228766" title="DOI">10.1145/1228716.1228766</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1228766&ftid=407236&dwn=1&CFID=105752387&CFTOKEN=75645419" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow50" style="display:inline;"><br /><div style="display:inline">Current research on robot navigation is focused on clear recognition of the map and optimal path planning. The human cleaning path is, however, not optimal regarding time but optimal to the cleaning purpose. We have analyzed in this paper the cleaning ...</div></span>
          <span id="toHide50" style="display:none;"><br /><div style="display:inline">Current research on robot navigation is focused on clear recognition of the map and optimal path planning. The human cleaning path is, however, not optimal regarding time but optimal to the cleaning purpose. We have analyzed in this paper the cleaning behaviors in home environments and understood the user's path planning behaviors through usage tests of various vacuuming robots. We discovered that the actual user cleans with methods unique to specific areas of the house rather than following an optimal cleaning path. We not only suggest a path planning method for the vacuuming robot by using a layered map, but also a cleaning area designating method reflecting each area's characteristics. Based on these, we have designed a vacuuming robot's actions.</div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
          </div>
		  </td>
          </tr>          
          
</table>


</div> 
</div>


 <p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>



 <br />
<div class="footerbody" align="center" >
	

	The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2012 ACM, Inc.<br />
	<a href="http://www.acm.org/publications/policies/usage">Terms of Usage</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;	  
	<a href="http://www.acm.org/about/contact-us">Contact Us</a>

<br /><br />
Useful downloads: 
<a href="http://www.adobe.com/products/acrobat/readstep2.html"><img src="http://dl.acm.org/images/pdf_logo.gif" width="16" height="16" alt="" border="0" /> Adobe Acrobat</a>
&nbsp;&nbsp;
<a href="http://www.apple.com/quicktime/download/" target="_blank"><img src="http://dl.acm.org/images/qtlogo.gif" width="16" height="16" alt="" border="0" /> QuickTime</a>
&nbsp;&nbsp;
<a href="http://www.microsoft.com/windows/windowsmedia/download/default.asp" target="_blank"><img src="http://dl.acm.org/images/wmv.gif" width="16" height="15" alt="" border="0" /> Windows Media Player</a>
&nbsp;&nbsp;
<a href="http://www.real.com/" target="_blank"><img src="http://dl.acm.org/images/realplayer.gif" width="20" height="18" alt="" border="0" /> Real Player</a>

</div> 



<div  id="cf_window1338242039856" class="yuiextdlg">
	
	<div  id="theguide_title" class="x-dlg-hd">
		The ACM Guide to Computing Literature
	 </div>
	<div  id="theguide_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242039859" class="yuiextdlg">
	
	<div  id="thetags_title" class="x-dlg-hd">
		All Tags
	 </div>
	<div  id="thetags_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242039862" class="yuiextdlg">
	
	<div  id="theformats_title" class="x-dlg-hd">
		Export Formats
	 </div>
	<div  id="theformats_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242039864" class="yuiextdlg">
	
	<div  id="theexplaination_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theexplaination_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242039866" class="yuiextdlg">
	
	<div  id="theservices_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theservices_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242039868" class="yuiextdlg">
	
	<div  id="savetobinder_title" class="x-dlg-hd">
		Save to Binder
	 </div>
	<div  id="savetobinder_body" class="x-dlg-bd">
		
		
	 </div>
 </div> 

</body>
</html>