


<!doctype html>


<head><script type="text/javascript">_cf_loadingtexthtml="<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";
_cf_jsonprefix='//';
_cf_clientid='30A0CB3882F8AB5A82CC8B623A840809';</script><script type="text/javascript" src="/CFIDE/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfform.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/masks.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/FCKeditor/fckeditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/adapter/yui/ext-yui-adapter.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/ext-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/resizable.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dragdrop/dragdrop.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/util.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/build/state/State-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/widget-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dialog/dialogs.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/cf/cf.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />



<title>Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
	
  --></style>
 


<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>


<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  function setDiv() {
	var m = document.getElementById('divmain');
	var mh = m.offsetHeight;
	var t = document.getElementById('divtools');
	var th = t.offsetHeight;
	var tg = document.getElementById('divtags');
	var tgh = tg.offsetHeight;
	var calcheight = mh - th;
	if (tgh > calcheight  ){
	  var x = (th + tgh) - mh;
	  if ( (th + tgh) - mh < 65) {
	  }
	  else {
		 document.getElementById('divtags').innerHTML = ""; 
		 var tg = document.getElementById('divtags');
		 var tgh = tg.offsetHeight;
		 tg.style.height = tgh  + 'px';
	  }
	}
	else {
		tg.style.height = calcheight + 'px';
		document.getElementById('divtags').innerHTML = "";
	}

//  do I need to check after I resize to be sure I didn't go too big?
//	var tg2 = document.getElementById('divtags');
//	var tgh2 = tg.offsetHeight;	
//	if (tgh2 > mh + 65) {
//	  var y = mh + 65;
//	  alert('expanded too much ' + tgh2 + ' should be at most ' + y);
//	  tg.style.height = y + 'px';
//	  document.getElementById('divtags').innerHTML = "";
//	}

  }
</script>

<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>



<meta name="citation_publisher" content="IEEE Press"> <meta name="citation_authors" content="General Chair-Hinds, Pamela; General Chair-Ishiguro, Hiroshi; Program Chair-Kanda, Takayuki; Program Chair-Kahn, Peter"> <meta name="citation_title" content="Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction"> <meta name="citation_date" content="03/02/2010"> <meta name="citation_isbn" content="978-1-4244-4893-7"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1734454"> 



<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFFORM');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFDIV');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFTEXTAREA');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFWINDOW');
</script>

<script type="text/javascript">
	var _cf_window_init_1338242341338=function()
	{
		_cf_bind_init_1338242341339=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide_body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242341339);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window1338242341337', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242341338);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242341341=function()
	{
		_cf_bind_init_1338242341342=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags_body','bindExpr':['showthetags.cfm?id=1734454']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242341342);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1734454',{ modal:false, closable:true, divid:'cf_window1338242341340', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242341341);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242341344=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window1338242341343', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242341344);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242341346=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window1338242341345', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242341346);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242341348=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window1338242341347', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242341348);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242341350=function()
	{
		_cf_bind_init_1338242341351=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder_body','bindExpr':['savetobinder.cfm?id=1734454']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242341351);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1734454',{ modal:false, closable:true, divid:'cf_window1338242341349', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242341350);
</script>
</head>

<body style="text-align:center" onLoad="window.focus();">

<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'http://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
                        
<script src='AC_RunActiveContent.js' type="text/javascript"></script>




<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>



<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
	
    <tr style="vertical-align:top">
		
		<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text"  ><img src="http://dl.acm.org/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
		</td>
        
        <td style="padding-left: 5px; padding-right:10px; padding-bottom:0px;" class="small-link-text">
        	<table style="width:100%; border-collapse:collapse; padding:0px">
			<tr><td style="text-align:center">
				
                            <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
                    
					</td>		
			</tr>
			</table> 
        </td>
		<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text">
			 <p style="margin-top:0px; margin-bottom:10px;">
					
                            <a href="https://dl.acm.org/signin.cfm?cfid=85077225&amp;cftoken=85986120" class="small-link-text" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
                            &nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm?cfid=85077225&amp;cftoken=85986120"  class="small-link-text" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
						
			 </p>
            
			<table style="padding: 5px; border-collapse:collapse; float:right">
				
				
                            
                            <tr>
                            <td class="small-link-text" style="text-align:right">
                            <form name="qiksearch" action="results.cfm?h=1&amp;cfid=85077225&amp;cftoken=85986120" method="post">
                           
                           
                            
                                     
                                    <span style="margin-left:0px"><label><input type="text" name="query" size="34" value=" " /></label>&nbsp;
                                    <input style="vertical-align:top;" type="image" alt="Search" name="Go" src="http://dl.acm.org/images/search_small.jpg" />
                                    
                                    </span>
							  </form>
                                </td>
                            </tr>
                          
				  
			</table>

			
			
		</td>

	</tr>
    
    
    <tr><td colspan="3" class="small-link-text" style="padding-bottom:5px; padding-top:0px; text-align:center">
		<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
         
         </td>
    </tr>
    </table>
	
<map name="port" id="port" > 
  <area shape="rect" coords="1,1,60,50" href="http://www.acm.org/" alt="ACM Home Page" />
  <area shape="rect" coords="65,1,275,68" href="http://dl.acm.org/dl.cfm?CFID=85077225&CFTOKEN=85986120" alt="ACM Digital Library Home Page" />
</map>

<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
	<tr style="vertical-align:top">
		<td style="padding-right:10px; text-align:left" class="small-link-text">
        	<div id="divmain" style="border:1px solid #356b20;">
				 
				<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
					
                    	<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;"><strong>Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction</strong></h1>
                        
                </div>
                
                  

<table class="medium-text" style="border-collapse:collapse; padding:0px;">

<col style="width:540px" />

<tr style="vertical-align:top">
  <td>
    <table style="border-collapse:collapse; padding:2px;" class="medium-text">
      <col style="width:80px;" />
      <col style="width:auto" />
      <tr style="vertical-align:top">
        
      </tr>
    </table>

	
        <table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
            <col style="width:80px" />
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             General Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100572021&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=85077225&amp;cftoken=85986120" title="Author Profile Page" target="_self">Pamela Hinds</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1030795&CFID=85077225&CFTOKEN=85986120" title="Institutional Profile Page"><small>Stanford University, USA</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100572813&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=85077225&amp;cftoken=85986120" title="Author Profile Page" target="_self">Hiroshi Ishiguro</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1017649&CFID=85077225&CFTOKEN=85986120" title="Institutional Profile Page"><small>Osaka University, Japan</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             Program Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81311482839&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=85077225&amp;cftoken=85986120" title="Author Profile Page" target="_self">Takayuki Kanda</a>
                
            </td>
            <td valign="bottom">
                
                        <small>ATR, Japan</small>
                    	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81408592441&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=85077225&amp;cftoken=85986120" title="Author Profile Page" target="_self">Peter Kahn</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1031804&CFID=85077225&CFTOKEN=85986120" title="Institutional Profile Page"><small>University of Washington, USA</small></a>
                      	
            </td>
            </tr>
            
        </table>
    

  </td>

  <td rowspan="20" nowrap="nowrap">
	<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
		<tr>
        	<td align="center" style="padding-bottom: 5px;">
			  
               <img src="http://portalparts.acm.org/1740000/1734454/thumb/cover_thumb.jpg" title="Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction" height="100"  width="77" ALT="Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction" /> 
              </td>
              <td valign="top" align="left" nowrap="nowrap">
	             2010 Proceeding<br />
                 
        	 </td>
        </tr>
        
        <tr>
        	<td colspan="2" valign="baseline" style="padding-bottom:5px;">
            <img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
            <a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
            </td>
         </tr>
         <tr>
            <td  class="small-text" colspan="2" valign="top" style="padding-left:30px;">
				
	                    	&middot;&nbsp;Downloads (6 Weeks): 516<br />
    	                    &middot;&nbsp;Downloads (12 Months): 3,902<br />
                          
                        &middot;&nbsp;Citation Count: 93 
			</td>
         </tr>

	</table>
  </td>
</tr>
</table>

<br clear="all" />

<table border="0" class="medium-text" cellpadding="2" cellspacing="0">

<tr valign="top">
    <td colspan="3"><table border="0" class="medium-text" cellpadding="1" cellspacing="0">

<tr valign="top">
    <td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>

	<tr valign="top">
    	<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
	</tr>
    <tr valign="top">
	    <td style="padding-left:10px;">
		   <a href="http://www.hri2010.org" title="Conference Website"  target="_self" class="link-text">HRI '10</a> International Conference on Human Robot Interaction 
        </td>
	</tr>
    
    <tr valign="top">
	    <td style="padding-left:10px; padding-bottom:10px"> Nara, Japan &mdash; March 02 - 05, 2010
                    
                  <br />
                    
                  <span class="small-link-text">IEEE Press</span> <span class="small-link-text">Piscataway, NJ</span><span class="small-link-text">, USA</span> <span class="small-link-text">  &copy;2010</span> 
                  <br />           
                  
      </td>
	</tr>
	 

</table></td>
</tr>

</table>

                  
                 <br clear="all" />
			</div>
			
		</td>
		<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
	
            <div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
              <div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;"><strong>Tools and Resources</strong></h1></div>


<ul title="Tools and Resources" style="list-style: none; list-style-position:inside;
margin-left: 0px;
padding-left: 0em;
text-indent: 5px;
margin-bottom: 0px;">


<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:6px;">
   <span class="small-link-text">TOC Service:</span>
   	  
	  <img src="http://dl.acm.org/images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
      <ul style="margin-left: 0; padding-left: 0; display:inline;">
      	
        <li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
        <li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');"  class="small-link-text">RSS</a></li>
		        
      </ul>
    </span>
</li>

        <li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:6px;">
        <a href="citation.cfm?id=1734454&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
         </span></li>
    




<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Export Formats:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1734454&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1734454&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1734454&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
      </ul>
    </span>
</li>



 
   <li style="list-style-image:url(img/calbullet.jpg);margin-top:15px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Upcoming Conference:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px; margin-left:25px;"><a href="http://humanrobotinteraction.org/2013/" title="ACM/IEEE International Conference on Human-Robot Interaction" class="small-link-text">HRI'13</a></li>
      </ul>
    </span>
	</li>
    


</ul>           

  <!-- ADDTHIS BUTTON BEGIN -->
  
  <!-- ADDTHIS BUTTON END -->

<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>
  <!-- AddThis Button BEGIN -->



<!-- AddThis Button BEGIN -->
<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_slashdot"></a>
<a class="addthis_button_reddit"></a>


<span class="addthis_separator">|</span>
<a href="http://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>
<!-- AddThis Button END -->

  
 

  
  
            </div>
            
		</td>
	</tr>
    
</table>



</div>


<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">




<div id="fback" style="text-align:left; padding-top:10px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="feedback" border="0" /></a>
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design"><strong>Feedback</strong></a>

<span style="padding:10px;">|</span>




<span>Switch to <a href="citation.cfm?id=1734454&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>


</span>

 
<div class="small-text" style="margin-top:10px; margin-bottom:5px;"> 
<br />

    <a href="#abstract"  title="Abstract" style="padding:5px"><span>Abstract</span></a> |
    
    <a href="#formats"  title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
    
    <a href="#authors"  title="Authors" style="padding:5px"><span>Authors</span></a> |
    <a href="#references"  title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
    <a href="#citedby"  title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
    <a href="#indexterms"  title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
    <a href="#source"  title="Publication" style="padding:5px"><span>Publication</span></a> |
    <a href="#revs"  title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |               
	<a href="#comments"  title="Comments" style="padding:5px"><span>Comments</span></a>
	
     |               
	<a href="#prox"  title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
    
</div>
    
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;"/>



</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
       	
			<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

			

		
			
           
			
				
				<p>
					<div style="display:inline"><p>It is our great pleasure to welcome you to the 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI 2010). HRI is a single-track, highly selective annual conference that showcases the very best research and thinking in human-robot interaction. HRI is inherently interdisciplinary and multidisciplinary, reflecting work from researchers in robotics, psychology, cognitive science, HCI, human factors, artificial intelligence, organizational behavior, anthropology, and many other fields.</p> <p>The theme of HRI 2010 is "Grand Technical and Social Challenges in HRI." Robots are increasingly becoming part of people's everyday social lives. In future years, robots may become caretaking assistants for the elderly, or academic tutors for our children, or medical assistants, day care assistants, or psychological counselors. Robots may become our co-workers in factories and offices, or maids in our homes. As we move to create our future with robots, hard problems in human-robot interaction (HRI) exist, both technically and socially. This year's conference seeks to take up grand technical and social challenges in the field - and speak to their integration.</p> <p>The call for papers attracted 124 full paper submissions from Asia, Europe, the Middle East, and North America. The program committee led by the program co-chairs conducted a rigorous review process for full papers, accepting 26 full papers for oral presentation and publication in the proceedings. This year, taking advantage of having both IEEE and ACM as the sponsor, all papers are archived in both the IEEE Xplore and ACM Digital Library.</p> <p>Furthermore, 65 late-breaking reports were screened for relevance and accepted for presentation at the HRI conference as posters, exposing a broader perspective of solutions, challenges and issues in HRI. They will be made available in the IEEE Xplore as well as the ACM Digital Library. Finally, a total of 12 videos (out of 23 submissions) were accepted based on importance, novelty and entertainment value and will be shown in a special video session. The accepted presentations cover a variety of topics, including verbal and non-verbal human-robot communication, social learning, social and moral interaction with robots, interface design, and methods for studying human-robot interaction.</p> <p>This year's local event is "the Drama with Robots", where humanoid robots are used in art. The drama is directed by a famous director Oriza Hirata. It is designed to show how people might have relationships with robots in future society. We hope that visitors experience the interacting capability of robots and methods to express robots' feelings.</p></div>
				</p>
   				
           	</div>
			
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


  <div class="abstract">
        <SPAN><strong>FRONT MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1740000/1734454/fm/frontmatter.pdf?ip=188.194.239.219&CFID=85077225&CFTOKEN=85986120" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(title page, copyright, welcome, contents, organization, reviewers, sponsors &#38; supporters) 
  </div>          
  
  <div style="margin-top: 10px;"  class="abstract">
        <SPAN><strong>BACK MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1740000/1734454/bm/backmatter.pdf?ip=188.194.239.219&CFID=85077225&CFTOKEN=85986120" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(author index) 
  </div>          
  
<div style="margin-top: 10px; height: auto; padding: 5px; ">
		
		
        


	</div>

<br clear="all" />
</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<dl title="Authors" style="margin-top:0px">




<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 General Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Pamela Hinds" href="author_page.cfm?id=81100572021&CFID=85077225&CFTOKEN=85986120">Pamela Hinds</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">2000-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">18</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">157</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">12</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">148</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">673</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Pamela Hinds" href="author_page.cfm?id=81100572021&amp;dsp=coll&amp;trk=1&amp;CFID=85077225&CFTOKEN=85986120" target="_self">View colleagues</a> of Pamela Hinds
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Hiroshi Ishiguro" href="author_page.cfm?id=81100572813&CFID=85077225&CFTOKEN=85986120">Hiroshi Ishiguro</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1991-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">125</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">414</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">46</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">389</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">3,226</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Hiroshi Ishiguro" href="author_page.cfm?id=81100572813&amp;dsp=coll&amp;trk=1&amp;CFID=85077225&CFTOKEN=85986120" target="_self">View colleagues</a> of Hiroshi Ishiguro
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              



<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 Program Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Takayuki Kanda" href="author_page.cfm?id=81311482839&CFID=85077225&CFTOKEN=85986120">Takayuki Kanda</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">2003-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">63</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">277</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">40</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">362</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">2,519</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Takayuki Kanda" href="author_page.cfm?id=81311482839&amp;dsp=coll&amp;trk=1&amp;CFID=85077225&CFTOKEN=85986120" target="_self">View colleagues</a> of Takayuki Kanda
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Peter Kahn" href="author_page.cfm?id=81408592441&CFID=85077225&CFTOKEN=85986120">Peter Kahn</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1992-2011</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">20</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">152</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">14</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">110</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">801</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Peter Kahn" href="author_page.cfm?id=81408592441&amp;dsp=coll&amp;trk=1&amp;CFID=85077225&CFTOKEN=85986120" target="_self">View colleagues</a> of Peter Kahn
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              

</dl>
</div>

		  
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	References are not available

</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	Citings are not available
		
 </div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

Index Terms are not available


</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">



<table border="0" class="medium-text" cellpadding="0" cellspacing="5">



    <tr valign="top">
    	<td>Title</td> 
	    <td>
		   <a href="http://www.hri2010.org" title="Conference Website"  target="_self" class="link-text">HRI '10</a> International Conference on Human Robot Interaction 
        </td>
	</tr>
    <tr><td></td><td>Nara, Japan &mdash; March 02 - 05, 2010</td></tr> <tr><td>Pages</td><td>384</td></tr> 
                 <tr>
                 
                     <td>Sponsors</td>
                    
                  <td>
                  <a href="sig.cfm?id=SP918&CFID=85077225&CFTOKEN=85986120"> SIGART</a> ACM Special Interest Group on Artificial Intelligence
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="sig.cfm?id=SP923&CFID=85077225&CFTOKEN=85986120"> SIGCHI</a> ACM Special Interest Group on Computer-Human Interaction
                  </td>
                  </tr>
              
                  <tr><td>Publisher</td><td>IEEE Press Piscataway, NJ, USA</td>
				  </tr>
             <tr><td>ISBN</td><td> 978-1-4244-4893-7</td></tr> 
			<tr valign="top">
        	<td>Conference</td>
            <td valign="top" align="left"  style="padding-bottom: 25px;">
	            <strong style="padding-right:10px">HRI</strong><a href="event.cfm?id=RE285&CFID=85077225&CFTOKEN=85986120" title="ACM/IEEE International Conference on Human-Robot Interaction">ACM/IEEE International Conference on Human-Robot Interaction</a>
                
                       
                        <a href="event.cfm?id=RE285&CFID=85077225&CFTOKEN=85986120" title="ACM/IEEE International Conference on Human-Robot Interaction"><img border="0" src="http://portalparts.acm.org/event_logos/677/677.jpg" title="HRI logo" height="62"  width="100" ALT="HRI logo" style="vertical-align:top"></a>
						 

        	 </td>
            </tr>
		    <tr><td colspan="2">Paper Acceptance Rate 26 of 124 submissions, 21%</td></tr> <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 227 of 905 submissions, 25%</td></tr>
                       <tr valign="top">
                        <td colspan="2" style="padding-left:25px;">
                        	<table>
                            	<tr><td>
                                        <!-- WebCharts3D v5.1(2077) -->
<IMG SRC="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=Images/7227254740584120.JPG" id="Images_7227254740584120_JPG" name="Images_7227254740584120_JPG" usemap="#Images_7227254740584120_JPG_map" border="0"/>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAB' id='GP1338242341831AAAB'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>140</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAC' id='GP1338242341831AAAC'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>41</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAD' id='GP1338242341831AAAD'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>101</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAE' id='GP1338242341831AAAE'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>22</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAF' id='GP1338242341831AAAF'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>134</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAG' id='GP1338242341831AAAG'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>48</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAH' id='GP1338242341831AAAH'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>120</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAI' id='GP1338242341831AAAI'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>23</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAJ' id='GP1338242341831AAAJ'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>124</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAK' id='GP1338242341831AAAK'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>26</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAL' id='GP1338242341831AAAL'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>149</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAM' id='GP1338242341831AAAM'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>33</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAN' id='GP1338242341831AAAN'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>137</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242341831AAAO' id='GP1338242341831AAAO'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>34</td></tr></table>
<MAP name='Images_7227254740584120_JPG_map'>
<AREA shape='rect' coords='0,0,1,1'/>
<AREA shape="rect" coords="293,179,309,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAO",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAO",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAO",event)'/>
<AREA shape="rect" coords="277,74,293,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAN",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAN",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAN",event)'/>
<AREA shape="rect" coords="253,180,269,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAM",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAM",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAM",event)'/>
<AREA shape="rect" coords="237,62,253,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAL",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAL",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAL",event)'/>
<AREA shape="rect" coords="213,187,229,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAK",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAK",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAK",event)'/>
<AREA shape="rect" coords="197,87,213,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAJ",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAJ",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAJ",event)'/>
<AREA shape="rect" coords="173,190,189,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAI",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAI",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAI",event)'/>
<AREA shape="rect" coords="157,91,173,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAH",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAH",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAH",event)'/>
<AREA shape="rect" coords="133,165,149,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAG",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAG",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAG",event)'/>
<AREA shape="rect" coords="117,77,133,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAF",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAF",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAF",event)'/>
<AREA shape="rect" coords="93,191,109,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAE",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAE",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAE",event)'/>
<AREA shape="rect" coords="77,111,93,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAD",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAD",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAD",event)'/>
<AREA shape="rect" coords="53,172,69,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAC",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAC",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAC",event)'/>
<AREA shape="rect" coords="37,71,53,213" onMouseover='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAB",event,true)' onMouseout='xx_set_visible("Images_7227254740584120_JPG","GP1338242341831AAAB",event,false)' onMousemove='xx_move_tag("Images_7227254740584120_JPG","GP1338242341831AAAB",event)'/>
<AREA shape="rect" coords="160,13,227,27"/>
<AREA shape="rect" coords="89,13,160,27"/>
</MAP>

<script language="javascript" src="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=script.js"></script>

                                      </td>
                                      <td style="padding-left:20px;">
                                             <table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
                                                <tr bgcolor="#ffffff">
                                                  <th style="width:50%">Year</th>
                                                  <th  align="right" style="width:15%">Submitted</th>
                                                  <th  align="right" style="width:15%">Accepted</th>
                                                  <th  align="center">Rate</th>
                                                </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '06</td>
                                                            <td align="right">140</td>
                                                            <td align="right">41</td>
                                                            <td align="center">29%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '07</td>
                                                            <td align="right">101</td>
                                                            <td align="right">22</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '08</td>
                                                            <td align="right">134</td>
                                                            <td align="right">48</td>
                                                            <td align="center">36%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '09</td>
                                                            <td align="right">120</td>
                                                            <td align="right">23</td>
                                                            <td align="center">19%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '10</td>
                                                            <td align="right">124</td>
                                                            <td align="right">26</td>
                                                            <td align="center">21%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '11</td>
                                                            <td align="right">149</td>
                                                            <td align="right">33</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '12</td>
                                                            <td align="right">137</td>
                                                            <td align="right">34</td>
                                                            <td align="center">25%</td>
                                                         </tr>
                                                
                                                 <tr bgcolor="#ffffff">
                                                    <td><strong>Overall</strong></td>
                                                    <td align="right">905</td>
                                                    <td align="right">227</td>
                                                    <td align="center">25%</td>
                                                  </tr>
                                                </table>
                                       </td>
                                     </tr>
                               </table>
                        </td>
                    </tr>
                     
                     
            
</table>


</table>




</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	<br />Reviews are not available for this item
        
        <div align="left" style="margin-top:30px">
					<a title="Computing Reviews" href="ocr_review_main.cfm?CFID=85077225&CFTOKEN=85986120">
                 <img src="http://dl.acm.org/images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
        
        
       		<ul style="list-style:disc; display:inline-block">
	            <li>Access <a href="ocr_review_main.cfm?CFID=85077225&CFTOKEN=85986120" target="_blank">critical reviews</a> of computing literature.</li>
            	<li><a href="http://www.computingreviews.com/Reviewer/"  target="_blank">Become a reviewer</a> for Computing Reviews</li>
            </ul>
        </div>
        
</div>



<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
         
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<div>
<div>
	
	<p style="margin-left:5px;">
    <strong>Be the first to comment</strong>
    	
          	To Post a comment please <a href="signin.cfm?CFID=85077225&CFTOKEN=85986120">sign in or create</a> a free Web account</a>
        
    
    
	 </p>
	   	
 
</div>


</div>

	
		<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
        
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" >

<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction</h5>


<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>

<div style="clear:both">
    
        <div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1514095&picked=prox&CFID=85077225&CFTOKEN=85986120" title="previous: HRI '09"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1957656&picked=prox&CFID=85077225&CFTOKEN=85986120" title="Next: HRI '11">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
        
</div>


 
<table class="text12" border="0">

  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>HRI 2010 tutorials, workshops, &#38; panels</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734458&CFID=85077225&CFTOKEN=85986120">Tutorial: cognitive analysis methods applied to human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100313646&CFID=85077225&CFTOKEN=85986120">Julie A. Adams</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=85077225&CFTOKEN=85986120">Robin R. Murphy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 1-1</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734458&ftid=764638&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow2" style="display:inline;"><br /><div style="display:inline">This half-day tutorial will cover topics related to conducting cognitive task analysis and cognitive work analysis for purposes of informing human-robot interaction design and development. The goal of the tutorial is to provide attendees with an overview ...</div></span>
          <span id="toHide2" style="display:none;"><br /><div style="display:inline"><p>This half-day tutorial will cover topics related to conducting cognitive task analysis and cognitive work analysis for purposes of informing human-robot interaction design and development. The goal of the tutorial is to provide attendees with an overview and comparison of various cognitive task analysis and cognitive work analysis methods, an understanding of how to conduct these types of analyses, collect the necessary data for analysis, and provide real-world case studies for specific cognitive task analysis and cognitive work analysis. The tutorial will include examples from actual analyses and data collection activities.</p></div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734459&CFID=85077225&CFTOKEN=85986120">HRI 2010 workshop 1: what do collaborations with the arts have to say about HRI?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100105515&CFID=85077225&CFTOKEN=85986120">William D. Smart</a>, 
                        <a href="author_page.cfm?id=81456637274&CFID=85077225&CFTOKEN=85986120">Annamaria Pileggi</a>, 
                        <a href="author_page.cfm?id=81100499212&CFID=85077225&CFTOKEN=85986120">Leila Takayama</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 3-3</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734459&ftid=764639&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow3" style="display:inline;"><br /><div style="display:inline">Human-Robot Interaction researchers are beginning to reach out to fields not traditionally associated with interaction research, such as the performing arts, cartooning, and animation. These collaborations offer the potential for novel insights about ...</div></span>
          <span id="toHide3" style="display:none;"><br /><div style="display:inline"><p>Human-Robot Interaction researchers are beginning to reach out to fields not traditionally associated with interaction research, such as the performing arts, cartooning, and animation. These collaborations offer the potential for novel insights about how to get robots and people to interact more effectively, but they also involve a number of unique challenges. This full-day workshop will offer a venue for HRI researchers and their collaborators from these diverse fields to report on their work, share insights about the collaboration process, and to help begin to define an exciting new area in HRI.</p></div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734460&CFID=85077225&CFTOKEN=85986120">Interaction science perspective on HRI: designing robot morphology</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100147388&CFID=85077225&CFTOKEN=85986120">Angel P. del Pobil</a>, 
                        <a href="author_page.cfm?id=81456626968&CFID=85077225&CFTOKEN=85986120">S. Shyam Sundar</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 5-5</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734460&ftid=764640&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow4" style="display:inline;"><br /><div style="display:inline">This workshop will address the impact of robot morphology on HRI from the perspective of Interaction Science, which encompasses theory and design of human interaction with technology. Anthropomorphic designs, which are common, have to be balanced with ...</div></span>
          <span id="toHide4" style="display:none;"><br /><div style="display:inline"><p>This workshop will address the impact of robot morphology on HRI from the perspective of Interaction Science, which encompasses theory and design of human interaction with technology. Anthropomorphic designs, which are common, have to be balanced with the "uncanny valley effect," since different morphologies suggest different affordances to users, triggering a variety of cognitive heuristics and thereby shaping their interactions with robots. We expect progress towards more human-acceptable interactions with robots by understanding the cognitive, behavioral, organizational, and contextual factors of morphology in HRI, as well as new meta-theories and design guidelines. We emphasize a highly multi-disciplinary approach, by involving participants from social sciences, engineering, and design.</p> <p>Topics of presentation include but not limited to: <ol><li>Engineering considerations in designing robot morphology</li> <li>Empirical psychological considerations in designing robot morphology</li> <li>Aesthetic parameters for transcending the uncanny valley effect (UVE) with static, dynamic and interactive robots</li> <li>Physiological (fMRI) bases of UVE</li> <li>Cognitive heuristics triggered by morphological cues on robot interfaces</li> <li>Adaptation for multimodal robot interfaces</li> <li>Cognitive Robotic Engine for Dependable HRI</li> <li>Acceptance of Socially Interactive Robots</li> <li>Evaluation frameworks for human-like robots</li> <li>Robot appearances for social interactions among Autistic children</li></ol></p></div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734461&CFID=85077225&CFTOKEN=85986120">HRI 2010 workshop 3: learning and adaptation of humans in HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100572813&CFID=85077225&CFTOKEN=85986120">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81456631429&CFID=85077225&CFTOKEN=85986120">Robin Murphy</a>, 
                        <a href="author_page.cfm?id=81336491870&CFID=85077225&CFTOKEN=85986120">Tatsuya Nomura</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 7-7</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734461&ftid=764641&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow5" style="display:inline;"><br /><div style="display:inline">On the current situation where robots having functions of communication with humans begin to appear in daily-life fields, it should be considered how symbiosis of humans and robots can be achieved. Many existing studies have focused on how robots can ...</div></span>
          <span id="toHide5" style="display:none;"><br /><div style="display:inline"><p>On the current situation where robots having functions of communication with humans begin to appear in daily-life fields, it should be considered how symbiosis of humans and robots can be achieved. Many existing studies have focused on how robots can learn from and adapt for humans. This full-day workshop focuses not only on this classical theme but also on how humans can learn in and adapt for environments where robots are acting. In particular, human learning from and adaptation for robots should be covered by interdisciplinary research fields including robotics, computer science, psychology, sociology, and pedagogy.</p></div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734462&CFID=85077225&CFTOKEN=85986120">HRI pioneers workshop 2010</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309505435&CFID=85077225&CFTOKEN=85986120">Kate Tsui</a>, 
                        <a href="author_page.cfm?id=81456609775&CFID=85077225&CFTOKEN=85986120">Min Kyung Lee</a>, 
                        <a href="author_page.cfm?id=81350575708&CFID=85077225&CFTOKEN=85986120">Kristen Stubbs</a>, 
                        <a href="author_page.cfm?id=81328487965&CFID=85077225&CFTOKEN=85986120">Henriette Cramer</a>, 
                        <a href="author_page.cfm?id=81325489801&CFID=85077225&CFTOKEN=85986120">Laurel D. Riek</a>, 
                        <a href="author_page.cfm?id=81330498575&CFID=85077225&CFTOKEN=85986120">Ja-Young Sung</a>, 
                        <a href="author_page.cfm?id=81456634200&CFID=85077225&CFTOKEN=85986120">Osawa Hirotaka</a>, 
                        <a href="author_page.cfm?id=81321498073&CFID=85077225&CFTOKEN=85986120">Satoru Satake</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 9-9</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734462&ftid=764642&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow6" style="display:inline;"><br /><div style="display:inline">The field of human-robot interaction is new but growing rapidly. While there are now several established researchers in the field, many of the current human-robot interaction practitioners are students or recently graduated. This workshop, to be held ...</div></span>
          <span id="toHide6" style="display:none;"><br /><div style="display:inline"><p>The field of human-robot interaction is new but growing rapidly. While there are now several established researchers in the field, many of the current human-robot interaction practitioners are students or recently graduated. This workshop, to be held in conjunction with the HRI 2010 Conference, aims to bring together graduate students to present their current research to an audience of their peers in a setting that is less formal and more interactive than the main HRI conference, to talk about the important issues in their field, and to hear about what their colleagues are doing. Participants are encouraged to actively engage in and form relationships with others by discussing fundamental topics in HRI and by engaging in hands-on group activities.</p></div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734463&CFID=85077225&CFTOKEN=85986120">Panel 1: grand technical and social challenges in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100563869&CFID=85077225&CFTOKEN=85986120">Nathan Freier</a>, 
                        <a href="author_page.cfm?id=81100515894&CFID=85077225&CFTOKEN=85986120">Minoru Asada</a>, 
                        <a href="author_page.cfm?id=81100572021&CFID=85077225&CFTOKEN=85986120">Pam Hinds</a>, 
                        <a href="author_page.cfm?id=81100458351&CFID=85077225&CFTOKEN=85986120">Gerhard Sagerer</a>, 
                        <a href="author_page.cfm?id=81414599084&CFID=85077225&CFTOKEN=85986120">Greg Trafton</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 11-11</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734463&ftid=764643&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow7" style="display:inline;"><br /><div style="display:inline">Robots are becoming part of people's everyday social lives - and will increasingly become so. In future years, robots may become caretaking assistants for the elderly, or academic tutors for our children, or medical assistants, day care assistants, or ...</div></span>
          <span id="toHide7" style="display:none;"><br /><div style="display:inline"><p>Robots are becoming part of people's everyday social lives - and will increasingly become so. In future years, robots may become caretaking assistants for the elderly, or academic tutors for our children, or medical assistants, day care assistants, or psychological counselors. Robots may become our co-workers in factories and offices, or maids in our homes. They may become our friends. As we move to create our future with robots, hard problems in HRI exist, both technically and socially. The Fifth Annual Conference on HRI seeks to take up grand technical and social challenges in the field - and speak to their integration. This panel brings together 4 leading experts in the field of HRI to speak on this topic.</p></div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734464&CFID=85077225&CFTOKEN=85986120">Panel 2: social responsibility in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100563869&CFID=85077225&CFTOKEN=85986120">Nathan Freier</a>, 
                        <a href="author_page.cfm?id=81100342762&CFID=85077225&CFTOKEN=85986120">Aude Billard</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=85077225&CFTOKEN=85986120">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100538966&CFID=85077225&CFTOKEN=85986120">Illah Nourbakhsh</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 11-11</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734464&ftid=764644&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow8" style="display:inline;"><br /><div style="display:inline">At the 2008 ACM/IEEE Conference on Human-Robot Interaction, a provocative panel was held to discuss the complicated ethical issues that abound in the field of human-robot interaction. The panel members and the audience participation made it clear that ...</div></span>
          <span id="toHide8" style="display:none;"><br /><div style="display:inline"><p>At the 2008 ACM/IEEE Conference on Human-Robot Interaction, a provocative panel was held to discuss the complicated ethical issues that abound in the field of human-robot interaction. The panel members and the audience participation made it clear that the HRI community desires - indeed, is in need of - an ongoing discussion on the nature of social responsibility in the field of human-robot interaction. At the 2010 Conference, we will hold a panel on the issues of social responsibility in HRI, focusing on the unique features of robotic interaction that call for responsible action (e.g., value-specific domains such as autonomy, accountability, trust, and/or human dignity; and application areas such as military applications, domestic care, entertainment, and/or communication). As a young and rapidly growing field, we have a responsibility to conduct our research in such a way that it leads to human-robot interaction outcomes that promote rather than hinder the flourishing of humans across society. What does social responsibility within the HRI field look like, and how do we conduct our work while adhering to such an obligation? The panelists will be asked to address this and related questions as a means of continuing an ongoing conversation on social responsibility in human-robot interaction.</p></div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734465&CFID=85077225&CFTOKEN=85986120">Company talks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456629248&CFID=85077225&CFTOKEN=85986120">Y. Hosoda</a>, 
                        <a href="author_page.cfm?id=81456621376&CFID=85077225&CFTOKEN=85986120">N. Sumida</a>, 
                        <a href="author_page.cfm?id=81456624997&CFID=85077225&CFTOKEN=85986120">T. Mita</a>, 
                        <a href="author_page.cfm?id=81456634356&CFID=85077225&CFTOKEN=85986120">Y. Matsukawa</a>, 
                        <a href="author_page.cfm?id=81456613222&CFID=85077225&CFTOKEN=85986120">D. Yamamoto</a>, 
                        <a href="author_page.cfm?id=81456620976&CFID=85077225&CFTOKEN=85986120">N. Shibatani</a>, 
                        <a href="author_page.cfm?id=81456625216&CFID=85077225&CFTOKEN=85986120">L. Takayama</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 13-13</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734465&ftid=764645&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow9" style="display:inline;"><br /><div style="display:inline">The aim of the company talks is (1)to provide a good picture about forefront technologies about robots related to human-robot interaction,and (2)to provide a opportunity to connect researchers and people from industries.Seven companies gives 8 minutes ...</div></span>
          <span id="toHide9" style="display:none;"><br /><div style="display:inline"><p>The aim of the company talks is (1)to provide a good picture about forefront technologies about robots related to human-robot interaction,and (2)to provide a opportunity to connect researchers and people from industries.Seven companies gives 8 minutes talk to present their cutting-edge technologies.Instead of having Q&#38;A time after each presentation researchers and company presenters are Q&#38;A time after each presentation,researchers and company presenters are encouraged to communicate with each other during reception just after company talks,where research posters will be presented.</p></div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 1: robots in daily life</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Vanessa Evers 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734467&CFID=85077225&CFTOKEN=85986120">MeBot: a robotic platform for socially embodied presence</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456639175&CFID=85077225&CFTOKEN=85986120">Sigurdur O. Adalgeirsson</a>, 
                        <a href="author_page.cfm?id=81100258451&CFID=85077225&CFTOKEN=85986120">Cynthia Breazeal</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 15-22</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734467&ftid=764646&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734467&ftid=820088&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow11" style="display:inline;"><br /><div style="display:inline">Telepresence refers to a set of technologies that allow users to feel present at a distant location; telerobotics is a subfield of telepresence. This paper presents the design and evaluation of a telepresence robot which allows for social expression. ...</div></span>
          <span id="toHide11" style="display:none;"><br /><div style="display:inline"><p>Telepresence refers to a set of technologies that allow users to feel present at a distant location; telerobotics is a subfield of telepresence. This paper presents the design and evaluation of a telepresence robot which allows for social expression. Our hypothesis is that a telerobot that communicates more than simply audio or video but also expressive gestures, body pose and proxemics, will allow for a more engaging and enjoyable interaction. An iterative design process of the MeBot platform is described in detail, as well as the design of supporting systems and various control interfaces. We conducted a human subject study where the effects of expressivity were measured. Our results show that a socially expressive robot was found to be more engaging and likable than a static one. It was also found that expressiveness contributes to more psychological involvement and better cooperation.</p></div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734468&CFID=85077225&CFTOKEN=85986120">Robots asking for directions: the willingness of passers-by to support robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81381602151&CFID=85077225&CFTOKEN=85986120">Astrid Weiss</a>, 
                        <a href="author_page.cfm?id=81453659538&CFID=85077225&CFTOKEN=85986120">Judith Igelsb&#246;ck</a>, 
                        <a href="author_page.cfm?id=81100481664&CFID=85077225&CFTOKEN=85986120">Manfred Tscheligi</a>, 
                        <a href="author_page.cfm?id=81100159766&CFID=85077225&CFTOKEN=85986120">Andrea Bauer</a>, 
                        <a href="author_page.cfm?id=81453653084&CFID=85077225&CFTOKEN=85986120">Kolja K&#252;hnlenz</a>, 
                        <a href="author_page.cfm?id=81384619620&CFID=85077225&CFTOKEN=85986120">Dirk Wollherr</a>, 
                        <a href="author_page.cfm?id=81336488084&CFID=85077225&CFTOKEN=85986120">Martin Buss</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 23-30</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734468&ftid=764219&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow12" style="display:inline;"><br /><div style="display:inline">This paper reports about a human-robot interaction field trial conducted with the autonomous mobile robot ACE (Autonomous City Explorer) in a public place, where the ACE robot needs the support of human passers-by to find its way to a target ...</div></span>
          <span id="toHide12" style="display:none;"><br /><div style="display:inline"><p>This paper reports about a human-robot interaction field trial conducted with the autonomous mobile robot <i>ACE (Autonomous City Explorer)</i> in a public place, where the <i>ACE</i> robot needs the support of human passers-by to find its way to a target location. Since the robot does not possess any prior map knowledge or GPS support, it has to acquire missing information through interaction with humans. The robot thus has to initiate communication by asking for the way, and retrieves information from passers-by showing the way by gestures (pointing) and marking goal positions on a still image on the touch screen of the robot. The aims of the field trial where threefold: (1) Investigating the aptitude of the navigation architecture, (2) Evaluating the intuitiveness of the interaction concept for the passers-by, (3) Assessing people's willingness to support the <i>ACE</i> robot in its task, i.e. assessing the social acceptability. The field trial demonstrates that the architecture enables successful autonomous path finding without any prior map knowledge just by route directions given by passers-by. An additional street survey and observational data moreover attests the intuitiveness of the interaction paradigm and the high acceptability of the <i>ACE</i> robot in the public place.</p></div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734469&CFID=85077225&CFTOKEN=85986120">A larger audience, please!: encouraging people to listen to a guide robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310499910&CFID=85077225&CFTOKEN=85986120">Masahiro Shiomi</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=85077225&CFTOKEN=85986120">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=85077225&CFTOKEN=85986120">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=85077225&CFTOKEN=85986120">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 31-38</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734469&ftid=764220&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMpeg" title="Other Formats Mpeg" href="ft_gateway.cfm?id=1734469&ftid=820089&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/mpeg.gif" alt="Mpeg" class="fulltext_lnk" border="0" />Mpeg</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow13" style="display:inline;"><br /><div style="display:inline">Tour guidance is a common task of social robots. Such a robot must be able to encourage the participation of people who are not directly interacting with it. We are particularly interested in encouraging people to overhear its interaction with others, ...</div></span>
          <span id="toHide13" style="display:none;"><br /><div style="display:inline"><p>Tour guidance is a common task of social robots. Such a robot must be able to encourage the participation of people who are not directly interacting with it. We are particularly interested in encouraging people to overhear its interaction with others, since it has often been observed that even people who hesitate to interact with a robot are willing to observe its activity. To encourage such participation as <i>bystanders</i>, we developed a robot that walks backwards based on observations of human tour guides. Our developed system uses a robust human tracking system that enables a robot to guide people by walking forward/backward and allows us to scrutinize people's behavior after the experiment. We conducted a field experiment to compare the ratios of overhearing in "walking forward" and "walking backward." The experimental results revealed that in fact people do more often overhear the robot's conversation in the "walking backward" condition.</p></div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 2: affect from appearance &#38; motion</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Miguel Salichs 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734471&CFID=85077225&CFTOKEN=85986120">A study of a retro-projected robotic face and its effectiveness for gaze reading by humans</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81442614689&CFID=85077225&CFTOKEN=85986120">Fr&#233;d&#233;ric Delaunay</a>, 
                        <a href="author_page.cfm?id=81442614609&CFID=85077225&CFTOKEN=85986120">Joachim de Greeff</a>, 
                        <a href="author_page.cfm?id=81100462469&CFID=85077225&CFTOKEN=85986120">Tony Belpaeme</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 39-44</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734471&ftid=764221&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow15" style="display:inline;"><br /><div style="display:inline">Reading gaze direction is important in human-robot interactions as it supports, among others, joint attention and non-linguistic interaction. While most previous work focuses on implementing gaze direction reading on the robot, little is known about ...</div></span>
          <span id="toHide15" style="display:none;"><br /><div style="display:inline"><p>Reading gaze direction is important in human-robot interactions as it supports, among others, joint attention and non-linguistic interaction. While most previous work focuses on implementing gaze direction reading on the robot, little is known about how the human partner in a human-robot interaction is able to read gaze direction <i>from</i> a robot. The purpose of this paper is twofold: (1) to introduce a new technology to implement robotic face using retro-projected animated faces and (2) to test how well this technology supports gaze reading by humans. We briefly discuss the robot design and discuss parameters influencing the ability to read gaze direction. We present an experiment assessing the user's ability to read gaze direction for a selection of different robotic face designs, using an actual human face as baseline. Results indicate that it is hard to recreate human-human interaction performance. If the robot face is implemented as a semi sphere, performance is worst. While robot faces having a human-like physiognomy and, perhaps surprisingly, video projected on a flat screen perform equally well and seem to suggest that these are the good candidates to implement joint attention in HRI.</p></div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734472&CFID=85077225&CFTOKEN=85986120">Judging a bot by its cover: an experiment on expectation setting for personal robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456634192&CFID=85077225&CFTOKEN=85986120">Steffi Paepcke</a>, 
                        <a href="author_page.cfm?id=81100499212&CFID=85077225&CFTOKEN=85986120">Leila Takayama</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 45-52</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734472&ftid=764222&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow16" style="display:inline;"><br /><div style="display:inline">Managing user expectations of personal robots becomes particularly challenging when the end-user just wants to know what the robot can do, and neither understands nor cares about its technical specifications. In describing what a robot can do to such ...</div></span>
          <span id="toHide16" style="display:none;"><br /><div style="display:inline"><p>Managing user expectations of personal robots becomes particularly challenging when the end-user just wants to know what the robot can do, and neither understands nor cares about its technical specifications. In describing what a robot can do to such an end-user, we explored the questions of (a) whether or not such users would respond to expectation setting about personal robots and, if so, (b) how such expectation setting would influence human-robot interactions and people's perceptions of the robots. Using a 2 (expectation setting: high vs. low) x 2 (robot type: Pleo vs. AIBO) between-participants experiment (N=24), we examined these questions. We found that people's initial beliefs about the robot's capabilities are indeed influenced by expectation setting tactics. Contrary to the hypotheses predicted by the Self-Fulfilling Prophecy and Confirmation Bias, we found that erring on the side of setting expectations lower rather than higher led to less disappointment and more positive appraisals of the robot's competence.</p></div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734473&CFID=85077225&CFTOKEN=85986120">Perception of affect elicited by robot motion</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414620233&CFID=85077225&CFTOKEN=85986120">Martin Saerbeck</a>, 
                        <a href="author_page.cfm?id=81100461702&CFID=85077225&CFTOKEN=85986120">Christoph Bartneck</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 53-60</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734473&ftid=764223&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow17" style="display:inline;"><br /><div style="display:inline">Nonverbal behaviors serve as a rich source of information in inter human communication. In particular, motion cues can reveal details on a person's current physical and mental state. Research has shown, that people do not only interpret motion cues of ...</div></span>
          <span id="toHide17" style="display:none;"><br /><div style="display:inline"><p>Nonverbal behaviors serve as a rich source of information in inter human communication. In particular, motion cues can reveal details on a person's current physical and mental state. Research has shown, that people do not only interpret motion cues of humans in these terms, but also the motion of animals and inanimate devices such as robots. In order to successfully integrate mobile robots in domestic environments, designers have therefore to take into account how the device will be perceived by the user.</p> <p>In this study we analyzed the relationship between motion characteristics of a robot and perceived affect. Based on a literature study we selected two motion characteristics, namely acceleration and curvature, which appear to be most influential for how motion is perceived. We systematically varied these motion parameters and recorded participants interpretations in terms of affective content. Our results suggest a strong relation between motion parameters and attribution of affect, while the type of embodiment had no effect. Furthermore, we found that the level of acceleration can be used to predict perceived arousal and that valence information is at least partly encoded in an interaction between acceleration and curvature. These findings are important for the design of behaviors for future autonomous household robots.</p></div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734474&CFID=85077225&CFTOKEN=85986120">Cooperative gestures: effective signaling for humanoid robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325489801&CFID=85077225&CFTOKEN=85986120">Laurel D. Riek</a>, 
                        <a href="author_page.cfm?id=81414621180&CFID=85077225&CFTOKEN=85986120">Tal-Chen Rabinowitch</a>, 
                        <a href="author_page.cfm?id=81456621774&CFID=85077225&CFTOKEN=85986120">Paul Bremner</a>, 
                        <a href="author_page.cfm?id=81100152547&CFID=85077225&CFTOKEN=85986120">Anthony G. Pipe</a>, 
                        <a href="author_page.cfm?id=81100364452&CFID=85077225&CFTOKEN=85986120">Mike Fraser</a>, 
                        <a href="author_page.cfm?id=81350590377&CFID=85077225&CFTOKEN=85986120">Peter Robinson</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 61-68</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734474&ftid=764224&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow18" style="display:inline;"><br /><div style="display:inline">Cooperative gestures are a key aspect of human-human pro-social interaction. Thus, it is reasonable to expect that endowing humanoid robots with the ability to use such gestures when interacting with humans would be useful. However, while people are ...</div></span>
          <span id="toHide18" style="display:none;"><br /><div style="display:inline"><p>Cooperative gestures are a key aspect of human-human pro-social interaction. Thus, it is reasonable to expect that endowing humanoid robots with the ability to use such gestures when interacting with humans would be useful. However, while people are used to responding to such gestures expressed by other humans, it is unclear how they might react to a robot making them. To explore this topic, we conducted a within-subjects, video based laboratory experiment, measuring time to cooperate with a humanoid robot making interactional gestures. We manipulated the gesture type (beckon, give, shake hands), the gesture style (smooth, abrupt), and the gesture orientation (front, side). We also employed two measures of individual differences: negative attitudes toward robots (NARS) and human gesture decoding ability (DANVA2-POS). Our results show that people cooperate with abrupt gestures more quickly than smooth ones and front-oriented gestures more quickly than those made to the side, people's speed at decoding robot gestures is correlated with their ability to decode human gestures, and negative attitudes toward robots is strongly correlated with a decreased ability in decoding human gestures.</p></div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Late-breaking abstracts chairs' welcome</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Bilge Mutlu, Andrea L. Thomaz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734475&ftid=764225&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="2">POSTER SESSION: <strong>Late-breaking abstracts session/poster session 1</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Andrea Thomaz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734477&CFID=85077225&CFTOKEN=85986120">Towards robust human robot collaboration in industrial environments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456636012&CFID=85077225&CFTOKEN=85986120">Batu Akan</a>, 
                        <a href="author_page.cfm?id=81456632225&CFID=85077225&CFTOKEN=85986120">Baran &#199;&#252;r&#252;kl&#252;</a>, 
                        <a href="author_page.cfm?id=81456626939&CFID=85077225&CFTOKEN=85986120">Giacomo Spampinato</a>, 
                        <a href="author_page.cfm?id=81100003613&CFID=85077225&CFTOKEN=85986120">Lars Asplund</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 71-72</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734477&ftid=764226&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow21" style="display:inline;"><br /><div style="display:inline">In this paper a system, which is driven through natural language, that allows operators to select and manipulate objects in the environment using an industrial robot is proposed. In order to hide the complexities of robot programming we propose a natural ...</div></span>
          <span id="toHide21" style="display:none;"><br /><div style="display:inline"><p>In this paper a system, which is driven through natural language, that allows operators to select and manipulate objects in the environment using an industrial robot is proposed. In order to hide the complexities of robot programming we propose a natural language where the user can control and jog the robot based on reference objects in the scene. We used semantic networks to relate different types of objects in the scene.</p></div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734478&CFID=85077225&CFTOKEN=85986120">Similarities and differences in users' interaction with a humanoid and a pet robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81384590877&CFID=85077225&CFTOKEN=85986120">Anja Austermann</a>, 
                        <a href="author_page.cfm?id=81321500077&CFID=85077225&CFTOKEN=85986120">Seiji Yamada</a>, 
                        <a href="author_page.cfm?id=81100237137&CFID=85077225&CFTOKEN=85986120">Kotaro Funakoshi</a>, 
                        <a href="author_page.cfm?id=81100633290&CFID=85077225&CFTOKEN=85986120">Mikio Nakano</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 73-74</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734478&ftid=764227&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow22" style="display:inline;"><br /><div style="display:inline">In this paper, we compare user behavior towards the humanoid robot ASIMO and the dog-shaped robot AIBO in a simple task, in which the users has to teach commands and feedback to the robot.</div></span>
          <span id="toHide22" style="display:none;"><br /><div style="display:inline"><p>In this paper, we compare user behavior towards the humanoid robot ASIMO and the dog-shaped robot AIBO in a simple task, in which the users has to teach commands and feedback to the robot.</p></div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734479&CFID=85077225&CFTOKEN=85986120">Create children, not robots!</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100461702&CFID=85077225&CFTOKEN=85986120">Christoph Bartneck</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 75-76</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734479&ftid=764228&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow23" style="display:inline;"><br /><div style="display:inline">This essay investigates the situation of young researchers in the HRI community. I argue that we need to have a more child friendly environment to encourage young staff members to create children.</div></span>
          <span id="toHide23" style="display:none;"><br /><div style="display:inline"><p>This essay investigates the situation of young researchers in the HRI community. I argue that we need to have a more child friendly environment to encourage young staff members to create children.</p></div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734480&CFID=85077225&CFTOKEN=85986120">Robots, children, and helping: do children help a robot in need?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456615287&CFID=85077225&CFTOKEN=85986120">Tanya N. Beran</a>, 
                        <a href="author_page.cfm?id=81456611646&CFID=85077225&CFTOKEN=85986120">Alejandro Ramirez-Serrano</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 77-78</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734480&ftid=764229&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow24" style="display:inline;"><br /><div style="display:inline">This study examined the interactions between children and robots by observing whether children help a robot complete a task under five conditions to determine which elicited the most help. Each condition had an experimental and control group, with 20-32 ...</div></span>
          <span id="toHide24" style="display:none;"><br /><div style="display:inline"><p>This study examined the interactions between children and robots by observing whether children help a robot complete a task under five conditions to determine which elicited the most help. Each condition had an experimental and control group, with 20-32 children (even number of boys and girls) in each group. Visitors to a science centre located in a major Western Canadian city were invited to participate in an experiment set up at the centre. Their behaviors with a robot, a small 5 degree of freedom robot arm, were observed. Results of chi-square analyses indicated that children are most likely to help a robot after being introduced to it, <i>X<sup>2</sup></i>(1) = 4.15, <i>p</i> = .04. This condition was the only one of five tested that demonstrated a significant increase in children's helping behaviors. These results suggest that an adult's demonstrated positive introduction to a robot impacts children's helping behaviors towards it.</p></div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734481&CFID=85077225&CFTOKEN=85986120">Learning context-based feature descriptors for object tracking</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81418595362&CFID=85077225&CFTOKEN=85986120">Ali Borji</a>, 
                        <a href="author_page.cfm?id=81322493710&CFID=85077225&CFTOKEN=85986120">Simone Frintrop</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 79-80</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734481&ftid=764230&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow25" style="display:inline;"><br /><div style="display:inline">A major problem with previous object tracking approaches is adapting object representations depending on scene context to account for changes in illumination, viewpoint changes, etc. To adapt our previous approach to deal with background changes, here ...</div></span>
          <span id="toHide25" style="display:none;"><br /><div style="display:inline"><p>A major problem with previous object tracking approaches is adapting object representations depending on scene context to account for changes in illumination, viewpoint changes, etc. To adapt our previous approach to deal with background changes, here we first derive some clusters from a training sequence and the corresponding object representations for those clusters. Next, for each frame of a separate test sequence, its nearest background cluster is determined and then the corresponding descriptor of that cluster is used for object representation in this frame. Experiments show that the proposed approach tracks objects and persons in natural scenes more effectively.</p></div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734482&CFID=85077225&CFTOKEN=85986120">RoboLeader: an agent for supervisory control of multiple robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414613395&CFID=85077225&CFTOKEN=85986120">Jessie Y.C. Chen</a>, 
                        <a href="author_page.cfm?id=81100476496&CFID=85077225&CFTOKEN=85986120">Michael J. Barnes</a>, 
                        <a href="author_page.cfm?id=81100337797&CFID=85077225&CFTOKEN=85986120">Zhihua Qu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 81-82</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734482&ftid=764231&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow26" style="display:inline;"><br /><div style="display:inline">We developed an intelligent agent, RoboLeader, that could assist human operators in route planning for a team of ground robots. We compared the operators' target detection performance in the 4-robot and 8-robot conditions. Results showed that the participants ...</div></span>
          <span id="toHide26" style="display:none;"><br /><div style="display:inline"><p>We developed an intelligent agent, RoboLeader, that could assist human operators in route planning for a team of ground robots. We compared the operators' target detection performance in the 4-robot and 8-robot conditions. Results showed that the participants detected significantly less targets and had significantly worse situation awareness when there were 8 robots compared to the 4-robot condition. Those participants with higher spatial ability detected more targets than did those with lower spatial ability. Participants' self-assessed workload was affected by the number of robots under control, their gender, and their attentional control ability.</p></div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734483&CFID=85077225&CFTOKEN=85986120">Evaluation of on screen navigational methods for a touch screen device</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456633772&CFID=85077225&CFTOKEN=85986120">Andrew Ho Siyong</a>, 
                        <a href="author_page.cfm?id=81456616805&CFID=85077225&CFTOKEN=85986120">Chua Wei Liang Kenny</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 83-84</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734483&ftid=764232&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow27" style="display:inline;"><br /><div style="display:inline">This study involves the design and evaluation of control methods for a touch screen device to enable effective navigation for UGVs (Unmanned Ground Vehicles). 6 different control methods were designed and evaluated. An experiment was conducted requiring ...</div></span>
          <span id="toHide27" style="display:none;"><br /><div style="display:inline"><p>This study involves the design and evaluation of control methods for a touch screen device to enable effective navigation for UGVs (Unmanned Ground Vehicles). 6 different control methods were designed and evaluated. An experiment was conducted requiring participants to conduct navigational tasks. Analysis considers number of errors committed, task completion time and user preference.</p></div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734484&CFID=85077225&CFTOKEN=85986120">Towards industrial robots with human-like moral responsibilities</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456632225&CFID=85077225&CFTOKEN=85986120">Baran &#199;&#252;r&#252;kl&#252;</a>, 
                        <a href="author_page.cfm?id=81100463048&CFID=85077225&CFTOKEN=85986120">Gordana Dodig-Crnkovic</a>, 
                        <a href="author_page.cfm?id=81456636012&CFID=85077225&CFTOKEN=85986120">Batu Akan</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 85-86</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734484&ftid=764233&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow28" style="display:inline;"><br /><div style="display:inline">Robots do not have any capability of taking moral responsibility. At the same time industrial robotics is entering a new era with "intelligent" robots sharing workbench with humans. Teams consisting of humans and industrial robots are no longer science ...</div></span>
          <span id="toHide28" style="display:none;"><br /><div style="display:inline"><p>Robots do not have any capability of taking moral responsibility. At the same time industrial robotics is entering a new era with "intelligent" robots sharing workbench with humans. Teams consisting of humans and industrial robots are no longer science fiction. The biggest worry in this scenario is the fear of humans losing control and robots running amok. We believe that the current way of implementing safety measures have shortcomings, and cannot address challenges related to close collaboration between humans and robots. We propose that "intelligent" industrial robots of the future should have moral responsibilities towards their human colleagues. We also propose that implementation of moral responsibility is radically different from standard safety measures.</p></div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734485&CFID=85077225&CFTOKEN=85986120">Neel: an intelligent shopping guide - using web data for rich interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414609591&CFID=85077225&CFTOKEN=85986120">Chandan Datta</a>, 
                        <a href="author_page.cfm?id=81456612183&CFID=85077225&CFTOKEN=85986120">Ritukar Vijay</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 87-88</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734485&ftid=764234&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow29" style="display:inline;"><br /><div style="display:inline">The project Myneel and its portal myneel.com together were envisaged to provide us with crucial insights into the commercialization of service robots. In this paper we describe our system and propose an approach to develop an interactive conversational ...</div></span>
          <span id="toHide29" style="display:none;"><br /><div style="display:inline"><p>The project Myneel and its portal myneel.com together were envisaged to provide us with crucial insights into the commercialization of service robots. In this paper we describe our system and propose an approach to develop an interactive conversational agent which can serve shopping needs of the visitors in a shopping mall.</p></div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734486&CFID=85077225&CFTOKEN=85986120">An adaptive probabilistic graphical model for representing skills in pbd settings</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81384598020&CFID=85077225&CFTOKEN=85986120">Haris Dindo</a>, 
                        <a href="author_page.cfm?id=81456635340&CFID=85077225&CFTOKEN=85986120">Guido Schillaci</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 89-90</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734486&ftid=764235&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow30" style="display:inline;"><br /><div style="display:inline">Understanding and efficiently representing skills is one of the most important problems in a general Programming by Demonstration (PbD) paradigm. We present Growing Hierarchical Dynamic Bayesian Networks (GHDBN), an adaptive variant of the general DBN ...</div></span>
          <span id="toHide30" style="display:none;"><br /><div style="display:inline"><p>Understanding and efficiently representing skills is one of the most important problems in a general Programming by Demonstration (PbD) paradigm. We present Growing Hierarchical Dynamic Bayesian Networks (GHDBN), an adaptive variant of the general DBN model able to learn and to represent complex skills. The structure of the model, in terms of number of states and possible transitions between them, is not needed to be known <i>a priori</i>. Learning in the model is performed incrementally and in an unsupervised manner.</p></div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734487&CFID=85077225&CFTOKEN=85986120">A midsummer night's dream: social proof in HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456612928&CFID=85077225&CFTOKEN=85986120">Brittany A. Duncan</a>, 
                        <a href="author_page.cfm?id=81100545387&CFID=85077225&CFTOKEN=85986120">Robin R. Murphy</a>, 
                        <a href="author_page.cfm?id=81418595064&CFID=85077225&CFTOKEN=85986120">Dylan Shell</a>, 
                        <a href="author_page.cfm?id=81456616309&CFID=85077225&CFTOKEN=85986120">Amy G. Hopper</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 91-92</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734487&ftid=764236&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow31" style="display:inline;"><br /><div style="display:inline">The introduction of two types of unmanned aerial vehicles into a production of A Midsummer Night's Dream suggests that social proof informs untrained human groups. We describe the metaphors used in instructing actors, who were otherwise untrained and ...</div></span>
          <span id="toHide31" style="display:none;"><br /><div style="display:inline"><p>The introduction of two types of unmanned aerial vehicles into a production of A Midsummer Night's Dream suggests that social proof informs untrained human groups. We describe the metaphors used in instructing actors, who were otherwise untrained and inexperienced with robots, in order to shape their expectations. Audience response to a robot crash depended on whether the audience had seen how the actors interacted with the robot "baby fairies." If they had not seen the actors treating a robot gently, an audience member would likely throw the robot expecting it to fly or handle it roughly. If they had seen the actors with the robots, the audience appeared to adopt the same gentle style and mechanisms for re-launching the micro-helicopter. The difference in audience behavior suggests that the principle of social proof will govern how untrained humans will react to robots.</p></div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734488&CFID=85077225&CFTOKEN=85986120">iForgot: a model of forgetting in robotic memories</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100120238&CFID=85077225&CFTOKEN=85986120">Cathal Gurrin</a>, 
                        <a href="author_page.cfm?id=81100391083&CFID=85077225&CFTOKEN=85986120">Hyowon Lee</a>, 
                        <a href="author_page.cfm?id=81456611684&CFID=85077225&CFTOKEN=85986120">Jer Hayes</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 93-94</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734488&ftid=764237&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow32" style="display:inline;"><br /><div style="display:inline">Much effort has focused in recent years on developing more life-like robots. In this paper we propose a model of memory for robots, based on human digital memories, though our model incorporates an element of forgetting to ensure that the robotic memory ...</div></span>
          <span id="toHide32" style="display:none;"><br /><div style="display:inline"><p>Much effort has focused in recent years on developing more life-like robots. In this paper we propose a model of memory for robots, based on human digital memories, though our model incorporates an element of forgetting to ensure that the robotic memory appears more human and therefore can address some of the challenges for human-robot interaction.</p></div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734489&CFID=85077225&CFTOKEN=85986120">Exploring emotive actuation and its role in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456623140&CFID=85077225&CFTOKEN=85986120">John Harris</a>, 
                        <a href="author_page.cfm?id=81339528031&CFID=85077225&CFTOKEN=85986120">Ehud Sharlin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 95-96</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734489&ftid=764238&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow33" style="display:inline;"><br /><div style="display:inline">In this paper, we present our research efforts in exploring the role of motion and actuation in human-robot interaction. We define Emotive Actuation, and briefly discuss its function and importance in social robotic interaction. We propose a suggested ...</div></span>
          <span id="toHide33" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present our research efforts in exploring the role of motion and actuation in human-robot interaction. We define <i>Emotive Actuation</i>, and briefly discuss its function and importance in social robotic interaction. We propose a suggested methodology for exploring <i>Emotive Actuation</i> in HRI, and present a robotic testbed we designed for this purpose. We conclude with informal results of a preliminary design critique we performed using our testbed.</p></div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734490&CFID=85077225&CFTOKEN=85986120">Multi-touch interaction for tasking robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456613388&CFID=85077225&CFTOKEN=85986120">Sean Timothy Hayes</a>, 
                        <a href="author_page.cfm?id=81456640080&CFID=85077225&CFTOKEN=85986120">Eli R. Hooten</a>, 
                        <a href="author_page.cfm?id=81100313646&CFID=85077225&CFTOKEN=85986120">Julie A. Adams</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 97-98</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734490&ftid=764239&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow34" style="display:inline;"><br /><div style="display:inline">The objective is to develop a mobile human-robot interface that is optimized for multi-touch input. Our existing interface was designed for mouse and keyboard input and was later adopted for voice and touch interaction. A new multi-touch interface permits ...</div></span>
          <span id="toHide34" style="display:none;"><br /><div style="display:inline"><p>The objective is to develop a mobile human-robot interface that is optimized for multi-touch input. Our existing interface was designed for mouse and keyboard input and was later adopted for voice and touch interaction. A new multi-touch interface permits multi-touch gestures, for example zooming and panning a map, and robot task specific touch interactions. An initial user evaluation found that the multi-touch interface is preferred and yields superior performance.</p></div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734491&CFID=85077225&CFTOKEN=85986120">Active navigation landmarks for a service robot in a home environment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81384618568&CFID=85077225&CFTOKEN=85986120">Kentaro Ishii</a>, 
                        <a href="author_page.cfm?id=81456611271&CFID=85077225&CFTOKEN=85986120">Akihiko Ishida</a>, 
                        <a href="author_page.cfm?id=81453626910&CFID=85077225&CFTOKEN=85986120">Greg Saul</a>, 
                        <a href="author_page.cfm?id=81100424140&CFID=85077225&CFTOKEN=85986120">Masahiko Inami</a>, 
                        <a href="author_page.cfm?id=81100444444&CFID=85077225&CFTOKEN=85986120">Takeo Igarashi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 99-100</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734491&ftid=764240&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow35" style="display:inline;"><br /><div style="display:inline">This paper proposes a physical user interface for a user to teach a robot to navigate a home environment. The user places small devices containing infrared based communication functionality as landmarks in the environment. The robot follows these landmarks ...</div></span>
          <span id="toHide35" style="display:none;"><br /><div style="display:inline"><p>This paper proposes a physical user interface for a user to teach a robot to navigate a home environment. The user places small devices containing infrared based communication functionality as landmarks in the environment. The robot follows these landmarks to navigate to a goal landmark. Active landmarks communicate with each other to map their spatial relationships. Our method allows the user to start using the system immediately after placing the landmarks without installing any global position sensing system or prior mapping by the robot.</p></div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734492&CFID=85077225&CFTOKEN=85986120">Toward coactivity</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100451427&CFID=85077225&CFTOKEN=85986120">Matthew Johnson</a>, 
                        <a href="author_page.cfm?id=81100452561&CFID=85077225&CFTOKEN=85986120">Jeffrey M. Bradshaw</a>, 
                        <a href="author_page.cfm?id=81100238977&CFID=85077225&CFTOKEN=85986120">Paul J. Feltovich</a>, 
                        <a href="author_page.cfm?id=81100393293&CFID=85077225&CFTOKEN=85986120">Catholijn Jonker</a>, 
                        <a href="author_page.cfm?id=81100440761&CFID=85077225&CFTOKEN=85986120">Maarten Sierhuis</a>, 
                        <a href="author_page.cfm?id=81100453465&CFID=85077225&CFTOKEN=85986120">Birna van Riemsdijk</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 101-102</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734492&ftid=764241&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow36" style="display:inline;"><br /><div style="display:inline">This paper introduces the concept of Coactivity as a new focal point for Human-Robot Interaction to address the more sophisticated roles of partner or teammate envisioned for future human-robot systems. We propose that most approaches to date ...</div></span>
          <span id="toHide36" style="display:none;"><br /><div style="display:inline"><p>This paper introduces the concept of <i>Coactivity</i> as a new focal point for Human-Robot Interaction to address the more sophisticated roles of partner or teammate envisioned for future human-robot systems. We propose that most approaches to date have focused on autonomy and suggest that autonomy is the wrong focal point. The envisioned roles, if properly performed, have a high level of interdependence that cannot be addressed solely by autonomy and necessitate a focus on the <i>coactivity</i>.</p></div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734493&CFID=85077225&CFTOKEN=85986120">A code of ethics for robotics engineers</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456626835&CFID=85077225&CFTOKEN=85986120">Brandon Ingram</a>, 
                        <a href="author_page.cfm?id=81456612727&CFID=85077225&CFTOKEN=85986120">Daniel Jones</a>, 
                        <a href="author_page.cfm?id=81456626781&CFID=85077225&CFTOKEN=85986120">Andrew Lewis</a>, 
                        <a href="author_page.cfm?id=81456616528&CFID=85077225&CFTOKEN=85986120">Matthew Richards</a>, 
                        <a href="author_page.cfm?id=81100311645&CFID=85077225&CFTOKEN=85986120">Charles Rich</a>, 
                        <a href="author_page.cfm?id=81456639091&CFID=85077225&CFTOKEN=85986120">Lance Schachterle</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 103-104</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734493&ftid=764242&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow37" style="display:inline;"><br /><div style="display:inline">The future of robotics engineering is in the hands of engineers and must be handled to ensure the safety of all people and the reputation of the field. We are in the process of developing a code of ethics for professional robotics engineers to serve ...</div></span>
          <span id="toHide37" style="display:none;"><br /><div style="display:inline"><p>The future of robotics engineering is in the hands of engineers and must be handled to ensure the safety of all people and the reputation of the field. We are in the process of developing a code of ethics for professional robotics engineers to serve as a guideline for the ethical development of the field. This document contains the current version of this code and describes the methodology used in developing it.</p></div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734494&CFID=85077225&CFTOKEN=85986120">Sociable dining table: the effectiveness of a "konkon" interface for reciprocal adaptation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456619641&CFID=85077225&CFTOKEN=85986120">Yuki Kado</a>, 
                        <a href="author_page.cfm?id=81456639575&CFID=85077225&CFTOKEN=85986120">Takanori Kamoda</a>, 
                        <a href="author_page.cfm?id=81456636141&CFID=85077225&CFTOKEN=85986120">Yuta Yoshiike</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=85077225&CFTOKEN=85986120">P. Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=85077225&CFTOKEN=85986120">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 105-106</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734494&ftid=764243&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow38" style="display:inline;"><br /><div style="display:inline">We developed a creatures-based social dining table that can communicate through a knocking sound, which in Japanese is pronounced as "KonKon." Our main focus was to create a minimal number of cues for a proto-communication by establishing social interactions ...</div></span>
          <span id="toHide38" style="display:none;"><br /><div style="display:inline"><p>We developed a creatures-based social dining table that can communicate through a knocking sound, which in Japanese is pronounced as "KonKon." Our main focus was to create a minimal number of cues for a proto-communication by establishing social interactions between a creature and a human. In particular, humans used the "KonKon" interface to communicate with a creature to demonstrate the social behaviors necessary to adapt to a person's intentions. The creature used a mutual adaptation model for achieving a more ideal adaptation during the interactions. In the experimental results, we discuss the concept of the creature and indicate the effectiveness of the communication-protocol on the "KonKon" interface for mutual adaptation.</p></div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734495&CFID=85077225&CFTOKEN=85986120">Effects of intergroup relations on people's acceptance of robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456617526&CFID=85077225&CFTOKEN=85986120">Yunkyung Kim</a>, 
                        <a href="author_page.cfm?id=81339511173&CFID=85077225&CFTOKEN=85986120">Sonya S. Kwak</a>, 
                        <a href="author_page.cfm?id=81414595687&CFID=85077225&CFTOKEN=85986120">Myung-suk Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 107-108</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734495&ftid=764244&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow39" style="display:inline;"><br /><div style="display:inline">The objective of this study is to examine the effect of intergroup relations on robots through comparison with other objects. In an experiment, participants watched eight stimuli drawn from four types of objects (people vs. robots vs. animals vs. products) ...</div></span>
          <span id="toHide39" style="display:none;"><br /><div style="display:inline"><p>The objective of this study is to examine the effect of intergroup relations on robots through comparison with other objects. In an experiment, participants watched eight stimuli drawn from four types of objects (people vs. robots vs. animals vs. products) according to two types of intergroup relations (in-group vs. out-group) and rated each stimuli in terms of familiarity, reliability, and preference. Regarding familiarity and reliability, the effect of intergroup relations on robots was greater than that on animals or products, but smaller than that on people. The degree of the effect regarding reliability was larger than that regarding familiarity for all types of object. In the case of preference, the effects of intergroup relations between people and robots and between animals and products were similar, and the effect on people and robots was greater than that on animals and products.</p></div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734496&CFID=85077225&CFTOKEN=85986120">Choosing answerers by observing gaze responses for museum guide robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100604144&CFID=85077225&CFTOKEN=85986120">Yoshinori Kobayashi</a>, 
                        <a href="author_page.cfm?id=81456616700&CFID=85077225&CFTOKEN=85986120">Takashi Shibata</a>, 
                        <a href="author_page.cfm?id=81416597851&CFID=85077225&CFTOKEN=85986120">Yosuke Hoshi</a>, 
                        <a href="author_page.cfm?id=81100082332&CFID=85077225&CFTOKEN=85986120">Yoshinori Kuno</a>, 
                        <a href="author_page.cfm?id=81416604529&CFID=85077225&CFTOKEN=85986120">Mai Okada</a>, 
                        <a href="author_page.cfm?id=81350572857&CFID=85077225&CFTOKEN=85986120">Keiichi Yamazaki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 109-110</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734496&ftid=764245&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow40" style="display:inline;"><br /><div style="display:inline">This paper presents a method of selecting the answerer from audiences for a museum guide robot. We performed the preliminary experiments that a robot distributed its gaze towards visitors to select an answerer and analyzed visitors' responses. From these ...</div></span>
          <span id="toHide40" style="display:none;"><br /><div style="display:inline"><p>This paper presents a method of selecting the answerer from audiences for a museum guide robot. We performed the preliminary experiments that a robot distributed its gaze towards visitors to select an answerer and analyzed visitors' responses. From these experiments, we have found that the visitors who are asked questions by the robot feel embarrassed when they have no prior knowledge about the question and the visitor's gaze during the question plays an important role to avoid being asked question. Based on these findings we developed functions for a guide robot to select the answerer by observing behaviors of multiple visitors. Multiple visitors' head motions are tracked and recognized by using an omni-directional camera and a laser range sensor. The robot detects the visitors' positive and negative responses by observing their head motions while asking questions. We confirmed the effectiveness of our method by experiments.</p></div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734497&CFID=85077225&CFTOKEN=85986120">From cartoons to robots: facial regions as cues to recognize emotions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309512404&CFID=85077225&CFTOKEN=85986120">Tomoko Koda</a>, 
                        <a href="author_page.cfm?id=81456624468&CFID=85077225&CFTOKEN=85986120">Yuka Nakagawa</a>, 
                        <a href="author_page.cfm?id=81456629988&CFID=85077225&CFTOKEN=85986120">Kyota Tabuchi</a>, 
                        <a href="author_page.cfm?id=81100237640&CFID=85077225&CFTOKEN=85986120">Zsofia Ruttkay</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 111-112</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734497&ftid=764246&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow41" style="display:inline;"><br /><div style="display:inline">This paper introduces a preliminary result of a cross-cultural study on the facial regions as cues to recognize virtual agents' facial expressions. We believe providing research results on the perception of cartoonish virtual agents' facial expressions ...</div></span>
          <span id="toHide41" style="display:none;"><br /><div style="display:inline"><p>This paper introduces a preliminary result of a cross-cultural study on the facial regions as cues to recognize virtual agents' facial expressions. We believe providing research results on the perception of cartoonish virtual agents' facial expressions to HRI research community is meaningful in order to minimize the effort to develop robot's facial expressions. The result implies 1) the mouth region is more effective in conveying the emotions of the facial expressions than the eye region, 2) there are cultural differences in using facial regions as cues to recognize cartoonish facial expressions between Hungary and Japan.</p></div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734498&CFID=85077225&CFTOKEN=85986120">Human training using HRI approach based on fuzzy ARTMap networks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456621307&CFID=85077225&CFTOKEN=85986120">Felipe Machorro-Fern&#225;ndez</a>, 
                        <a href="author_page.cfm?id=81430643967&CFID=85077225&CFTOKEN=85986120">Vicente Parra-Vega</a>, 
                        <a href="author_page.cfm?id=81456611697&CFID=85077225&CFTOKEN=85986120">Ismael L&#243;pez-Ju&#225;rez</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 113-114</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734498&ftid=764247&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow42" style="display:inline;"><br /><div style="display:inline">Based on recent studies which establishes that skill acquisition requires not just specification of motor skills, learning and skill application but also intervention of human expert only in certain phases, we present an approach which encode the human ...</div></span>
          <span id="toHide42" style="display:none;"><br /><div style="display:inline"><p>Based on recent studies which establishes that skill acquisition requires not just specification of motor skills, learning and skill application but also intervention of human expert only in certain phases, we present an approach which encode the human expert demonstration into a teacher class based on Fuzzy ArtMap network. Then, the human novice trainee produces the approximate knowledge, which is in turn coded into student class. The evaluation function introduces a class metric which simultaneously allows the student to refine motor commands to increase the trainee pace while modifies accordingly the desired trajectory of the robot. Preliminary experiments indicates a high success rate in contact robotic tasks, in a deterministic setting.</p></div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734499&CFID=85077225&CFTOKEN=85986120">Robot-assisted upper-limb rehabilitation platform</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456638936&CFID=85077225&CFTOKEN=85986120">Matteo Malosio</a>, 
                        <a href="author_page.cfm?id=81456638312&CFID=85077225&CFTOKEN=85986120">Nicola Pedrocchi</a>, 
                        <a href="author_page.cfm?id=81456639583&CFID=85077225&CFTOKEN=85986120">Lorenzo Molinari Tosatti</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 115-116</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734499&ftid=764248&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow43" style="display:inline;"><br /><div style="display:inline">This work presents a robotic platform for upper-limb rehabilitation robotics. It integrates devices for human multi-sensorial feedback for engaging and immersive therapies. Its modular software design and architecture allows the implementation of advanced ...</div></span>
          <span id="toHide43" style="display:none;"><br /><div style="display:inline"><p>This work presents a robotic platform for upper-limb rehabilitation robotics. It integrates devices for human multi-sensorial feedback for engaging and immersive therapies. Its modular software design and architecture allows the implementation of advanced control algorithms for effective and customized rehabilitations. A flexible communication infrastructure allows straightforward devices integration and system expandability.</p></div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734500&CFID=85077225&CFTOKEN=85986120">The development of small size humanoid robot which is easy to use</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456612963&CFID=85077225&CFTOKEN=85986120">Hirofumi Niimi</a>, 
                        <a href="author_page.cfm?id=81456617543&CFID=85077225&CFTOKEN=85986120">Minoru Koike</a>, 
                        <a href="author_page.cfm?id=81456609174&CFID=85077225&CFTOKEN=85986120">Seiichi Takeuchi</a>, 
                        <a href="author_page.cfm?id=81456636153&CFID=85077225&CFTOKEN=85986120">Noriyoshi Douhara</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 117-118</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734500&ftid=764249&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow44" style="display:inline;"><br /><div style="display:inline">We designed humanoid robots based on the skeleton. They were easy to make the motions by designing them in the proportion of human. The motions of crawl on the hands and knees, roll-over and crawl on the back were made by using humanoid robot SANDY-3.</div></span>
          <span id="toHide44" style="display:none;"><br /><div style="display:inline"><p>We designed humanoid robots based on the skeleton. They were easy to make the motions by designing them in the proportion of human. The motions of crawl on the hands and knees, roll-over and crawl on the back were made by using humanoid robot SANDY-3.</p></div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734501&CFID=85077225&CFTOKEN=85986120">Application of unexpectedness to the behavioral design of an entertainment robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456628965&CFID=85077225&CFTOKEN=85986120">Hyojung Oh</a>, 
                        <a href="author_page.cfm?id=81339511173&CFID=85077225&CFTOKEN=85986120">Sonya S. Kwak</a>, 
                        <a href="author_page.cfm?id=81414595687&CFID=85077225&CFTOKEN=85986120">Myung-Suk Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 119-120</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734501&ftid=764250&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow45" style="display:inline;"><br /><div style="display:inline">The objectives of this study are to apply unexpectedness to the behavioral design of an entertainment robot and to evaluate the impression and satisfaction provided by the robot. Participants(N=44) observed four robot behaviors, which are distinguished ...</div></span>
          <span id="toHide45" style="display:none;"><br /><div style="display:inline"><p>The objectives of this study are to apply unexpectedness to the behavioral design of an entertainment robot and to evaluate the impression and satisfaction provided by the robot. Participants(N=44) observed four robot behaviors, which are distinguished by type of expectancy disconfirmation (positive disconfirmation, negative disconfirmation, simply confirmation, unexpected disconfirmation), and evaluated each behavior in terms of novelty, enjoyment, satisfaction, performance, and reliability. Participants perceived the unexpected disconfirmation behavior to be more novel and enjoyable such that they preferred this type over the other types. On the other hand, they evaluated the positive disconfirmation behavior as more intelligent and reliable than the other types. These findings will provide an essential basis for designing the behavior of an entertainment robot with the use of unexpectedness.</p></div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734502&CFID=85077225&CFTOKEN=85986120">Design guidelines for industrial power assist robots for lifting heavy objects based on human's weight perception for better HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453642491&CFID=85077225&CFTOKEN=85986120">S.M. Mizanoor Rahman</a>, 
                        <a href="author_page.cfm?id=81453660531&CFID=85077225&CFTOKEN=85986120">Ryojun Ikeura</a>, 
                        <a href="author_page.cfm?id=81453661574&CFID=85077225&CFTOKEN=85986120">Masaya Nobe</a>, 
                        <a href="author_page.cfm?id=81453616491&CFID=85077225&CFTOKEN=85986120">Hideki Sawai</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 121-122</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734502&ftid=764251&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow46" style="display:inline;"><br /><div style="display:inline">We hypothesized that weight perception (WP) due to inertia might be different from WP due to gravity for lifting an object with a power assist robot (PAR). Objects were lifted with a PAR independently under three different lifting schemes-unimanual, ...</div></span>
          <span id="toHide46" style="display:none;"><br /><div style="display:inline"><p>We hypothesized that weight perception (WP) due to inertia might be different from WP due to gravity for lifting an object with a power assist robot (PAR). Objects were lifted with a PAR independently under three different lifting schemes-unimanual, bimanual, and cooperative lift. Then, psychophysical relationships between actual and power-assisted weights (PAWs) as well as excess in load forces (LFs) were determined for each scheme separately. A novel control strategy was introduced to reduce the excess in LFs for each scheme. Finally, we proposed to use the findings as design guidelines to design PARs for lifting heavy objects in industries that would improve HRI in terms of human, robot and system.</p></div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734503&CFID=85077225&CFTOKEN=85986120">Psychological intimacy with robots?: using interaction patterns to uncover depth of relation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81408592441&CFID=85077225&CFTOKEN=85986120">Peter H. Kahn, Jr.</a>, 
                        <a href="author_page.cfm?id=81350569693&CFID=85077225&CFTOKEN=85986120">Jolina H. Ruckert</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=85077225&CFTOKEN=85986120">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=85077225&CFTOKEN=85986120">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81456638149&CFID=85077225&CFTOKEN=85986120">Aimee Reichert</a>, 
                        <a href="author_page.cfm?id=81456635299&CFID=85077225&CFTOKEN=85986120">Heather Gary</a>, 
                        <a href="author_page.cfm?id=81456605597&CFID=85077225&CFTOKEN=85986120">Solace Shen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 123-124</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734503&ftid=764252&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow47" style="display:inline;"><br /><div style="display:inline">This conceptual paper broaches possibilities and limits of establishing psychological intimacy in HRI.</div></span>
          <span id="toHide47" style="display:none;"><br /><div style="display:inline"><p>This conceptual paper broaches possibilities and limits of establishing psychological intimacy in HRI.</p></div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734504&CFID=85077225&CFTOKEN=85986120">Exploring interruption in HRI using wizard of oz</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414619149&CFID=85077225&CFTOKEN=85986120">Paul Saulnier</a>, 
                        <a href="author_page.cfm?id=81339528031&CFID=85077225&CFTOKEN=85986120">Ehud Sharlin</a>, 
                        <a href="author_page.cfm?id=81100197069&CFID=85077225&CFTOKEN=85986120">Saul Greenberg</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 125-126</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734504&ftid=764253&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow48" style="display:inline;"><br /><div style="display:inline">We are interested in exploring how robots controlled using Wizard of Oz (WoO) should interrupt humans in various social settings. While there is considerable work on interruption and interruptibility in HCI, little has been done to explore how these ...</div></span>
          <span id="toHide48" style="display:none;"><br /><div style="display:inline"><p>We are interested in exploring how robots controlled using Wizard of Oz (WoO) should interrupt humans in various social settings. While there is considerable work on interruption and interruptibility in HCI, little has been done to explore how these concepts will map robotic interaction. As part of our efforts to investigate interruption and interruptibility in HRI we used WoO-based methodology to investigate robot behaviours in a simple interruption scenario. In this report we contribute a design critique that discusses this methodology, and common concerns that could be generalized to other social HRI experiments as well as reflections on our future interruption HRI research.</p></div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734505&CFID=85077225&CFTOKEN=85986120">Survivor buddy and SciGirls: affect, outreach, and questions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100545387&CFID=85077225&CFTOKEN=85986120">Robin Murphy</a>, 
                        <a href="author_page.cfm?id=81456616934&CFID=85077225&CFTOKEN=85986120">Vasant Srinivasan</a>, 
                        <a href="author_page.cfm?id=81456605479&CFID=85077225&CFTOKEN=85986120">Negar Rashidi</a>, 
                        <a href="author_page.cfm?id=81456612928&CFID=85077225&CFTOKEN=85986120">Brittany Duncan</a>, 
                        <a href="author_page.cfm?id=81309480823&CFID=85077225&CFTOKEN=85986120">Aaron Rice</a>, 
                        <a href="author_page.cfm?id=81456635712&CFID=85077225&CFTOKEN=85986120">Zachary Henkel</a>, 
                        <a href="author_page.cfm?id=81456634890&CFID=85077225&CFTOKEN=85986120">Marco Garza</a>, 
                        <a href="author_page.cfm?id=81100153283&CFID=85077225&CFTOKEN=85986120">Clifford Nass</a>, 
                        <a href="author_page.cfm?id=81416605221&CFID=85077225&CFTOKEN=85986120">Victoria Groom</a>, 
                        <a href="author_page.cfm?id=81337495278&CFID=85077225&CFTOKEN=85986120">Takis Zourntos</a>, 
                        <a href="author_page.cfm?id=81456622422&CFID=85077225&CFTOKEN=85986120">Roozbeh Daneshvar</a>, 
                        <a href="author_page.cfm?id=81456623804&CFID=85077225&CFTOKEN=85986120">Sharath Prasad</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 127-128</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734505&ftid=764254&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow49" style="display:inline;"><br /><div style="display:inline">This paper describes the Survivor Buddy human-robot interaction project and how it was used by four middle-school girls to illustrate the scientific process for an episode of "SciGirls", a Public Broadcast System science reality show. Survivor Buddy ...</div></span>
          <span id="toHide49" style="display:none;"><br /><div style="display:inline"><p>This paper describes the Survivor Buddy human-robot interaction project and how it was used by four middle-school girls to illustrate the scientific process for an episode of "SciGirls", a Public Broadcast System science reality show. Survivor Buddy is a four degree of freedom robot head, with the face being a MIMO 740 multi-media touch screen monitor. It is being used to explore consistency and trust in the use of robots as social mediums, where robots serve as intermediaries between dependents (e.g., trapped survivors) and the outside world (doctors, rescuers, family members). While the SciGirl experimentation was neither statistically significant nor rigorously controlled, the experience makes three contributions. It introduces the Survivor Buddy project and social medium role, it illustrates that human-robot interaction is an appealing way to make robotics more accessible to the general public, and raises interesting questions about the existence of a minimum set of degrees of freedom for sufficient expressiveness, the relative importance of voice versus non-verbal affect, and the range and intensity of robot motions.</p></div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734506&CFID=85077225&CFTOKEN=85986120">Considering the bystander's perspective for indirect human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414601137&CFID=85077225&CFTOKEN=85986120">Katherine M. Tsui</a>, 
                        <a href="author_page.cfm?id=81443595444&CFID=85077225&CFTOKEN=85986120">Munjal Desai</a>, 
                        <a href="author_page.cfm?id=81100443257&CFID=85077225&CFTOKEN=85986120">Holly A. Yanco</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 129-130</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734506&ftid=764255&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734507&CFID=85077225&CFTOKEN=85986120">Interactive story creation for knowledge acquisition</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453647136&CFID=85077225&CFTOKEN=85986120">Shohei Yoshioka</a>, 
                        <a href="author_page.cfm?id=81100418496&CFID=85077225&CFTOKEN=85986120">Takuya Maekawa</a>, 
                        <a href="author_page.cfm?id=81341491389&CFID=85077225&CFTOKEN=85986120">Yasushi Hirano</a>, 
                        <a href="author_page.cfm?id=81100490660&CFID=85077225&CFTOKEN=85986120">Shoji Kajita</a>, 
                        <a href="author_page.cfm?id=81100070056&CFID=85077225&CFTOKEN=85986120">Kenji Mase</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 131-132</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734507&ftid=764256&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow51" style="display:inline;"><br /><div style="display:inline">This paper proposes an agent system that semi-automatically creates stories about daily events detected by ubiquitous sensors. These stories are knowledge of inhabitants' daily lives and it may be useful for human-friendly agent. Story flows in daily ...</div></span>
          <span id="toHide51" style="display:none;"><br /><div style="display:inline"><p>This paper proposes an agent system that semi-automatically creates stories about daily events detected by ubiquitous sensors. These stories are knowledge of inhabitants' daily lives and it may be useful for human-friendly agent. Story flows in daily lives are extracted from interaction between sensor room inhabitants and a symbiotic agent. The agent asks causal relationships among daily events for inhabitants to create the story flow. Experimental results show that created stories let created stories perceive agent's intelligence.</p></div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734508&CFID=85077225&CFTOKEN=85986120">Showing robots how to follow people using a broomstick interface</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81416601902&CFID=85077225&CFTOKEN=85986120">James E. Young</a>, 
                        <a href="author_page.cfm?id=81384618568&CFID=85077225&CFTOKEN=85986120">Kentaro Ishii</a>, 
                        <a href="author_page.cfm?id=81100444444&CFID=85077225&CFTOKEN=85986120">Takeo Igarashi</a>, 
                        <a href="author_page.cfm?id=81339528031&CFID=85077225&CFTOKEN=85986120">Ehud Sharlin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 133-134</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734508&ftid=764257&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow52" style="display:inline;"><br /><div style="display:inline">Robots are poised to enter our everyday environments such as our homes and offices, contexts that present unique questions such as the style of the robot's actions. Style-oriented characteristics are difficult to define programmatically, a problem that ...</div></span>
          <span id="toHide52" style="display:none;"><br /><div style="display:inline"><p>Robots are poised to enter our everyday environments such as our homes and offices, contexts that present unique questions such as the style of the robot's actions. Style-oriented characteristics are difficult to define programmatically, a problem that is particularly prominent for a robot's interactive behaviors, those that must react accordingly to dynamic actions of people. In this paper, we present a technique for programming the style of how a robot should follow a person by demonstration, such that non-technical designers and users can directly create the style of following using their existing skill sets. We envision that simple physical interfaces like ours can be used by non-technical people to design the style of a wide range of robotic behaviors.</p></div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734509&CFID=85077225&CFTOKEN=85986120">Cues for sociable PC: coordinate and synchronize its cues based on user attention and activities on display</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456636141&CFID=85077225&CFTOKEN=85986120">Yuta Yoshiike</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=85077225&CFTOKEN=85986120">P. Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=85077225&CFTOKEN=85986120">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 135-136</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734509&ftid=764258&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow53" style="display:inline;"><br /><div style="display:inline">A sociable PC (SPC) is capable of engaging and interacting with social cues while users use it. SPC is a kind of artifact which is capable of coordinating and synchronizing its behaviors based on user attention and information on a display. In particular, ...</div></span>
          <span id="toHide53" style="display:none;"><br /><div style="display:inline"><p>A sociable PC (SPC) is capable of engaging and interacting with social cues while users use it. SPC is a kind of artifact which is capable of coordinating and synchronizing its behaviors based on user attention and information on a display. In particular, SPC can exhibit behaviors to induce a trust through social rapport with the user while responding to the user's behaviors and activities on a PC. We used the concept of a minimalism designing mechanism to invent the SPC. The SPC appearance is much like soft "Tofu," so the user can touch and sense it. The SPS can also provide feedback to the user using attractive social cues such as shaking its body, displaying an attractive motion and joint attention with the user, etc.</p></div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">POSTER SESSION: <strong>Late-breaking abstracts session/poster session 2</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Bilge Mutlu, Andrea Thomaz 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734511&CFID=85077225&CFTOKEN=85986120">Do children perceive robots as alive?: children's attributions of human characteristics</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456615287&CFID=85077225&CFTOKEN=85986120">Tanya N. Beran</a>, 
                        <a href="author_page.cfm?id=81456611646&CFID=85077225&CFTOKEN=85986120">Alejandro Ramirez-Serrano</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 137-138</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734511&ftid=764259&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow55" style="display:inline;"><br /><div style="display:inline">Centuries ago, the existence of life was explained by the presence of a soul [1]. Known as animism, this term was re-defined in the 1970s by Piaget as young children's beliefs that inanimate objects are capable of actions and have lifelike qualities. ...</div></span>
          <span id="toHide55" style="display:none;"><br /><div style="display:inline"><p>Centuries ago, the existence of life was explained by the presence of a soul [1]. Known as animism, this term was re-defined in the 1970s by Piaget as young children's beliefs that inanimate objects are capable of actions and have lifelike qualities. With the development of robots in the 21st century, researchers have yet to examine whether animism is apparent in children's impressions of robots. The purpose of this study was to examine children's perspectives about the cognitive, affective, and behavioral attributes of a robot. Visitors to a science centre located in a major Western Canadian city were invited to participate in an experiment set up at the centre. A total of 198 children ages 5 to 16 years (M = 8.18 years) with an approximate even number of boys and girls participated. Children were interviewed after observing a robot, a small 5 degree of freedom robot arm, perform a block stacking task. Answers to the six questions about the robot were scored according to whether they referenced humanistic qualities. Frequency and content analysis results suggest that a significant proportion of children ascribe cognitive, affective, and behavioral characteristics to robots.</p></div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734512&CFID=85077225&CFTOKEN=85986120">Effects of operator spatial ability on uav-guided ground navigation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414613395&CFID=85077225&CFTOKEN=85986120">Jessie Y.C. Chen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 139-140</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734512&ftid=764260&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow56" style="display:inline;"><br /><div style="display:inline">We simulated a military reconnaissance environment and examined the performance of ground robotics operators who were instructed to utilize streaming video from an unmanned aerial vehicle to navigate his/her ground robot to the locations of the targets. ...</div></span>
          <span id="toHide56" style="display:none;"><br /><div style="display:inline"><p>We simulated a military reconnaissance environment and examined the performance of ground robotics operators who were instructed to utilize streaming video from an unmanned aerial vehicle to navigate his/her ground robot to the locations of the targets. We evaluated participants' spatial ability and examined if it affected their performance or perceived workload. Results showed that participants with higher spatial ability performed significantly better in target-mapping performance and reported less workload than those with lower spatial ability. Participants with poor sense-of-direction performed significantly worse in the target search task in the night condition compared with those with better sense-of-direction.</p></div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734513&CFID=85077225&CFTOKEN=85986120">Effects of (in)accurate empathy and situational valence on attitudes towards robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81328487965&CFID=85077225&CFTOKEN=85986120">Henriette Cramer</a>, 
                        <a href="author_page.cfm?id=81456639670&CFID=85077225&CFTOKEN=85986120">Jorrit Goddijn</a>, 
                        <a href="author_page.cfm?id=81100478624&CFID=85077225&CFTOKEN=85986120">Bob Wielinga</a>, 
                        <a href="author_page.cfm?id=81100133286&CFID=85077225&CFTOKEN=85986120">Vanessa Evers</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 141-142</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734513&ftid=764261&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow57" style="display:inline;"><br /><div style="display:inline">Empathy has great potential in human-robot interaction. However, the challenging nature of assessing the user's emotional state points to the importance of also understanding the effects of empathic behaviours incongruent with users' affective experience. ...</div></span>
          <span id="toHide57" style="display:none;"><br /><div style="display:inline"><p>Empathy has great potential in human-robot interaction. However, the challenging nature of assessing the user's emotional state points to the importance of also understanding the effects of empathic behaviours incongruent with users' affective experience. A 3x2 between-subject video-based survey experiment (N=133) was conducted with empathic robot behaviour (empathically accurate, neutral, inaccurate) and valence of the situation (positive, negative) as dimensions. Trust decreased when empathic responses were incongruent with the affective state of the user. However, in the negative valence condition, reported perceived empathic abilities were greater when the robot responded as if the situation were positive.</p></div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734514&CFID=85077225&CFTOKEN=85986120">Using proxemics to evaluate human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81385594144&CFID=85077225&CFTOKEN=85986120">David Feil-Seifer</a>, 
                        <a href="author_page.cfm?id=81452618049&CFID=85077225&CFTOKEN=85986120">Maja Matari&#263;</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 143-144</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734514&ftid=764262&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow58" style="display:inline;"><br /><div style="display:inline">Recent feasibility studies involving children with autism spectrum disorders (ASD) interacting with socially assistive robots have shown that children can have both positive and negative reactions to robots. These reactions can be readily identified ...</div></span>
          <span id="toHide58" style="display:none;"><br /><div style="display:inline"><p>Recent feasibility studies involving children with autism spectrum disorders (ASD) interacting with socially assistive robots have shown that children can have both positive and negative reactions to robots. These reactions can be readily identified by a human observer watching videos from an overhead camera. Our goal is to automate the process of such behavior analysis. This paper shows how a heuristic classifier can be used to discriminate between children that are attempting to interact socially with a robot and children that are not.</p></div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734515&CFID=85077225&CFTOKEN=85986120">Is a telepresence-system an effective alternative to manned missions?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456630680&CFID=85077225&CFTOKEN=85986120">Lena Geiger</a>, 
                        <a href="author_page.cfm?id=81100061885&CFID=85077225&CFTOKEN=85986120">Michael Popp</a>, 
                        <a href="author_page.cfm?id=81331492332&CFID=85077225&CFTOKEN=85986120">Berthold F&#228;rber</a>, 
                        <a href="author_page.cfm?id=81384610379&CFID=85077225&CFTOKEN=85986120">Jordi Artigas</a>, 
                        <a href="author_page.cfm?id=81416606278&CFID=85077225&CFTOKEN=85986120">Philipp Kremer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 145-146</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734515&ftid=764263&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow59" style="display:inline;"><br /><div style="display:inline">Telepresence-systems have the potential to take on an important role in on-orbit servicing scenarios. In comparison to manned missions, these systems offer a safer way to operate in outer space. One of the main goals of telepresence research is to learn ...</div></span>
          <span id="toHide59" style="display:none;"><br /><div style="display:inline"><p>Telepresence-systems have the potential to take on an important role in on-orbit servicing scenarios. In comparison to manned missions, these systems offer a safer way to operate in outer space. One of the main goals of telepresence research is to learn whether immersive telepresence systems can achieve the efficiency of astronauts in typical mounting tasks, considering that astronauts' mobility is restricted by a range of factors including microgravity and space suits. In order to determine whether a telepresence system is more efficient in performing tasks compared to suited astronauts, an experimental study comparing both scenarios was accomplished.</p></div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734516&CFID=85077225&CFTOKEN=85986120">Specialization, fan-out, and multi-human/multi-robot supervisory control</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456640340&CFID=85077225&CFTOKEN=85986120">Jonathan M. Whetten</a>, 
                        <a href="author_page.cfm?id=81350575550&CFID=85077225&CFTOKEN=85986120">Michael A. Goodrich</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 147-148</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734516&ftid=764264&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow60" style="display:inline;"><br /><div style="display:inline">This paper explores supervisory control of multiple, heterogeneous, independent robots by operator teams. Experimental evidence is presented which suggests that two cooperating operators may have free capacity that can be used to improve primary task ...</div></span>
          <span id="toHide60" style="display:none;"><br /><div style="display:inline"><p>This paper explores supervisory control of multiple, heterogeneous, independent robots by operator teams. Experimental evidence is presented which suggests that two cooperating operators may have free capacity that can be used to improve primary task performance without increasing average fan-out.</p></div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734517&CFID=85077225&CFTOKEN=85986120">Practical evaluation of robots for elderly in denmark: an overview</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414609902&CFID=85077225&CFTOKEN=85986120">Soren Tranberg Hansen</a>, 
                        <a href="author_page.cfm?id=81100570681&CFID=85077225&CFTOKEN=85986120">Hans Jorgen Andersen</a>, 
                        <a href="author_page.cfm?id=81414602620&CFID=85077225&CFTOKEN=85986120">Thomas Bak</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 149-150</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734517&ftid=764265&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow61" style="display:inline;"><br /><div style="display:inline">Robots for elderly have drawn a great deal of attention as it is a controversial topic being pushed forward by the fact that there will be a dramatic increase of elderly in most western countries. Within the field of HRI, much research has been conducted ...</div></span>
          <span id="toHide61" style="display:none;"><br /><div style="display:inline"><p>Robots for elderly have drawn a great deal of attention as it is a controversial topic being pushed forward by the fact that there will be a dramatic increase of elderly in most western countries. Within the field of HRI, much research has been conducted on robots interacting with elderly and also a number of commercial products have been introduced to the market. Since 2006, a number of projects have been launched in Denmark in order to evaluate robot technology in practice in elder care. This paper gives an brief overview of a selected number of projects and outlines the characteristics and results. Finally it is discussed how HRI can benefit from these.</p></div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734518&CFID=85077225&CFTOKEN=85986120">Human performance moderator functions for human-robot peer-based teams</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456634287&CFID=85077225&CFTOKEN=85986120">Caroline E. Harriott</a>, 
                        <a href="author_page.cfm?id=81100313646&CFID=85077225&CFTOKEN=85986120">Julie A. Adams</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 151-152</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734518&ftid=764266&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow62" style="display:inline;"><br /><div style="display:inline">Interaction between humans and robots in peer-based teams can be dramatically affected by human performance. Our research is focused on determining if existing human performance moderator functions apply to peer-based human-robot interaction and if not, ...</div></span>
          <span id="toHide62" style="display:none;"><br /><div style="display:inline"><p>Interaction between humans and robots in peer-based teams can be dramatically affected by human performance. Our research is focused on determining if existing human performance moderator functions apply to peer-based human-robot interaction and if not, how such functions must be modified. Our initial work focuses on modeling workload. Validation of the models will require human subject evaluations. Future work will incorporate larger numbers of performance moderator functions and will apply the results to distributing tasks to team members.</p></div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734519&CFID=85077225&CFTOKEN=85986120">Photograph-based interaction for teaching object delivery tasks to robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81363603235&CFID=85077225&CFTOKEN=85986120">Sunao Hashimoto</a>, 
                        <a href="author_page.cfm?id=81456640368&CFID=85077225&CFTOKEN=85986120">Andrei Ostanin</a>, 
                        <a href="author_page.cfm?id=81100424140&CFID=85077225&CFTOKEN=85986120">Masahiko Inami</a>, 
                        <a href="author_page.cfm?id=81100444444&CFID=85077225&CFTOKEN=85986120">Takeo Igarashi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 153-154</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734519&ftid=764267&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow63" style="display:inline;"><br /><div style="display:inline">Personal photographs are important media for communication in our daily lives. People take photos to remember things about themselves and show them to others to share the experience. We expect that a photograph can be useful tool for teaching a task ...</div></span>
          <span id="toHide63" style="display:none;"><br /><div style="display:inline"><p>Personal photographs are important media for communication in our daily lives. People take photos to remember things about themselves and show them to others to share the experience. We expect that a photograph can be useful tool for teaching a task to a robot. We propose a novel human-robot interaction using photographs. The user takes a photo to remember the target in a real-world situation involving a task and shows it to the system to make it physically execute the task. We developed a prototype system in which the user took a photo of a dish arrangement on a table and showed it to the system later to then have a small robot deliver and arrange the dishes in the same way.</p></div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734520&CFID=85077225&CFTOKEN=85986120">Human-robot collaboration for a shared mission</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453605652&CFID=85077225&CFTOKEN=85986120">Abir-Beatrice Karami</a>, 
                        <a href="author_page.cfm?id=81100602354&CFID=85077225&CFTOKEN=85986120">Laurent Jeanpierre</a>, 
                        <a href="author_page.cfm?id=81100517889&CFID=85077225&CFTOKEN=85986120">Abdel-Illah Mouaddib</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 155-156</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734520&ftid=764268&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow64" style="display:inline;"><br /><div style="display:inline">We are interested in collaboration domains between a robot and a human partner, the partners share a common mission without an explicit communication about their plans. The decision process of the robot agent should consider the presence of its human ...</div></span>
          <span id="toHide64" style="display:none;"><br /><div style="display:inline"><p>We are interested in collaboration domains between a robot and a human partner, the partners share a common mission without an explicit communication about their plans. The decision process of the robot agent should consider the presence of its human partner. Also, the robot planning should be flexible to human comfortability and all possible changes in the shared environment. To solve the problem of human-robot collaboration with no communication, we present a model that gives the robot the ability to build a belief over human intentions in order to predict his goals, this model counts mainly on observing the human actions. We integrate this prediction into a Partially Observable Markov Decision Process (POMDP) model to achieve the most appropriate and flexible decisions for the robot.</p></div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734521&CFID=85077225&CFTOKEN=85986120">Humanoid robot vs. projector robot: exploring an indirect approach to human robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453622474&CFID=85077225&CFTOKEN=85986120">Eun Kwon</a>, 
                        <a href="author_page.cfm?id=81452598848&CFID=85077225&CFTOKEN=85986120">Gerard Jounghyun Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 157-158</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734521&ftid=764269&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=1734521&ftid=820090&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow65" style="display:inline;"><br /><div style="display:inline">In this paper, we compare the efficiency in information transfer and user acceptance between the traditional humanoid robot and the projector robot.</div></span>
          <span id="toHide65" style="display:none;"><br /><div style="display:inline"><p>In this paper, we compare the efficiency in information transfer and user acceptance between the traditional humanoid robot and the projector robot.</p></div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734522&CFID=85077225&CFTOKEN=85986120">Dona: urban donation motivating robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456612485&CFID=85077225&CFTOKEN=85986120">Min Su Kim</a>, 
                        <a href="author_page.cfm?id=81456628114&CFID=85077225&CFTOKEN=85986120">Byung Keun Cha</a>, 
                        <a href="author_page.cfm?id=81456606335&CFID=85077225&CFTOKEN=85986120">Dong Min Park</a>, 
                        <a href="author_page.cfm?id=81456609046&CFID=85077225&CFTOKEN=85986120">Sae Mee Lee</a>, 
                        <a href="author_page.cfm?id=81339511173&CFID=85077225&CFTOKEN=85986120">Sonya Kwak</a>, 
                        <a href="author_page.cfm?id=81414592368&CFID=85077225&CFTOKEN=85986120">Min Kyung Lee</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 159-160</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734522&ftid=764270&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734522&ftid=820091&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow66" style="display:inline;"><br /><div style="display:inline">The rate of donations made by individuals is relatively low in Korea when compared to other developed countries. To address this problem, we propose the DONA, an urban donation motivating robot prototype. The robot roams around in a public space and ...</div></span>
          <span id="toHide66" style="display:none;"><br /><div style="display:inline"><p>The rate of donations made by individuals is relatively low in Korea when compared to other developed countries. To address this problem, we propose the DONA, an urban donation motivating robot prototype. The robot roams around in a public space and solicits donation from passers-by by engaging them through a pet like interaction. In this paper, we present the prototype of the robot and our design process.</p></div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734523&CFID=85077225&CFTOKEN=85986120">Design targeting voice interface robot capable of active listening</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456630778&CFID=85077225&CFTOKEN=85986120">Yuka Kobayashi</a>, 
                        <a href="author_page.cfm?id=81456613223&CFID=85077225&CFTOKEN=85986120">Daisuke Yamamoto</a>, 
                        <a href="author_page.cfm?id=81456607454&CFID=85077225&CFTOKEN=85986120">Toshiyuki Koga</a>, 
                        <a href="author_page.cfm?id=81456610104&CFID=85077225&CFTOKEN=85986120">Sachie Yokoyama</a>, 
                        <a href="author_page.cfm?id=81100266546&CFID=85077225&CFTOKEN=85986120">Miwako Doi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 161-162</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734523&ftid=764271&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow67" style="display:inline;"><br /><div style="display:inline">The EU, South Korea and Japan have a pressing need to compensate for growing labor shortages in their aging societies. There is growing awareness that robotic technology has the potential to ameliorate this problem in terms of both physical and mental ...</div></span>
          <span id="toHide67" style="display:none;"><br /><div style="display:inline"><p>The EU, South Korea and Japan have a pressing need to compensate for growing labor shortages in their aging societies. There is growing awareness that robotic technology has the potential to ameliorate this problem in terms of both physical and mental labor. To take an example of mental labor, a human therapist dealing with elderly people must be an active listener. In order to realize a robot capable of active listening, we adopt Ivey's basic listening sequence skills in microcounseling. In this paper, we describe a voice interface robot that realizes simple feedback, repeat feedback and questions for Ivey's basic listening sequence. We conducted an experiment, whose results show that 69% of incidences of feedback have adequate reflective words for spoken sentences and 56% of questions are adequate for these reflective words.</p></div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734524&CFID=85077225&CFTOKEN=85986120">Users' reactions toward an on-screen agent appearing on different media</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81328489326&CFID=85077225&CFTOKEN=85986120">Takanori Komatsu</a>, 
                        <a href="author_page.cfm?id=81456618654&CFID=85077225&CFTOKEN=85986120">Yuuki Seki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 163-164</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734524&ftid=764272&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow68" style="display:inline;"><br /><div style="display:inline">We experimentally investigated users' reactions toward an on-screen agent appearing on three different types of media: a 42-inch television, 17-inch display, and 4.5-inch mobile PC. Specifically, we observed whether the users accepted the agent's invitation ...</div></span>
          <span id="toHide68" style="display:none;"><br /><div style="display:inline"><p>We experimentally investigated users' reactions toward an on-screen agent appearing on three different types of media: a 42-inch television, 17-inch display, and 4.5-inch mobile PC. Specifically, we observed whether the users accepted the agent's invitation to a Shiritori game while they were engaged in given tasks. The results showed that most participants who received the invitation from the on-screen agent appearing on a 4.5-inch mobile PC accepted the agent's invitation, while most participants did not accept the invitation from the agent appearing on the other two formats. Therefore, the mobile PC appears to be an appropriate media for an on-screen agent that is required for interaction with users.</p></div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734525&CFID=85077225&CFTOKEN=85986120">5w viewpoints associative topic search for networked conversation support system</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81335493462&CFID=85077225&CFTOKEN=85986120">Yukitaka Kusumura</a>, 
                        <a href="author_page.cfm?id=81418593855&CFID=85077225&CFTOKEN=85986120">Hironori Mizuguchi</a>, 
                        <a href="author_page.cfm?id=81418593666&CFID=85077225&CFTOKEN=85986120">Dai Kusui</a>, 
                        <a href="author_page.cfm?id=81456637703&CFID=85077225&CFTOKEN=85986120">Yoshio Ishizawa</a>, 
                        <a href="author_page.cfm?id=81456623633&CFID=85077225&CFTOKEN=85986120">Yusuke Muraoka</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 165-166</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734525&ftid=764273&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow69" style="display:inline;"><br /><div style="display:inline">To build up spontaneous conversation, it is important to select topics without a feeling of strangeness. When someone notices others are not interested in a topic, he/she tries to find a new topic. Then, he/she thinks of viewpoints of the conversation ...</div></span>
          <span id="toHide69" style="display:none;"><br /><div style="display:inline"><p>To build up spontaneous conversation, it is important to select topics without a feeling of strangeness. When someone notices others are not interested in a topic, he/she tries to find a new topic. Then, he/she thinks of viewpoints of the conversation and selects a topic associated with the current topic from the viewpoints. To automate viewpoint-based topic selection, we present 5W viewpoint associative topic search. The method estimates the weights of 5W viewpoints (who, what, where, when and why) from conversation, to use an appropriate similarity to search for the next topic.</p></div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734526&CFID=85077225&CFTOKEN=85986120">Dialogue patterns of an arabic robot receptionist</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100431442&CFID=85077225&CFTOKEN=85986120">Maxim Makatchev</a>, 
                        <a href="author_page.cfm?id=81456636105&CFID=85077225&CFTOKEN=85986120">Imran Fanaswala</a>, 
                        <a href="author_page.cfm?id=81456640130&CFID=85077225&CFTOKEN=85986120">Ameer Abdulsalam</a>, 
                        <a href="author_page.cfm?id=81100556983&CFID=85077225&CFTOKEN=85986120">Brett Browning</a>, 
                        <a href="author_page.cfm?id=81456636278&CFID=85077225&CFTOKEN=85986120">Wael Ghazzawi</a>, 
                        <a href="author_page.cfm?id=81100350931&CFID=85077225&CFTOKEN=85986120">Majd Sakr</a>, 
                        <a href="author_page.cfm?id=81332527865&CFID=85077225&CFTOKEN=85986120">Reid Simmons</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 167-168</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734526&ftid=764274&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow70" style="display:inline;"><br /><div style="display:inline">Hala is a bilingual (Arabic and English) culturally-sensitive robot receptionist located at Carnegie Mellon University in Qatar. We report results from Hala's deployment by comparing her English dialogue corpus to that of a similar monolingual robot ...</div></span>
          <span id="toHide70" style="display:none;"><br /><div style="display:inline"><p>Hala is a bilingual (Arabic and English) culturally-sensitive robot receptionist located at Carnegie Mellon University in Qatar. We report results from Hala's deployment by comparing her English dialogue corpus to that of a similar monolingual robot (named "Tank") located at CMU's Pittsburgh campus. Specifically, we compare the average number of turns per interaction, duration of interactions, frequency of interactions with personal questions, rate of non-understandings, and rate of thanks after the robot's answer. We provide possible explanations for observed similarities and differences and highlight potential cultural implications on the interactions.</p></div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734527&CFID=85077225&CFTOKEN=85986120">Modular control for human motion analysis and classification in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456637006&CFID=85077225&CFTOKEN=85986120">Juan Alberto Rivera-Bautista</a>, 
                        <a href="author_page.cfm?id=81456636093&CFID=85077225&CFTOKEN=85986120">Ana Cristina Ramirez-Hernandez</a>, 
                        <a href="author_page.cfm?id=81456614160&CFID=85077225&CFTOKEN=85986120">Virginia A. Garcia-Vega</a>, 
                        <a href="author_page.cfm?id=81100357384&CFID=85077225&CFTOKEN=85986120">Antonio Marin-Hernandez</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 169-170</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734527&ftid=764275&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1734527&ftid=820092&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow71" style="display:inline;"><br /><div style="display:inline">Trajectories followed by the humans can be interpreted as an attitude gesture. Based on this interpretation an autonomous mobile robot can decide how to initiate interaction with a given human. In this work is presented a modular control system to analyze ...</div></span>
          <span id="toHide71" style="display:none;"><br /><div style="display:inline"><p>Trajectories followed by the humans can be interpreted as an attitude gesture. Based on this interpretation an autonomous mobile robot can decide how to initiate interaction with a given human. In this work is presented a modular control system to analyze human walking trajectories in order to engage a robot on a Human-Robot interaction. When the robot detects a human with their vision system a visual tracking module begins to work over the Pan/Tilt/Zoom (PTZ) camera unit. Camera parameters configuration and global robot localization are then used by another module to filter and track human's legs over the laser range finder (LRF) data. Path followed by the human over the global reference frame is then processed by another module which determines the kind of attitude showed by the human. Based on the result the robot decides if an interaction is needed and who is expected to begin it. At this moment are used only three kinds of attitudes: confidence, curiosity and nervousness.</p></div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734528&CFID=85077225&CFTOKEN=85986120">A panoramic vision system for human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81375605665&CFID=85077225&CFTOKEN=85986120">Ester Mart&#237;nez</a>, 
                        <a href="author_page.cfm?id=81100147388&CFID=85077225&CFTOKEN=85986120">Angel P. del Pobil</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 171-172</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734528&ftid=764276&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow72" style="display:inline;"><br /><div style="display:inline">We have proposed a new approach for solving a fundamental issue in HRI, that is, how to properly detect and identify people in everyday environments since some conditions might make it a difficult task. For that, fisheye cameras are used since they provide ...</div></span>
          <span id="toHide72" style="display:none;"><br /><div style="display:inline"><p>We have proposed a new approach for solving a fundamental issue in HRI, that is, how to properly detect and identify people in everyday environments since some conditions might make it a difficult task. For that, fisheye cameras are used since they provide panoramic vision and one or two of them allow to cover the whole workspace. A modified background maintenance approach was developed for fast, robust motion detection; while person identification for interaction is dealt with <i>Viola-Jones classifier</i>, although, instead of searching in the whole image, its input is only composed of the detected moving elements. Moreover, in order to avoid restricting the system autonomy by requiring the person has to face the system any time, once a person is identified as a target for interaction, they are tracked by using another designed method. We have also carried out an implementation of the proposed approach and a comparative experiment to assess its feasibility.</p></div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734529&CFID=85077225&CFTOKEN=85986120">Multimodal human-humanoid interaction using motions, brain nirs and spike trains</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100019225&CFID=85077225&CFTOKEN=85986120">Yasuo Matsuyama</a>, 
                        <a href="author_page.cfm?id=81453625833&CFID=85077225&CFTOKEN=85986120">Nimiko Ochiai</a>, 
                        <a href="author_page.cfm?id=81453648267&CFID=85077225&CFTOKEN=85986120">Takashi Hatakeyama</a>, 
                        <a href="author_page.cfm?id=81456615958&CFID=85077225&CFTOKEN=85986120">Keita Noguchi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 173-174</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734529&ftid=764277&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsAvi" title="Other Formats Avi" href="ft_gateway.cfm?id=1734529&ftid=820093&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/vid.gif" alt="Avi" class="fulltext_lnk" border="0" />Avi</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow73" style="display:inline;"><br /><div style="display:inline">Heterogeneous bio-signals including human motions, brain NIRS and neural spike trains are utilized for operating biped humanoids. The Bayesian network comprising Hidden Markov Models and Support Vector Machines is designed for the signal integration. ...</div></span>
          <span id="toHide73" style="display:none;"><br /><div style="display:inline"><p>Heterogeneous bio-signals including human motions, brain NIRS and neural spike trains are utilized for operating biped humanoids. The Bayesian network comprising Hidden Markov Models and Support Vector Machines is designed for the signal integration. By this method, the system complexity is reduced so that that total operation is within the scope of PCs. The designed system is capable of transducing original sensory meaning to another. This leads to prosthesis, rehabilitation and gaming. In addition to the supervised mode, the humanoid can act autonomously for its own designed tasks.</p></div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734530&CFID=85077225&CFTOKEN=85986120">Changes of utterances in the skill acquisition of collaborative conveyer task</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456605910&CFID=85077225&CFTOKEN=85986120">Shuichi Nakata</a>, 
                        <a href="author_page.cfm?id=81456620592&CFID=85077225&CFTOKEN=85986120">Harumi Kobayashi</a>, 
                        <a href="author_page.cfm?id=81100183876&CFID=85077225&CFTOKEN=85986120">Satoshi Suzuki</a>, 
                        <a href="author_page.cfm?id=81456618672&CFID=85077225&CFTOKEN=85986120">Hiroshi Igarashi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 175-176</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734530&ftid=764278&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow74" style="display:inline;"><br /><div style="display:inline">Importance of developing human adaptive robots or systems is increasing these days. To accomplish it, we have to clarify features of human-human communication. In this study, we analyzed human speech while completing computerized collaborate task to ...</div></span>
          <span id="toHide74" style="display:none;"><br /><div style="display:inline"><p>Importance of developing human adaptive robots or systems is increasing these days. To accomplish it, we have to clarify features of human-human communication. In this study, we analyzed human speech while completing computerized collaborate task to clarify how humans speak in collaborative work. We extracted questioning speeches from conversations using CLAN, and classified them into four categories according to their content. We investigated whether the number and ratio of each type of questions changed over ten trials. The results indicate that leader role person is sensitive to task planning</p></div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734531&CFID=85077225&CFTOKEN=85986120">Intuitive multimodal interaction for service robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456637493&CFID=85077225&CFTOKEN=85986120">Matthias Nieuwenhuisen</a>, 
                        <a href="author_page.cfm?id=81456635196&CFID=85077225&CFTOKEN=85986120">J&#246;rg St&#252;ckler</a>, 
                        <a href="author_page.cfm?id=81100455319&CFID=85077225&CFTOKEN=85986120">Sven Behnke</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 177-178</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734531&ftid=764279&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow75" style="display:inline;"><br /><div style="display:inline">Domestic service tasks require three main skills from autonomous robots: robust navigation in indoor environments, flexible object manipulation, and intuitive communication with the users. In this report, we present the communication skills of our anthropomorphic ...</div></span>
          <span id="toHide75" style="display:none;"><br /><div style="display:inline"><p>Domestic service tasks require three main skills from autonomous robots: robust navigation in indoor environments, flexible object manipulation, and intuitive communication with the users. In this report, we present the communication skills of our anthropomorphic service and communication robots Dynamaid and Robotinho. Both robots are equipped with an intuitive multimodal communication system, including speech synthesis and recognition, gestures and mimic. We evaluate our systems in the @Home league of the RoboCup competitions and in a museum tour guide scenario.</p></div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734532&CFID=85077225&CFTOKEN=85986120">Toward the body image horizon: how do users recognize the body of a robot?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414597701&CFID=85077225&CFTOKEN=85986120">Hirotaka Osawa</a>, 
                        <a href="author_page.cfm?id=81456607338&CFID=85077225&CFTOKEN=85986120">Yuji Matsuda</a>, 
                        <a href="author_page.cfm?id=81314484611&CFID=85077225&CFTOKEN=85986120">Ren Ohmura</a>, 
                        <a href="author_page.cfm?id=81100282909&CFID=85077225&CFTOKEN=85986120">Michita Imai</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 179-180</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734532&ftid=764280&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow76" style="display:inline;"><br /><div style="display:inline">In this study, we investigated the boundary for recognizing robots. Many anthropomorphic robots are used for interactions with users. These robots show various body forms and appearances, which are recognized by their users. This ability to recognize ...</div></span>
          <span id="toHide76" style="display:none;"><br /><div style="display:inline"><p>In this study, we investigated the boundary for recognizing robots. Many anthropomorphic robots are used for interactions with users. These robots show various body forms and appearances, which are recognized by their users. This ability to recognize a variety of robotic appearances suggests that a user can recognize a wide range of imaginary body forms compared with the native human appearance. We attempted to determine the boundary for the recognition of robot appearances. On the basis of our previous studies, we hypothesized that the discrimination of robot appearances depends of the order of the parts. If the body parts of a robot are placed in order from top to bottom, the user can recognize the assembly as a robot body. We performed a human-robot experiment in which we compared the results for robots with ordered parts with those for robots with inverted parts. The result showed that the users' perception of the robot's body differed between the two groups. This result confirms our hypothesized boundary for the recognition of robot appearances.</p></div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734533&CFID=85077225&CFTOKEN=85986120">Solving ambiguities with perspective taking</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310500089&CFID=85077225&CFTOKEN=85986120">Raquel Ros</a>, 
                        <a href="author_page.cfm?id=81309507877&CFID=85077225&CFTOKEN=85986120">Emrah Akin Sisbot</a>, 
                        <a href="author_page.cfm?id=81100454878&CFID=85077225&CFTOKEN=85986120">Rachid Alami</a>, 
                        <a href="author_page.cfm?id=81456634649&CFID=85077225&CFTOKEN=85986120">Jasmin Steinwender</a>, 
                        <a href="author_page.cfm?id=81456620906&CFID=85077225&CFTOKEN=85986120">Katharina Hamann</a>, 
                        <a href="author_page.cfm?id=81456637829&CFID=85077225&CFTOKEN=85986120">Felix Warneken</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 181-182</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734533&ftid=764281&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow77" style="display:inline;"><br /><div style="display:inline">Humans constantly generate and solve ambiguities while interacting with each other in their every day activities. Hence, having a robot that is able to solve ambiguous situations is essential if we aim at achieving a fluent and acceptable human-robot ...</div></span>
          <span id="toHide77" style="display:none;"><br /><div style="display:inline"><p>Humans constantly generate and solve ambiguities while interacting with each other in their every day activities. Hence, having a robot that is able to solve ambiguous situations is essential if we aim at achieving a fluent and acceptable human-robot interaction. We propose a strategy that combines three mechanisms to clarify ambiguous situations generated by the human partner. We implemented our approach and successfully performed validation tests in several different situations both, in simulation and with the HRP-2 robot.</p></div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734534&CFID=85077225&CFTOKEN=85986120">Validating interaction patterns in HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81408592441&CFID=85077225&CFTOKEN=85986120">Peter H. Kahn, Jr.</a>, 
                        <a href="author_page.cfm?id=81100631042&CFID=85077225&CFTOKEN=85986120">Brian T. Gill</a>, 
                        <a href="author_page.cfm?id=81456638149&CFID=85077225&CFTOKEN=85986120">Aimee L. Reichert</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=85077225&CFTOKEN=85986120">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=85077225&CFTOKEN=85986120">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81350569693&CFID=85077225&CFTOKEN=85986120">Jolina H. Ruckert</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 183-184</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734534&ftid=764282&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow78" style="display:inline;"><br /><div style="display:inline">In recent work, "interaction patterns" have been proposed as a means to characterize essential features of human-robot interaction. A problem arises, however, in knowing whether the interaction patterns generated are valid. The same problem arises when ...</div></span>
          <span id="toHide78" style="display:none;"><br /><div style="display:inline"><p>In recent work, "interaction patterns" have been proposed as a means to characterize essential features of human-robot interaction. A problem arises, however, in knowing whether the interaction patterns generated are valid. The same problem arises when researchers in HRI propose other broad conceptualizations that seek to structure social interaction. In this paper, we address this general problem by distinguishing three ways of establishing the validity of interaction patterns. The first form of validity seeks to establish whether the conclusions about interaction patterns are warranted from the data. The second seeks to establish whether the interaction patterns account for the data. And the third seeks to provide sound reasons for the labels of the patterns themselves. Often these three forms of validity are confused in discussions about conceptual categories in HRI.</p></div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734535&CFID=85077225&CFTOKEN=85986120">A study of three interfaces allowing non-expert users to teach new visual objects to a robot and their impact on learning efficiency</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456637844&CFID=85077225&CFTOKEN=85986120">Pierre Rouanet</a>, 
                        <a href="author_page.cfm?id=81100662126&CFID=85077225&CFTOKEN=85986120">Pierre-Yves Oudeyer</a>, 
                        <a href="author_page.cfm?id=81100465124&CFID=85077225&CFTOKEN=85986120">David Filliat</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 185-186</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734535&ftid=764283&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow79" style="display:inline;"><br /><div style="display:inline">We developed three interfaces to allow non-expert users to teach name for new visual objects and compare them through user's studies in term of learning efficiency.</div></span>
          <span id="toHide79" style="display:none;"><br /><div style="display:inline"><p>We developed three interfaces to allow non-expert users to teach name for new visual objects and compare them through user's studies in term of learning efficiency.</p></div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734536&CFID=85077225&CFTOKEN=85986120">Help me help you: interfaces for personal robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456636411&CFID=85077225&CFTOKEN=85986120">Ian J. Goodfellow</a>, 
                        <a href="author_page.cfm?id=81456623442&CFID=85077225&CFTOKEN=85986120">Nate Koenig</a>, 
                        <a href="author_page.cfm?id=81456634939&CFID=85077225&CFTOKEN=85986120">Marius Muja</a>, 
                        <a href="author_page.cfm?id=81331501616&CFID=85077225&CFTOKEN=85986120">Caroline Pantofaru</a>, 
                        <a href="author_page.cfm?id=81387604394&CFID=85077225&CFTOKEN=85986120">Alexander Sorokin</a>, 
                        <a href="author_page.cfm?id=81100499212&CFID=85077225&CFTOKEN=85986120">Leila Takayama</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 187-188</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734536&ftid=764284&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734537&CFID=85077225&CFTOKEN=85986120">The hesitation of a robot: a delay in its motion increases learning efficiency and impresses humans as teachable</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100134372&CFID=85077225&CFTOKEN=85986120">Kazuaki Tanaka</a>, 
                        <a href="author_page.cfm?id=81337492395&CFID=85077225&CFTOKEN=85986120">Motoyuki Ozeki</a>, 
                        <a href="author_page.cfm?id=81100082224&CFID=85077225&CFTOKEN=85986120">Natsuki Oka</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 189-190</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734537&ftid=764285&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow81" style="display:inline;"><br /><div style="display:inline">If robots learn new actions through human-robot interaction, it is important that the robots can utilize rewards as well as instructions to reduce humans' efforts. Additioanlly, "interval" which allows humans to give instructions and evaluations is also ...</div></span>
          <span id="toHide81" style="display:none;"><br /><div style="display:inline"><p>If robots learn new actions through human-robot interaction, it is important that the robots can utilize rewards as well as instructions to reduce humans' efforts. Additioanlly, "interval" which allows humans to give instructions and evaluations is also important. We hence focused on "delays in initiating actions" and changed them according to the progress of learning: long delays at early stages, and short at later stages. We compared the proposed varying delay with a constant delay by an experiment. The result demonstrated that the varying delay improves learning efficiency significantly and impresses humans as teachable.</p></div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734538&CFID=85077225&CFTOKEN=85986120">Can a robot deceive humans?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100125670&CFID=85077225&CFTOKEN=85986120">Kazunori Terada</a>, 
                        <a href="author_page.cfm?id=81100600115&CFID=85077225&CFTOKEN=85986120">Akira Ito</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 191-192</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734538&ftid=764286&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow82" style="display:inline;"><br /><div style="display:inline">In the present study, we investigated whether a robot is able to deceive a human by producing a behavior against him/her prediction. A feeling of being deceived by a robot would be a strong indicator that the human treat the robot as an intentional entity. ...</div></span>
          <span id="toHide82" style="display:none;"><br /><div style="display:inline"><p>In the present study, we investigated whether a robot is able to deceive a human by producing a behavior against him/her prediction. A feeling of being deceived by a robot would be a strong indicator that the human treat the robot as an intentional entity. We conducted a psychological experiment in which a subject played Darumasan ga Koronda, a Japanese children's game, with a robot. A main strategy to deceive a subject was to make his/her mind believe that the robot is stupid so as not to be able to move quickly. The experimental result indicated that unexpected change of a robot behavior gave rise to an impression of being deceived by the robot.</p></div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734539&CFID=85077225&CFTOKEN=85986120">Developing heuristics for assistive robotics</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414601137&CFID=85077225&CFTOKEN=85986120">Katherine M. Tsui</a>, 
                        <a href="author_page.cfm?id=81456635831&CFID=85077225&CFTOKEN=85986120">Kareem Abu-Zahra</a>, 
                        <a href="author_page.cfm?id=81456636430&CFID=85077225&CFTOKEN=85986120">Renato Casipe</a>, 
                        <a href="author_page.cfm?id=81456637174&CFID=85077225&CFTOKEN=85986120">Jason M'Sadoques</a>, 
                        <a href="author_page.cfm?id=81100059315&CFID=85077225&CFTOKEN=85986120">Jill L. Drury</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 193-194</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734539&ftid=764287&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          
          </td>
          </tr>
		  
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734540&CFID=85077225&CFTOKEN=85986120">Effect of social robot's behavior in collaborative learning</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100391918&CFID=85077225&CFTOKEN=85986120">Hirohide Ushida</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 195-196</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734540&ftid=764288&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow84" style="display:inline;"><br /><div style="display:inline">This paper describes about the effect of social robot's behavior on human performance. The robot behaves based on an artificial mind model, and it expresses emotions according to the situation. In this research, we consider about the case where human ...</div></span>
          <span id="toHide84" style="display:none;"><br /><div style="display:inline"><p>This paper describes about the effect of social robot's behavior on human performance. The robot behaves based on an artificial mind model, and it expresses emotions according to the situation. In this research, we consider about the case where human and the robot learn cooperatively. The robot emotionally reacts to the joint learner's success and failure. The experimental result shows that social behavior of the robot influences the performance of human learners.</p></div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734541&CFID=85077225&CFTOKEN=85986120">STB: human-dependent sociable trash box</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456607684&CFID=85077225&CFTOKEN=85986120">Yuto Yamaji</a>, 
                        <a href="author_page.cfm?id=81456623192&CFID=85077225&CFTOKEN=85986120">Taisuke Miyake</a>, 
                        <a href="author_page.cfm?id=81456636141&CFID=85077225&CFTOKEN=85986120">Yuta Yoshiike</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=85077225&CFTOKEN=85986120">P. Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=85077225&CFTOKEN=85986120">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 197-198</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734541&ftid=764289&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow85" style="display:inline;"><br /><div style="display:inline">We developed a Sociable Trash Box (STB) as a children-assisted robot able to collect the trash in order to convey its intentional stance to children. The STB is capable of engaging manifold affiliation behaviors to build a social rapport with children ...</div></span>
          <span id="toHide85" style="display:none;"><br /><div style="display:inline"><p>We developed a Sociable Trash Box (STB) as a children-assisted robot able to collect the trash in order to convey its intentional stance to children. The STB is capable of engaging manifold affiliation behaviors to build a social rapport with children by collecting the trash around their environment. In particular, the STB is a child-dependent robot that walks alone in a public space for tracing humans and trash for the purpose of collecting the trash. The robot is incapable of collecting the trash by itself, and it engages by using interactive behaviors and vocalizations to make a social coupling with children based on the robot's anticipation to accomplish its goal. The present experiment investigates how STB behaviors are effective in conveying intentions to evoke children's social interactions and to assist in collecting the trash in their environment.</p></div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734542&CFID=85077225&CFTOKEN=85986120">Relationships between user experiences and children's perceptions of the education robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81453627084&CFID=85077225&CFTOKEN=85986120">Eunja Hyun</a>, 
                        <a href="author_page.cfm?id=81482655714&CFID=85077225&CFTOKEN=85986120">Hyunmin Yoon</a>, 
                        <a href="author_page.cfm?id=81456605560&CFID=85077225&CFTOKEN=85986120">Sooryun Son</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 199-200</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734542&ftid=764290&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow86" style="display:inline;"><br /><div style="display:inline">The purpose of this study is to investigate the biological, mental, social, moral, and educational perceptions of young children of the intelligent robot iRobiQ and to explore the effects of user experience on them. The interview was conducted with 111 ...</div></span>
          <span id="toHide86" style="display:none;"><br /><div style="display:inline"><p>The purpose of this study is to investigate the biological, mental, social, moral, and educational perceptions of young children of the intelligent robot iRobiQ and to explore the effects of user experience on them. The interview was conducted with 111 five-year-old children attending two kindergartens and two childcare centers in which iRobiQ had been purchased and had been in use since March 2009. The young children interacted with the robot for one hour or less everyday over a period of two weeks or less. The robot contents were related to the socio-emotional perceptions of robots and had a high level of human-robot interactions, such as "Talking with the Robot" or "Attendance Check." Children who experienced the "voice" and "touch screen" functions of the robot showed higher educational perception. The social and educational perception was higher when the robot was placed in a classroom than when it was placed in the hallway or in the office. The results indicated that robot content focusing on socio-emotional characteristics should be developed for educational purposes and that a robot should be placed in the classroom for individual use</p></div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734455&CFID=85077225&CFTOKEN=85986120">Dance partner robot: an engineering approach to human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100578236&CFID=85077225&CFTOKEN=85986120">Kazuhiro Kosuge</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 201-201</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734455&ftid=764636&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow87" style="display:inline;"><br /><div style="display:inline">A Dance Partner Robot, PBDR (Partner Ball Room Dance Robot), dances a waltz as a female dancer together with a human male dancer. The waltz, a ball room dance, is usually performed by a male dancer and a female dancer, and consists of a certain number ...</div></span>
          <span id="toHide87" style="display:none;"><br /><div style="display:inline"><p>A Dance Partner Robot, PBDR (Partner Ball Room Dance Robot), dances a waltz as a female dancer together with a human male dancer. The waltz, a ball room dance, is usually performed by a male dancer and a female dancer, and consists of a certain number of steps, and transition of the steps. The dance is lead by the male dancer based on the transition rule of the dance. The female dance partner estimates the following step through physical interactions with the male dancer. The dance partner robot has a database about the waltz and its transition rule for estimating the following dance step and generating an appropriate step motion. The step estimation is done based on the time-series data of the force/torque applied by the male dancer to the robot upper body. The robot motion is generated for the estimated step using the step motion in the database compliantly against the interface force/moment between the human dancer and the robot in real time. The development of the dance partner robot has suggested us a lot of important issues for robots having interaction with a human. Why we are developing the dance partner robot and how the concept will be applied to other robot systems will be discussed in the presentation.</p></div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 3: social &#38; moral interaction with robots</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Selma Sabanovic 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734544&CFID=85077225&CFTOKEN=85986120">Gracefully mitigating breakdowns in robotic services</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414592368&CFID=85077225&CFTOKEN=85986120">Min Kyung Lee</a>, 
                        <a href="author_page.cfm?id=81456637528&CFID=85077225&CFTOKEN=85986120">Sara Kielser</a>, 
                        <a href="author_page.cfm?id=81100492013&CFID=85077225&CFTOKEN=85986120">Jodi Forlizzi</a>, 
                        <a href="author_page.cfm?id=81453611472&CFID=85077225&CFTOKEN=85986120">Siddhartha Srinivasa</a>, 
                        <a href="author_page.cfm?id=81100077697&CFID=85077225&CFTOKEN=85986120">Paul Rybski</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 203-210</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734544&ftid=764291&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow89" style="display:inline;"><br /><div style="display:inline">Robots that operate in the real world will make mistakes. Thus, those who design and build systems will need to understand how best to provide ways for robots to mitigate those mistakes. Building on diverse research literatures, we consider how to mitigate ...</div></span>
          <span id="toHide89" style="display:none;"><br /><div style="display:inline"><p>Robots that operate in the real world will make mistakes. Thus, those who design and build systems will need to understand how best to provide ways for robots to mitigate those mistakes. Building on diverse research literatures, we consider how to mitigate breakdowns in services provided by robots. Expectancy-setting strategies forewarn people of a robot's limitations so people will expect mistakes. Recovery strategies, including apologies, compensation, and options for the user, aim to reduce the negative consequence of breakdowns. We tested these strategies in an online scenario study with 317 participants. A breakdown in robotic service had severe impact on evaluations of the service and the robot, but forewarning and recovery strategies reduced the negative impact of the breakdown. People's orientation toward services influenced which recovery strategy worked best. Those with a relational orientation responded best to an apology; those with a utilitarian orientation responded best to compensation. We discuss robotic service design to mitigate service problems.</p></div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734545&CFID=85077225&CFTOKEN=85986120">Critic, compatriot, or chump?: responses to robot blame attribution</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81416605221&CFID=85077225&CFTOKEN=85986120">Victoria Groom</a>, 
                        <a href="author_page.cfm?id=81456625560&CFID=85077225&CFTOKEN=85986120">Jimmy Chen</a>, 
                        <a href="author_page.cfm?id=81456608632&CFID=85077225&CFTOKEN=85986120">Theresa Johnson</a>, 
                        <a href="author_page.cfm?id=81456632506&CFID=85077225&CFTOKEN=85986120">F. Arda Kara</a>, 
                        <a href="author_page.cfm?id=81100153283&CFID=85077225&CFTOKEN=85986120">Clifford Nass</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 211-218</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734545&ftid=764292&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow90" style="display:inline;"><br /><div style="display:inline">As their abilities improve, robots will be placed in roles of greater responsibility and specialization. In these contexts, robots may attribute blame to humans in order to identify problems and help humans make sense of complex information. In a between-participants ...</div></span>
          <span id="toHide90" style="display:none;"><br /><div style="display:inline"><p>As their abilities improve, robots will be placed in roles of greater responsibility and specialization. In these contexts, robots may attribute blame to humans in order to identify problems and help humans make sense of complex information. In a between-participants experiment with a single factor (blame target) and three levels (human blame vs. team blame vs. self blame) participants interacted with a robot in a learning context, teaching it their personal preferences. The robot performed poorly, then attributed blame to either the human, the team, or itself. Participants demonstrated a powerful and consistent negative response to the human-blaming robot. Participants preferred the self-blaming robot over both the human and team blame robots. Implications for theory and design are discussed.</p></div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734546&CFID=85077225&CFTOKEN=85986120">No fair!!: an interaction with a cheating robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456611923&CFID=85077225&CFTOKEN=85986120">Elaine Short</a>, 
                        <a href="author_page.cfm?id=81418596943&CFID=85077225&CFTOKEN=85986120">Justin Hart</a>, 
                        <a href="author_page.cfm?id=81456626259&CFID=85077225&CFTOKEN=85986120">Michelle Vu</a>, 
                        <a href="author_page.cfm?id=81326492306&CFID=85077225&CFTOKEN=85986120">Brian Scassellati</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 219-226</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734546&ftid=764293&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow91" style="display:inline;"><br /><div style="display:inline">Using a humanoid robot and a simple children's game, we examine the degree to which variations in behavior result in attributions of mental state and intentionality. Participants play the well-known children's game "rock-paper-scissors" against a robot ...</div></span>
          <span id="toHide91" style="display:none;"><br /><div style="display:inline"><p>Using a humanoid robot and a simple children's game, we examine the degree to which variations in behavior result in attributions of mental state and intentionality. Participants play the well-known children's game "rock-paper-scissors" against a robot that either plays fairly, or that cheats in one of two ways. In the "verbal cheat" condition, the robot announces the wrong outcome on several rounds which it loses, declaring itself the winner. In the "action cheat"' condition, the robot changes its gesture after seeing its opponent's play. We find that participants display a greater level of social engagement and make greater attributions of mental state when playing against the robot in the conditions in which it cheats.</p></div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 4: teleoperation</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Jennifer Burke 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734548&CFID=85077225&CFTOKEN=85986120">UAV video coverage quality maps and prioritized indexing for wilderness search and rescue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100405205&CFID=85077225&CFTOKEN=85986120">Bryan S. Morse</a>, 
                        <a href="author_page.cfm?id=81456635978&CFID=85077225&CFTOKEN=85986120">Cameron H. Engh</a>, 
                        <a href="author_page.cfm?id=81350575550&CFID=85077225&CFTOKEN=85986120">Michael A. Goodrich</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 227-234</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734548&ftid=764294&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734548&ftid=820094&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow93" style="display:inline;"><br /><div style="display:inline">Video-equipped mini unmanned aerial vehicles (mini-UAVs) are becoming increasingly popular for surveillance, remote sensing, law enforcement, and search and rescue operations, all of which rely on thorough coverage of a target observation area. However, ...</div></span>
          <span id="toHide93" style="display:none;"><br /><div style="display:inline"><p>Video-equipped mini unmanned aerial vehicles (mini-UAVs) are becoming increasingly popular for surveillance, remote sensing, law enforcement, and search and rescue operations, all of which rely on thorough coverage of a target observation area. However, coverage is not simply a matter of seeing the area (visibility) but of seeing it well enough to allow detection of targets of interest, a quality we here call "see-ability". Video flashlights, mosaics, or other geospatial compositions of the video may help place the video in context and convey that an area was observed, but not necessarily how well or how often. This paper presents a method for using UAV-acquired video georegistered to terrain and aerial reference imagery to create geospatial video coverage quality maps and indices that indicate relative video quality based on detection factors such as image resolution, number of observations, and variety of viewing angles. When used for offline post-analysis of the video, or for online review, these maps also enable geospatial quality-filtered or prioritized non-sequential access to the video. We present examples of static and dynamic see-ability coverage maps in wilderness search-and-rescue scenarios, along with examples of prioritized non-sequential video access. We also present the results of a user study demonstrating the correlation between see-ability computation and human detection performance.</p></div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734549&CFID=85077225&CFTOKEN=85986120">Single operator, multiple robots: an eye movement based theoretic model of operator situation awareness</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350583314&CFID=85077225&CFTOKEN=85986120">Raj M. Ratwani</a>, 
                        <a href="author_page.cfm?id=81350583559&CFID=85077225&CFTOKEN=85986120">J. Malcolm McCurry</a>, 
                        <a href="author_page.cfm?id=81100061774&CFID=85077225&CFTOKEN=85986120">J. Gregory Trafton</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 235-242</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734549&ftid=764295&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow94" style="display:inline;"><br /><div style="display:inline">For a single operator to effectively control multiple robots, operator situation awareness is a critical component of the human-robot system. There are three levels of situation awareness: perception, comprehension, and projection into the future [1]. ...</div></span>
          <span id="toHide94" style="display:none;"><br /><div style="display:inline"><p>For a single operator to effectively control multiple robots, operator situation awareness is a critical component of the human-robot system. There are three levels of situation awareness: perception, comprehension, and projection into the future [1]. We focus on the perception level to develop a theoretic model of the perceptual-cognitive processes underlying situation awareness. Eye movement measures were developed as indicators of cognitive processing and these measures were used to account for operator situation awareness on a supervisory control task. The eye movement based model emphasizes the importance of visual scanning and attention allocation as the cognitive processes that lead to operator situation awareness and the model lays the groundwork for real-time prediction of operator situation awareness.</p></div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734550&CFID=85077225&CFTOKEN=85986120">Multimodal interaction with an autonomous forklift</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81416595488&CFID=85077225&CFTOKEN=85986120">Andrew Correa</a>, 
                        <a href="author_page.cfm?id=81430665011&CFID=85077225&CFTOKEN=85986120">Matthew R. Walter</a>, 
                        <a href="author_page.cfm?id=81100337671&CFID=85077225&CFTOKEN=85986120">Luke Fletcher</a>, 
                        <a href="author_page.cfm?id=81100136957&CFID=85077225&CFTOKEN=85986120">Jim Glass</a>, 
                        <a href="author_page.cfm?id=81100244355&CFID=85077225&CFTOKEN=85986120">Seth Teller</a>, 
                        <a href="author_page.cfm?id=81100602780&CFID=85077225&CFTOKEN=85986120">Randall Davis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 243-250</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734550&ftid=764296&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow95" style="display:inline;"><br /><div style="display:inline">We describe a multimodal framework for interacting with an autonomous robotic forklift. A key element enabling effective interaction is a wireless, handheld tablet with which a human supervisor can command the forklift using speech and sketch. Most current ...</div></span>
          <span id="toHide95" style="display:none;"><br /><div style="display:inline"><p>We describe a multimodal framework for interacting with an autonomous robotic forklift. A key element enabling effective interaction is a wireless, handheld tablet with which a human supervisor can command the forklift using speech and sketch. Most current sketch interfaces treat the canvas as a blank slate. In contrast, our interface uses live and synthesized camera images from the forklift as a canvas, and augments them with object and obstacle information from the world. This connection enables users to "draw on the world," enabling a simpler set of sketched gestures. Our interface supports commands that include summoning the forklift and directing it to lift, transport, and place loads of palletized cargo. We describe an exploratory evaluation of the system designed to identify areas for detailed study.</p> <p>Our framework incorporates external signaling to interact with humans near the vehicle. The robot uses audible and visual annunciation to convey its current state and intended actions. The system also provides seamless autonomy handoff: any human can take control of the robot by entering its cabin, at which point the forklift can be operated manually until the human exits.</p></div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 5: natural language interaction</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Tony Belpaeme 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734552&CFID=85077225&CFTOKEN=85986120">Following directions using statistical machine translation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81443596877&CFID=85077225&CFTOKEN=85986120">Cynthia Matuszek</a>, 
                        <a href="author_page.cfm?id=81100498749&CFID=85077225&CFTOKEN=85986120">Dieter Fox</a>, 
                        <a href="author_page.cfm?id=81332509529&CFID=85077225&CFTOKEN=85986120">Karl Koscher</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 251-258</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734552&ftid=764297&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow97" style="display:inline;"><br /><div style="display:inline">Mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language. In this work we investigate how statistical machine translation techniques can be used to bridge the gap ...</div></span>
          <span id="toHide97" style="display:none;"><br /><div style="display:inline"><p>Mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language. In this work we investigate how statistical machine translation techniques can be used to bridge the gap between natural language route instructions and a map of an environment built by a robot. Our approach uses training data to learn to translate from natural language instructions to an automatically-labeled map. The complexity of the translation process is controlled by taking advantage of physical constraints imposed by the map. As a result, our technique can efficiently handle uncertainty in both map labeling and parsing. Our experiments demonstrate the promising capabilities achieved by our approach.</p></div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734553&CFID=85077225&CFTOKEN=85986120">Toward understanding natural language directions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100575483&CFID=85077225&CFTOKEN=85986120">Thomas Kollar</a>, 
                        <a href="author_page.cfm?id=81100089105&CFID=85077225&CFTOKEN=85986120">Stefanie Tellex</a>, 
                        <a href="author_page.cfm?id=81100653347&CFID=85077225&CFTOKEN=85986120">Deb Roy</a>, 
                        <a href="author_page.cfm?id=81339525411&CFID=85077225&CFTOKEN=85986120">Nicholas Roy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 259-266</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734553&ftid=764298&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow98" style="display:inline;"><br /><div style="display:inline">Speaking using unconstrained natural language is an intuitive and flexible way for humans to interact with robots. Understanding this kind of linguistic input is challenging because diverse words and phrases must be mapped into structures that the robot ...</div></span>
          <span id="toHide98" style="display:none;"><br /><div style="display:inline"><p>Speaking using unconstrained natural language is an intuitive and flexible way for humans to interact with robots. Understanding this kind of linguistic input is challenging because diverse words and phrases must be mapped into structures that the robot can understand, and elements in those structures must be grounded in an uncertain environment. We present a system that follows natural language directions by extracting a sequence of <i>spatial description clauses</i> from the linguistic input and then infers the most probable path through the environment given only information about the environmental geometry and detected visible objects. We use a probabilistic graphical model that factors into three key components. The first component grounds landmark phrases such as "the computers" in the perceptual frame of the robot by exploiting co-occurrence statistics from a database of tagged images such as Flickr. Second, a spatial reasoning component judges how well spatial relations such as "past the computers" describe a path. Finally, verb phrases such as "turn right" are modeled according to the amount of change in orientation in the path. Our system follows 60% of the directions in our corpus to within 15 meters of the true destination, significantly outperforming other approaches.</p></div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734554&CFID=85077225&CFTOKEN=85986120">Robot-directed speech: using language to assess first-time users' conceptualizations of a robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81336490641&CFID=85077225&CFTOKEN=85986120">Sarah Kriz</a>, 
                        <a href="author_page.cfm?id=81414612166&CFID=85077225&CFTOKEN=85986120">Gregory Anderson</a>, 
                        <a href="author_page.cfm?id=81100061774&CFID=85077225&CFTOKEN=85986120">J. Gregory Trafton</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 267-274</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734554&ftid=764299&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow99" style="display:inline;"><br /><div style="display:inline">It is expected that in the near-future people will have daily natural language interactions with robots. However, we know very little about how users feel they should talk to robots, especially users who have never before interacted with a robot. The ...</div></span>
          <span id="toHide99" style="display:none;"><br /><div style="display:inline"><p>It is expected that in the near-future people will have daily natural language interactions with robots. However, we know very little about how users feel they should talk to robots, especially users who have never before interacted with a robot. The present study evaluated first-time users' expectations about a robot's cognitive and communicative capabilities by comparing robot-directed speech to the way in which participants talked to a human partner. The results indicate that participants spoke more loudly, raised their pitch, and hyperarticulated their messages when they spoke to the robot, suggesting that they viewed the robot as having low linguistic competence. However, utterances show that speakers often assumed that the robot had humanlike cognitive capabilities. The results suggest that while first-time users were concerned with the fragility of the robot's speech recognition system, they believed that the robot had extremely strong information processing capabilities.</p></div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734555&CFID=85077225&CFTOKEN=85986120">Robust spoken instruction understanding for HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456624161&CFID=85077225&CFTOKEN=85986120">Rehj Cantrell</a>, 
                        <a href="author_page.cfm?id=81100029514&CFID=85077225&CFTOKEN=85986120">Matthias Scheutz</a>, 
                        <a href="author_page.cfm?id=81100573922&CFID=85077225&CFTOKEN=85986120">Paul Schermerhorn</a>, 
                        <a href="author_page.cfm?id=81456631679&CFID=85077225&CFTOKEN=85986120">Xuan Wu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 275-282</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734555&ftid=764300&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow100" style="display:inline;"><br /><div style="display:inline">Natural human-robot interaction requires different and more robust models of language understanding (NLU) than non-embodied NLU systems. In particular, architectures are required that (1) process language incrementally in order to be able to provide ...</div></span>
          <span id="toHide100" style="display:none;"><br /><div style="display:inline"><p>Natural human-robot interaction requires different and more robust models of language understanding (NLU) than non-embodied NLU systems. In particular, architectures are required that (1) process language incrementally in order to be able to provide early backchannel feedback to human speakers; (2) use pragmatic contexts throughout the understanding process to infer missing information; and (3) handle the underspecified, fragmentary, or otherwise ungrammatical utterances that are common in spontaneous speech. In this paper, we describe our attempts at developing an integrated natural language understanding architecture for HRI, and demonstrate its novel capabilities using challenging data collected in human-human interaction experiments.</p></div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734456&CFID=85077225&CFTOKEN=85986120">Action understanding and gesture acquisition in the great apes</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100376759&CFID=85077225&CFTOKEN=85986120">Josep Call</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 283-283</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734456&ftid=764637&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow101" style="display:inline;"><br /><div style="display:inline">A growing number of scholars have suggested that gestural communication may have been especially important in the early stages of language origins. Of special interest in this debate is the communication of other primates, especially those most closely ...</div></span>
          <span id="toHide101" style="display:none;"><br /><div style="display:inline"><p>A growing number of scholars have suggested that gestural communication may have been especially important in the early stages of language origins. Of special interest in this debate is the communication of other primates, especially those most closely related to humans, the great apes. The aim of this talk is to explore the interrelations between instrumental actions, action understanding and gesture generation in humans and other apes. In doing so, I will contrast the similarities and differences in the use and comprehension of gestures in humans and apes. Like humans, apes use gestures flexibly and they can even learn new gestures. Unlike humans, however, imitative learning does not seem to be the main mechanism underlying gesture acquisition in great apes. Instead apes seem to learn many of their gestures in social interaction with others via processes of ontogenetic ritualization by means of which instrumental actions are transformed into gestures. Like humans, apes can extract information about the goals contained in the actions of others but there is much less evidence that they also grasp some of the representational properties of certain kinds of gestures and the communicative intentions behind them.</p></div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 6: nonverbal interaction</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Sara Kiesler 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734557&CFID=85077225&CFTOKEN=85986120">Reconfiguring spatial formation arrangement by robot body orientation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100072975&CFID=85077225&CFTOKEN=85986120">Hideaki Kuzuoka</a>, 
                        <a href="author_page.cfm?id=81384598558&CFID=85077225&CFTOKEN=85986120">Yuya Suzuki</a>, 
                        <a href="author_page.cfm?id=81100617644&CFID=85077225&CFTOKEN=85986120">Jun Yamashita</a>, 
                        <a href="author_page.cfm?id=81350572857&CFID=85077225&CFTOKEN=85986120">Keiichi Yamazaki</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 285-292</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734557&ftid=764301&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow103" style="display:inline;"><br /><div style="display:inline">An information-presenting robot is expected to establish an appropriate spatial relationship with people. Drawing upon sociological studies of spatial relationships involving "F-formation" and "body torque," we examined the effect of a robot rotating ...</div></span>
          <span id="toHide103" style="display:none;"><br /><div style="display:inline"><p>An information-presenting robot is expected to establish an appropriate spatial relationship with people. Drawing upon sociological studies of spatial relationships involving "F-formation" and "body torque," we examined the effect of a robot rotating its body on the reconfiguration of the F-formation arrangement. The results showed that a robot can change the position of a visitor by rotating its body. We also confirmed that to reconfigure the F-formation arrangement, it is more effective to rotate the whole body of the robot than only its head.</p></div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734558&CFID=85077225&CFTOKEN=85986120">Head motions during dialogue speech and nod timing control in humanoid robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350582479&CFID=85077225&CFTOKEN=85986120">Carlos T. Ishi</a>, 
                        <a href="author_page.cfm?id=81456617871&CFID=85077225&CFTOKEN=85986120">ChaoRan Liu</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=85077225&CFTOKEN=85986120">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=85077225&CFTOKEN=85986120">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 293-300</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734558&ftid=764302&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMpeg" title="Other Formats Mpeg" href="ft_gateway.cfm?id=1734558&ftid=820095&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/mpeg.gif" alt="Mpeg" class="fulltext_lnk" border="0" />Mpeg</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow104" style="display:inline;"><br /><div style="display:inline">Head motion naturally occurs in synchrony with speech and may carry paralinguistic information, such as intention, attitude and emotion, in dialogue communication. With the aim of verifying the relationship between head motion and the dialogue acts carried ...</div></span>
          <span id="toHide104" style="display:none;"><br /><div style="display:inline"><p>Head motion naturally occurs in synchrony with speech and may carry paralinguistic information, such as intention, attitude and emotion, in dialogue communication. With the aim of verifying the relationship between head motion and the dialogue acts carried by speech, analyses were conducted on motion-captured data for several speakers during natural dialogues. The analysis results first confirmed the trends of our previous work, showing that regardless of the speaker, nods frequently occur during speech utterances, not only for expressing dialogue acts such as agreement and affirmation, but also appearing at the last syllable of the phrase, in strong phrase boundaries, especially when the speaker is talking confidently, or expressing interest in the interlocutor's talk. Inter-speaker variability indicated that the frequency of head motion may vary according to the speaker's age or status, while intra-speaker variability indicated that the frequency of head motion also differs depending on the inter-personal relationship with the interlocutor. A simple model for generating nods based on rules inferred from the analysis results was proposed and evaluated in two types of humanoid robots. Subjective scores showed that the proposed model could generate head motions with naturalness comparable to the original motions.</p></div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734559&CFID=85077225&CFTOKEN=85986120">Pointing to space: modeling of deictic interaction referring to regions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456638478&CFID=85077225&CFTOKEN=85986120">Yasuhiko Hato</a>, 
                        <a href="author_page.cfm?id=81321498073&CFID=85077225&CFTOKEN=85986120">Satoru Satake</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=85077225&CFTOKEN=85986120">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100282909&CFID=85077225&CFTOKEN=85986120">Michita Imai</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=85077225&CFTOKEN=85986120">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 301-308</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734559&ftid=764303&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow105" style="display:inline;"><br /><div style="display:inline">In daily conversation, we sometimes observe a deictic interaction scene that refers to a region in a space, such as saying "please put it over there" with pointing. How can such an interaction be possible with a robot? Is it enough to simulate ...</div></span>
          <span id="toHide105" style="display:none;"><br /><div style="display:inline"><p>In daily conversation, we sometimes observe a deictic interaction scene that refers to a region in a space, such as saying "please put it <i>over there</i>" with pointing. How can such an interaction be possible with a robot? Is it enough to simulate people's behaviors, such as utterance and pointing? Instead, we highlight the importance of simulating human cognition. In the first part of our study, we empirically demonstrate the importance of simulating human cognition of regions when a robot engages in a deictic interaction by referring to a region in a space. The experiments indicate that a robot with simulated cognition of regions improves efficiency of its deictic interaction. In the second part, we present a method for a robot to computationally simulate cognition of regions.</p></div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 7: social learning</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Yukie Nagai 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734561&CFID=85077225&CFTOKEN=85986120">Investigating multimodal real-time patterns of joint attention in an hri word learning task</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100474961&CFID=85077225&CFTOKEN=85986120">Chen Yu</a>, 
                        <a href="author_page.cfm?id=81100029514&CFID=85077225&CFTOKEN=85986120">Matthias Scheutz</a>, 
                        <a href="author_page.cfm?id=81100573922&CFID=85077225&CFTOKEN=85986120">Paul Schermerhorn</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 309-316</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734561&ftid=764304&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow107" style="display:inline;"><br /><div style="display:inline">Joint attention - the idea that humans make inferences from observable behaviors of other humans by attending to the objects and events that these others humans attend to - has been recognized as a critical component in human-robot interactions. While ...</div></span>
          <span id="toHide107" style="display:none;"><br /><div style="display:inline"><p>Joint attention - the idea that humans make inferences from observable behaviors of other humans by attending to the objects and events that these others humans attend to - has been recognized as a critical component in human-robot interactions. While various HRI studies showed that having robots to behave in ways that support human recognition of joint attention leads to better behavioral outcomes on the human side, there are no studies that investigate the detailed time course of <i>interactive joint attention processes.</i></p> <p>In this paper, we present the results from an HRI study that investigates the exact time course of human multi-modal attentional processes during an HRI word learning task in an unprecedented way. Using novel data analysis techniques, we are able to demonstrate that the temporal details of human attentional behavior are critical for understanding human expectations of joint attention in HRI and that failing to do so can force humans into assuming unnatural behaviors.</p></div></span> <a id="expcoll107" href="JavaScript: expandcollapse('expcoll107',107)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734562&CFID=85077225&CFTOKEN=85986120">Transparent active learning for robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456613213&CFID=85077225&CFTOKEN=85986120">Crystal Chao</a>, 
                        <a href="author_page.cfm?id=81414594397&CFID=85077225&CFTOKEN=85986120">Maya Cakmak</a>, 
                        <a href="author_page.cfm?id=81310502411&CFID=85077225&CFTOKEN=85986120">Andrea L. Thomaz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 317-324</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734562&ftid=764305&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow108" style="display:inline;"><br /><div style="display:inline">This research aims to enable robots to learn from human teachers. Motivated by human social learning, we believe that a transparent learning process can help guide the human teacher to provide the most informative instruction. We believe active learning ...</div></span>
          <span id="toHide108" style="display:none;"><br /><div style="display:inline"><p>This research aims to enable robots to learn from human teachers. Motivated by human social learning, we believe that a transparent learning process can help guide the human teacher to provide the most informative instruction. We believe active learning is an inherently transparent machine learning approach because the learner formulates queries to the oracle that reveal information about areas of uncertainty in the underlying model. In this work, we implement active learning on the Simon robot in the form of nonverbal gestures that query a human teacher about a demonstration within the context of a social dialogue. Our preliminary pilot study data show potential for transparency through active learning to improve the accuracy and efficiency of the teaching process. However, our data also seem to indicate possible undesirable effects from the human teacher's perspective regarding balance of the interaction. These preliminary results argue for control strategies that balance leading and following during a social learning interaction.</p></div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734563&CFID=85077225&CFTOKEN=85986120">From manipulation to communicative gesture</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100177036&CFID=85077225&CFTOKEN=85986120">Shichao Ou</a>, 
                        <a href="author_page.cfm?id=81100500609&CFID=85077225&CFTOKEN=85986120">Roderic Grupen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 325-332</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734563&ftid=764306&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow109" style="display:inline;"><br /><div style="display:inline">This paper advocates an approach for learning communicative actions and manual skills in the same framework. We exploit a fundamental relationship between the structure of motor skills, intention, and communication. Communicative actions are acquired ...</div></span>
          <span id="toHide109" style="display:none;"><br /><div style="display:inline"><p>This paper advocates an approach for learning communicative actions and manual skills in the same framework. We exploit a fundamental relationship between the structure of motor skills, intention, and communication. Communicative actions are acquired using the same learning framework and the same primitive states and actions that the robot uses to construct manual behavior for interacting with other objects in the environment. A <i>prospective behavior</i> algorithm is used to acquire modular policies for conveying intention and goals to nearby human beings and recruiting their assistance. The learning framework and a preliminary case study are presented in which a humanoid robot learns expressive communicative behavior incrementally by discovering the manual affordances of human beings. Results from interactions with 16 people provide support for the hypothesized benefits of this approach. Behavior reuse makes learning from relatively few interactions possible. This approach compliments other efforts in the field by grounding social behavior, and proposes a mechanism for negotiating a communicative vocabulary between humans and robots.</p></div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Video session</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:0">
          Jacob Crandall, Aaron Steinfeld 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:0">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734564&ftid=764307&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734565&CFID=85077225&CFTOKEN=85986120">A trial english class with a teaching assistant robot in elementary school</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414594930&CFID=85077225&CFTOKEN=85986120">Jeonghye Han</a>, 
                        <a href="author_page.cfm?id=81456609037&CFID=85077225&CFTOKEN=85986120">Seungmin Lee Lee</a>, 
                        <a href="author_page.cfm?id=81456621367&CFID=85077225&CFTOKEN=85986120">Bokhyun Kang</a>, 
                        <a href="author_page.cfm?id=81100161282&CFID=85077225&CFTOKEN=85986120">Sungju Park</a>, 
                        <a href="author_page.cfm?id=81456625032&CFID=85077225&CFTOKEN=85986120">Jungkwan Kim</a>, 
                        <a href="author_page.cfm?id=81456612481&CFID=85077225&CFTOKEN=85986120">Myungsook Kim</a>, 
                        <a href="author_page.cfm?id=81456612482&CFID=85077225&CFTOKEN=85986120">Mihee Kim</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 335-336</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734565&ftid=764308&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsWmv" title="Other Formats Wmv" href="ft_gateway.cfm?id=1734565&ftid=820096&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/wmv.gif" alt="Wmv" class="fulltext_lnk" border="0" />Wmv</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow111" style="display:inline;"><br /><div style="display:inline">Various studies propose that robots can be an effective tool for language teaching and learning. Especially they have been remarkably successful in elementary English classes [1][2][3][4]. The purpose of this study was to investigate some effects of ...</div></span>
          <span id="toHide111" style="display:none;"><br /><div style="display:inline"><p>Various studies propose that robots can be an effective tool for language teaching and learning. Especially they have been remarkably successful in elementary English classes [1][2][3][4]. The purpose of this study was to investigate some effects of a teaching assistant robot, Langbot, in elementary English classes in Korea. We adopted IROBIQ as Longbot for a pilot study.</p> <p>We designed some activities for elementary English classes using a teaching assistant robot, Langbot: introduction, look and listen, listen and say, look and say, act out, song and chant. The introduction includes the birth story of Langbot that children want to know where the robot comes from, how old it is, why it came to their classroom, etc, since Hur and Han (2009) found that the robot storytelling was working to increase children's tolerance toward the failure of recognition of a robot [2].</p></div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734566&CFID=85077225&CFTOKEN=85986120">Sociable trash box</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456636141&CFID=85077225&CFTOKEN=85986120">Yuta Yoshiike</a>, 
                        <a href="author_page.cfm?id=81456607684&CFID=85077225&CFTOKEN=85986120">Yuto Yamaji</a>, 
                        <a href="author_page.cfm?id=81456623192&CFID=85077225&CFTOKEN=85986120">Taisuke Miyake</a>, 
                        <a href="author_page.cfm?id=81456621996&CFID=85077225&CFTOKEN=85986120">P. Ravindra S. De Silva</a>, 
                        <a href="author_page.cfm?id=81456630139&CFID=85077225&CFTOKEN=85986120">Michio Okada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 337-338</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734566&ftid=764309&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1734566&ftid=820097&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow112" style="display:inline;"><br /><div style="display:inline">The STB is capable of engaging manifold affiliation behaviors to build a social rapport toward the goal of collecting trash around an environment. In particular, STB is a child-dependent robot that walks alone in a public space for the purpose of tracing ...</div></span>
          <span id="toHide112" style="display:none;"><br /><div style="display:inline"><p>The STB is capable of engaging manifold affiliation behaviors to build a social rapport toward the goal of collecting trash around an environment. In particular, STB is a child-dependent robot that walks alone in a public space for the purpose of tracing humans and trash and to collect the trash. In a crowded space, STBs move toward the trash by engaging with an attractive twisting motion (behaviors) and vocal interaction to convey STB's intention to children. Our STB robot is incapable of collecting the trash by itself. In this sense, children have to infer a robot's intentional stance or expectation for interaction with the STB. To collect trash while creating social rapport with children is a novel concept. The STB engages with twisting and bowing motions when children put trash into an STB container.</p></div></span> <a id="expcoll112" href="JavaScript: expandcollapse('expcoll112',112)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734567&CFID=85077225&CFTOKEN=85986120">Dona: urban donation motivating robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456612485&CFID=85077225&CFTOKEN=85986120">Min Su Kim</a>, 
                        <a href="author_page.cfm?id=81456628114&CFID=85077225&CFTOKEN=85986120">Byung Keun Cha</a>, 
                        <a href="author_page.cfm?id=81456606335&CFID=85077225&CFTOKEN=85986120">Dong Min Park</a>, 
                        <a href="author_page.cfm?id=81456609046&CFID=85077225&CFTOKEN=85986120">Sae Mee Lee</a>, 
                        <a href="author_page.cfm?id=81339511173&CFID=85077225&CFTOKEN=85986120">Sonya Kwak</a>, 
                        <a href="author_page.cfm?id=81414592368&CFID=85077225&CFTOKEN=85986120">Min Kyung Lee</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 339-340</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734567&ftid=764310&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734567&ftid=820098&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow113" style="display:inline;"><br /><div style="display:inline">The rate of donations made by individuals is relatively low in Korea when compared to other developed countries. To address this problem, we propose the DONA, an urban donation motivating robot prototype. The robot roams around in a public space and ...</div></span>
          <span id="toHide113" style="display:none;"><br /><div style="display:inline"><p>The rate of donations made by individuals is relatively low in Korea when compared to other developed countries. To address this problem, we propose the DONA, an urban donation motivating robot prototype. The robot roams around in a public space and solicits donation from passers-by by engaging them through a pet like interaction. In this paper, we present the prototype of the robot and our design process.</p></div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734568&CFID=85077225&CFTOKEN=85986120">FusionBot: a barista robot - fusionbot serving coffees to visitors during technology exhibition event</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81318498048&CFID=85077225&CFTOKEN=85986120">Dilip Kumar Limbu</a>, 
                        <a href="author_page.cfm?id=81442599975&CFID=85077225&CFTOKEN=85986120">Yeow Kee Tan</a>, 
                        <a href="author_page.cfm?id=81456618507&CFID=85077225&CFTOKEN=85986120">Lawrence T.C. Por</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 341-342</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734568&ftid=764311&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1734568&ftid=820099&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow114" style="display:inline;"><br /><div style="display:inline">This video shows a service robot named FusionBot autonomously serving coffees to visitors on their request, which occurred during two days-long experiment in TechFest 2008 event. The coffee serving task involves taking coffee order from a visitor, identifying ...</div></span>
          <span id="toHide114" style="display:none;"><br /><div style="display:inline"><p>This video shows a service robot named FusionBot autonomously serving coffees to visitors on their request, which occurred during two days-long experiment in TechFest 2008 event. The coffee serving task involves taking coffee order from a visitor, identifying a cup and smart coffee machine, moving towards the coffee machine, communicating with the coffee machine and fetching the coffee cup to the visitor.</p> <p>The main purpose of this experiment is to explore and demonstrate the utility of an interactive service robot in smart home environment, thereby improving the quality of human life. Before conducting the experiments, visitors were given general procedural instructions and simple introduction on how the FusionBot works. Visitors then performed experiment tasks, i.e., ordering a cup of coffee. Thereafter, the visitors were asked to fill out the satisfaction questionnaires to find out their reaction and perception on the FusionBot. Of just over 100 survey questionnaires handed out, sixty eight (68) valid responses (i.e. 68%) were received. Over all, with regards to the FusionBot task satisfaction, more than half of respondents were satisfied with what the FusionBot can do. Nearly one quarter of the respondents indicated that it was not easy to communicate with the FusionBot. This could be due to occurrence of various background noises, which were falsely picked up by the FusionBot as speech input from the visitor. Similarly, less than one quarter indicated that it was not easy to learn how to use the FusionBot. This could be due to the not knowing what to do with the FusionBot and not knowing what the FusionBot does.</p> <p>The experiment was successful in two main dimensions; 1) the robot demonstrated the ability to interact with visitors and perform challenging real-world task autonomously, and 2) It provided some evidence towards the feasibility of using autonomous service robot and smart coffee machine to serve drink in a reception/home or acting as a host in an organization. While preliminary, the experiment also suggests that while developing a service robot; 1) static appearance is very important, 2) requires robust speech recognition and vision understanding, and finally 3) requires comprehensive training on speech and vision with respective data.</p></div></span> <a id="expcoll114" href="JavaScript: expandcollapse('expcoll114',114)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734569&CFID=85077225&CFTOKEN=85986120">The step-on interface (SOI) on a mobile platform: basic functions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456634840&CFID=85077225&CFTOKEN=85986120">Takafumi Matsumaru</a>, 
                        <a href="author_page.cfm?id=81456619099&CFID=85077225&CFTOKEN=85986120">Yuichi Ito</a>, 
                        <a href="author_page.cfm?id=81456638446&CFID=85077225&CFTOKEN=85986120">Wataru Saitou</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 343-344</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734569&ftid=764312&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734569&ftid=820100&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow115" style="display:inline;"><br /><div style="display:inline">This video shows the basic functions of HFAMRO-2 equipped with the step-on interface (SOI). In the SOI the projected screen is used as a bilateral interface. It not only presents information from the equipment to the user but also delivers the instructions ...</div></span>
          <span id="toHide115" style="display:none;"><br /><div style="display:inline"><p>This video shows the basic functions of HFAMRO-2 equipped with the step-on interface (SOI). In the SOI the projected screen is used as a bilateral interface. It not only presents information from the equipment to the user but also delivers the instructions from the user to the equipment. HFAMRO is intended to represent the concept based on which robots interact with users. It assumes, for example, the ability to play 'tag' - in this case, playing tag with light, similar to 'shadow' tag. The HFAMRO-2 mobile robot, developed to study the SOI's application with mobility, has two sets of the SOI consisting of a projector and a range scanner on a mobile platform. The projector displays a direction screen on a travel surface and the two-dimensional range scanner detects and measures the user's stepping to specify the selected button.</p></div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734570&CFID=85077225&CFTOKEN=85986120">The step-on interface (SOI) on a mobile platform: rehabilitation of the physically challenged</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456634840&CFID=85077225&CFTOKEN=85986120">Takafumi Matsumaru</a>, 
                        <a href="author_page.cfm?id=81456619099&CFID=85077225&CFTOKEN=85986120">Yuichi Ito</a>, 
                        <a href="author_page.cfm?id=81456638446&CFID=85077225&CFTOKEN=85986120">Wataru Saitou</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 345-346</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734570&ftid=764626&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734570&ftid=820101&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow116" style="display:inline;"><br /><div style="display:inline">The rehabilitation of the physically challenged is one of the trial applications of the step-on interface (SOI) on a mobile platform as the friendly amusing mobile (FAM) function. This video shows the result of the preliminary trial.</div></span>
          <span id="toHide116" style="display:none;"><br /><div style="display:inline"><p>The rehabilitation of the physically challenged is one of the trial applications of the step-on interface (SOI) on a mobile platform as the friendly amusing mobile (FAM) function. This video shows the result of the preliminary trial.</p></div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734571&CFID=85077225&CFTOKEN=85986120">Robot rescue!: an HRI engineering outreach activity</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81443595373&CFID=85077225&CFTOKEN=85986120">Jonathan T. Morgan</a>, 
                        <a href="author_page.cfm?id=81336490641&CFID=85077225&CFTOKEN=85986120">Sarah Kriz</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 347-348</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734571&ftid=764627&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734571&ftid=820102&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow117" style="display:inline;"><br /><div style="display:inline">This video is an example of an engineering outreach activity that we designed to illustrate some of the core issues in HRI research. High school students attending the University of Washington 2009 Summer Math Academy were given a disaster scenario and ...</div></span>
          <span id="toHide117" style="display:none;"><br /><div style="display:inline"><p>This video is an example of an engineering outreach activity that we designed to illustrate some of the core issues in HRI research. High school students attending the University of Washington 2009 Summer Math Academy were given a disaster scenario and were asked think about how a robot could help a victim who was trapped by fallen rubble during an earthquake. Students were led through a series of thought questions that encouraged them to consider the type of information the robot would need to give to the victim, the victim's family, and the rescue team. They also considered how a victim might respond to a robot, and the behaviors the robot should display during the rescue. The students then created a script for the rescue scenario, writing not only their own lines, but the behavior and communication of the robot as well. All relevant consent forms were obtained from the participants prior to the outreach event.</p></div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734572&CFID=85077225&CFTOKEN=85986120">Mysterious machines</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456639280&CFID=85077225&CFTOKEN=85986120">Billy Schonenberg</a>, 
                        <a href="author_page.cfm?id=81100461702&CFID=85077225&CFTOKEN=85986120">Christoph Bartneck</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 349-350</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734572&ftid=764628&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1734572&ftid=820103&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow118" style="display:inline;"><br /><div style="display:inline">Alan Turing proposed a test for the intelligence of machines in 1950 [1]. Despite great efforts, no computer has passed this test so far. Each year, chat bots compete for the Loebner Prize, the first formal instantiation of a Turing Test. No contender ...</div></span>
          <span id="toHide118" style="display:none;"><br /><div style="display:inline"><p>Alan Turing proposed a test for the intelligence of machines in 1950 [1]. Despite great efforts, no computer has passed this test so far. Each year, chat bots compete for the Loebner Prize, the first formal instantiation of a Turing Test. No contender was able to fool the jury yet. Major problems of the chat bots are the lack of common knowledge and the logical consistency of a dialogue.</p> <p>We explore a new approach to chat bots by focusing on non-logical conversation topics: mysticism. The founding books of the major religions are widely acknowledged examples of mystical topics. We selected the New Testament, the Koran and Rigveda as the knowledge base for our conversational robots.</p> <p>The robots are able to autonomously talk to each other and to humans about their religious believe. Each robot represents a belief, but we do not reveal their convictions. This ambiguity forces observers to follow the actual conversations instead of quickly applying stereotypes.</p></div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734573&CFID=85077225&CFTOKEN=85986120">Olivia @ TechFest 09: receptionist robot impressed visitors with lively interactions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456618507&CFID=85077225&CFTOKEN=85986120">Lawrence T.C. Por</a>, 
                        <a href="author_page.cfm?id=81456625491&CFID=85077225&CFTOKEN=85986120">Adrian Tay</a>, 
                        <a href="author_page.cfm?id=81318498048&CFID=85077225&CFTOKEN=85986120">Dilip Kumar Limbu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 351-352</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734573&ftid=764629&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMp4" title="Other Formats Mp4" href="ft_gateway.cfm?id=1734573&ftid=820104&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/mp4.gif" alt="Mp4" class="fulltext_lnk" border="0" />Mp4</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow119" style="display:inline;"><br /><div style="display:inline">Olivia 2.0 is a Social Robot designed to interact and serve in office environment as a Robotic Receptionist. This is the forth model of Service Robot developed by A*STAR Robotics Team in Singapore. For a start, the occupation &#38; background story of ...</div></span>
          <span id="toHide119" style="display:none;"><br /><div style="display:inline"><p>Olivia 2.0 is a Social Robot designed to interact and serve in office environment as a Robotic Receptionist. This is the forth model of Service Robot developed by A*STAR Robotics Team in Singapore.</p> <p>For a start, the occupation &#38; background story of Olivia as a receptionist has set a common ground between human and robot for interaction around topics fitting to the job. The use of vision technology enabled Olivia to detect the presence of a visitor standing in front of her so that she will initiate a dialogue. Through speech recognition technology and careful designed dialogue management system, visitors are able to converse with Olivia to know more about the amenities in Fusionopolis building as well as to engage in small talk.</p> <p>Taking the persona of a 5 years old kid, with a cute face and child voice, coupled with nice decorations has set the stage for a fun interaction time, as Olivia proceeds to engage the visitor to play a simple game that showcased object recognition and tracking capability.</p> <p>Olivia is built with advanced Mechatronics design with 13 degree of freedom for head, body and hands motions. The advance motion control algorithm and imitation learning software trained her well to display humanlike hand gestures and upper body movements. We noticed the lively gestures coupled with expressive robotic voice are very crucial to draw attention from human for the continuous engagement with Social Robot.</p> <p>We knew that this would be a valuable field trial opportunity whereby many visitors were having first encountering with service robot. We took advantage to study social acceptance by taking video recording for every human-robot interactions, follow by inviting visitors to participate in on-site feedback gathering with questionnaires. Of more than 100 questionnaires completed, 62% gave an overall rating of good and above, several expressed that the response of the robot is slow and 75.5% found that the robot is able to recognize their speech without any difficulties. The top 3 robot features that people would like to have in the robot are: fast response; clear way of talking; delivering relevant information.</p> <p>At the end of this technology exhibition over two days, more than 100 visitors interacted with Olivia for information enquiry and playing games. They were greatly impressed by her capabilities and above all they had a lot of fun interacting with her.</p></div></span> <a id="expcoll119" href="JavaScript: expandcollapse('expcoll119',119)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734574&CFID=85077225&CFTOKEN=85986120">actDresses: interacting with robotic devices - fashion and comics</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81385597508&CFID=85077225&CFTOKEN=85986120">Rob Tieben</a>, 
                        <a href="author_page.cfm?id=81100544117&CFID=85077225&CFTOKEN=85986120">Ylva Fernaeus</a>, 
                        <a href="author_page.cfm?id=81309488890&CFID=85077225&CFTOKEN=85986120">Mattias Jacobsson</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 353-354</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734574&ftid=764630&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734574&ftid=820105&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow120" style="display:inline;"><br /><div style="display:inline">Robotic devices, such as the Roomba vacuum cleaner, are customised and personalised by their users, using for example signs, stickers and clothes. The actDresses project explores how these metaphors from fashion and comics can be used in novel interactions ...</div></span>
          <span id="toHide120" style="display:none;"><br /><div style="display:inline"><p>Robotic devices, such as the Roomba vacuum cleaner, are customised and personalised by their users, using for example signs, stickers and clothes.</p> <p>The actDresses project explores how these metaphors from fashion and comics can be used in novel interactions with robotic devices. This movie shows one explorative prototype, where clothes and accessories are used to program the Roomba's behaviour.</p> <p>The clothes influence personality characteristics of the Roomba; the accessories, iconic flags, determine movement characteristics. Combined, the clothes and flags allow the user to create different types of behaviour.</p></div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734575&CFID=85077225&CFTOKEN=85986120">Selecting and commanding individual robots in a vision-based multi-robot system</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81442618313&CFID=85077225&CFTOKEN=85986120">Alex Couture-Beil</a>, 
                        <a href="author_page.cfm?id=81100253690&CFID=85077225&CFTOKEN=85986120">Richard T. Vaughan</a>, 
                        <a href="author_page.cfm?id=81100500947&CFID=85077225&CFTOKEN=85986120">Greg Mori</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 355-356</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734575&ftid=764631&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
           
	          Other formats: 
            <a name="OtherFormatsMov" title="Other Formats Mov" href="ft_gateway.cfm?id=1734575&ftid=820106&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/qtlogo.gif" alt="Mov" class="fulltext_lnk" border="0" />Mov</a>
          </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow121" style="display:inline;"><br /><div style="display:inline">This video presents a computer vision based system for interaction between a single human and multiple robots. Face contact and motion-based gestures are used as two different non-verbal communication channels; a user first selects a particular robot ...</div></span>
          <span id="toHide121" style="display:none;"><br /><div style="display:inline"><p>This video presents a computer vision based system for interaction between a single human and multiple robots. Face contact and motion-based gestures are used as two different non-verbal communication channels; a user first selects a particular robot by simply looking at it, then assigns it a task by waving his or her hand.</p></div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734576&CFID=85077225&CFTOKEN=85986120">The articulated head pays attention</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456632470&CFID=85077225&CFTOKEN=85986120">Christian Kroos</a>, 
                        <a href="author_page.cfm?id=81456613376&CFID=85077225&CFTOKEN=85986120">Damith C. Herath</a>, 
                        <a href="author_page.cfm?id=81456620283&CFID=85077225&CFTOKEN=85986120"> Stelarc</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 357-358</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734576&ftid=764632&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow122" style="display:inline;"><br /><div style="display:inline">The Articulated Head (AH) is an artistic installation that consists of a LCD monitor mounted on an industrial robot arm (Fanuc LR Mate 200iC) displaying the head of a virtual human. It was conceived as the next step in the evolution of Embodied Conversational ...</div></span>
          <span id="toHide122" style="display:none;"><br /><div style="display:inline"><p>The Articulated Head (AH) is an artistic installation that consists of a LCD monitor mounted on an industrial robot arm (Fanuc LR Mate 200iC) displaying the head of a virtual human. It was conceived as the next step in the evolution of Embodied Conversational Agents (ECAs) transcending virtual reality into the physical space shared with the human interlocutor. Recently an attention module has been added as part of a behavioural control system for non-verbal interaction between robot/ECA and human.</p> <p>Unstructured incoming perceptual information (currently originating from a custom acoustic localisation algorithm and a commercial people tracking software) is narrowed down to the most salient aspects allowing the generation of a single motor response. The requirements of the current task determine what salient means at any point in time, that is, the rules and associated thresholds and weights of the attention system are modified by the requirements of the current task while the task itself is specified by the central control system depending on the overall state of the AH with respect to the ongoing interaction. The attention system determines a single attended event using a winner-takes-all strategy and relays it to the central control system. It also directly generates a motor goal and forwards it to the motor system.</p> <p>The video shows how the robot's attention system drives its behaviour, (1) when there is no stimulus over an extended period of time, (2) when a person moves within its visual field, and (3) when a sudden loud auditory event attracts attention during an ongoing visually-based interaction (auditory-visual attention conflict). The subtitles are direct mappings from numeric descriptions of the central control system's internal states to slightly more entertaining English sentences.</p></div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Paper session 8: evaluation of interaction</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734578&CFID=85077225&CFTOKEN=85986120">When in Rome: the role of culture &#38; context in adherence to robot recommendations</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414599200&CFID=85077225&CFTOKEN=85986120">Lin Wang</a>, 
                        <a href="author_page.cfm?id=81435602249&CFID=85077225&CFTOKEN=85986120">Pei-Luen Patrick Rau</a>, 
                        <a href="author_page.cfm?id=81100133286&CFID=85077225&CFTOKEN=85986120">Vanessa Evers</a>, 
                        <a href="author_page.cfm?id=81414594748&CFID=85077225&CFTOKEN=85986120">Benjamin Krisper Robinson</a>, 
                        <a href="author_page.cfm?id=81100572021&CFID=85077225&CFTOKEN=85986120">Pamela Hinds</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 359-366</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734578&ftid=764633&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow124" style="display:inline;"><br /><div style="display:inline">In this study, we sought to clarify the effects of users' cultural background and cultural context on human-robot team collaboration by investigating attitudes toward and the extent to which people changed their decisions based on the recommendations ...</div></span>
          <span id="toHide124" style="display:none;"><br /><div style="display:inline"><p>In this study, we sought to clarify the effects of users' cultural background and cultural context on human-robot team collaboration by investigating attitudes toward and the extent to which people changed their decisions based on the recommendations of a robot collaborator. We report the results of a 2&#215;2 experiment with nationality (Chinese vs. US) and communication style (implicit vs. explicit) as dimensions. The results confirm expectations that when robots behave in more culturally normative ways, subjects are more likely to heed their recommendations. Specifically, subjects with a Chinese vs. a US cultural background changed their decisions more when collaborating with robots that communicated implicitly vs. explicitly. We also found evidence that Chinese subjects were more negative in their attitude to robots and, as a result, relied less on the robot's advice. These findings suggest that cultural values affect responses to robots in collaborative situations and reinforce the importance of culturally sensitive design in HRI.</p></div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734579&CFID=85077225&CFTOKEN=85986120">Lead me by the hand: evaluation of a direct physical interface for nursing assistant robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81456608843&CFID=85077225&CFTOKEN=85986120">Tiffany L. Chen</a>, 
                        <a href="author_page.cfm?id=81100204143&CFID=85077225&CFTOKEN=85986120">Charles C. Kemp</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 367-374</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734579&ftid=764634&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow125" style="display:inline;"><br /><div style="display:inline">When a user is in close proximity to a robot, physical contact becomes a potentially valuable channel for communication. People often use direct physical contact to guide a person to a desired location (e.g., leading a child by the hand) or to adjust ...</div></span>
          <span id="toHide125" style="display:none;"><br /><div style="display:inline"><p>When a user is in close proximity to a robot, physical contact becomes a potentially valuable channel for communication. People often use direct physical contact to guide a person to a desired location (e.g., leading a child by the hand) or to adjust a person's posture for a task (e.g., a dance instructor working with a dancer). Within this paper, we present an implementation and evaluation of a direct physical interface for a human-scale anthropomorphic robot. We define a direct physical interface (DPI) to be an interface that enables a user to influence a robot's behavior by making contact with its body. Human-human interaction inspired our interface design, which enables a user to lead our robot by the hand and position its arms. We evaluated this interface in the context of assisting nurses with patient lifting, which we expect to be a high-impact application area. Our evaluation consisted of a controlled laboratory experiment with 18 nurses from the Atlanta area of Georgia, USA. We found that our DPI significantly outperformed a comparable wireless gamepad interface in both objective and subjective measures, including number of collisions, time to complete the tasks, workload (Raw Task Load Index), and overall preference. In contrast, we found no significant difference between the two interfaces with respect to the users' perceptions of personal safety.</p></div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1734580&CFID=85077225&CFTOKEN=85986120">Recognizing engagement in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100311645&CFID=85077225&CFTOKEN=85986120">Charles Rich</a>, 
                        <a href="author_page.cfm?id=81456633809&CFID=85077225&CFTOKEN=85986120">Brett Ponsleur</a>, 
                        <a href="author_page.cfm?id=81456610865&CFID=85077225&CFTOKEN=85986120">Aaron Holroyd</a>, 
                        <a href="author_page.cfm?id=81100223737&CFID=85077225&CFTOKEN=85986120">Candace L. Sidner</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 375-382</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1734580&ftid=764635&dwn=1&CFID=85077225&CFTOKEN=85986120" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow126" style="display:inline;"><br /><div style="display:inline">Based on a study of the engagement process between humans, we have developed and implemented an initial computational model for recognizing engagement between a human and a humanoid robot. Our model contains recognizers for four types of connection events ...</div></span>
          <span id="toHide126" style="display:none;"><br /><div style="display:inline"><p>Based on a study of the engagement process between humans, we have developed and implemented an initial computational model for recognizing engagement between a human and a humanoid robot. Our model contains recognizers for four types of connection events involving gesture and speech: directed gaze, mutual facial gaze, conversational adjacency pairs and backchannels. To facilitate integrating and experimenting with our model in a broad range of robot architectures, we have packaged it as a node in the open-source Robot Operating System (ROS) framework. We have conducted a preliminary validation of our computational model and implementation in a simple human-robot pointing game.</p></div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
          </div>
		  </td>
          </tr>          
          
</table>


</div> 
</div>


 <p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>



 <br />
<div class="footerbody" align="center" >
	

	The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2012 ACM, Inc.<br />
	<a href="http://www.acm.org/publications/policies/usage">Terms of Usage</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;	  
	<a href="http://www.acm.org/about/contact-us">Contact Us</a>

<br /><br />
Useful downloads: 
<a href="http://www.adobe.com/products/acrobat/readstep2.html"><img src="http://dl.acm.org/images/pdf_logo.gif" width="16" height="16" alt="" border="0" /> Adobe Acrobat</a>
&nbsp;&nbsp;
<a href="http://www.apple.com/quicktime/download/" target="_blank"><img src="http://dl.acm.org/images/qtlogo.gif" width="16" height="16" alt="" border="0" /> QuickTime</a>
&nbsp;&nbsp;
<a href="http://www.microsoft.com/windows/windowsmedia/download/default.asp" target="_blank"><img src="http://dl.acm.org/images/wmv.gif" width="16" height="15" alt="" border="0" /> Windows Media Player</a>
&nbsp;&nbsp;
<a href="http://www.real.com/" target="_blank"><img src="http://dl.acm.org/images/realplayer.gif" width="20" height="18" alt="" border="0" /> Real Player</a>

</div> 



<div  id="cf_window1338242341337" class="yuiextdlg">
	
	<div  id="theguide_title" class="x-dlg-hd">
		The ACM Guide to Computing Literature
	 </div>
	<div  id="theguide_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242341340" class="yuiextdlg">
	
	<div  id="thetags_title" class="x-dlg-hd">
		All Tags
	 </div>
	<div  id="thetags_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242341343" class="yuiextdlg">
	
	<div  id="theformats_title" class="x-dlg-hd">
		Export Formats
	 </div>
	<div  id="theformats_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242341345" class="yuiextdlg">
	
	<div  id="theexplaination_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theexplaination_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242341347" class="yuiextdlg">
	
	<div  id="theservices_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theservices_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242341349" class="yuiextdlg">
	
	<div  id="savetobinder_title" class="x-dlg-hd">
		Save to Binder
	 </div>
	<div  id="savetobinder_body" class="x-dlg-bd">
		
		
	 </div>
 </div> 

</body>
</html>