


<!doctype html>


<head><script type="text/javascript">_cf_loadingtexthtml="<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/CFIDE/scripts/ajax";
_cf_jsonprefix='//';
_cf_clientid='E90D90AB2C8AA37647BC10F94C4F423C';</script><script type="text/javascript" src="/CFIDE/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfform.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/masks.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/FCKeditor/fckeditor.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/adapter/yui/ext-yui-adapter.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/ext-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/resizable.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dragdrop/dragdrop.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/util.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/build/state/State-min.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/widget-core.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/ext/package/dialog/dialogs.js"></script>
<script type="text/javascript" src="/CFIDE/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/CFIDE/scripts/ajax/resources/cf/cf.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />



<title>Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
	
  --></style>
 


<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>


<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  function setDiv() {
	var m = document.getElementById('divmain');
	var mh = m.offsetHeight;
	var t = document.getElementById('divtools');
	var th = t.offsetHeight;
	var tg = document.getElementById('divtags');
	var tgh = tg.offsetHeight;
	var calcheight = mh - th;
	if (tgh > calcheight  ){
	  var x = (th + tgh) - mh;
	  if ( (th + tgh) - mh < 65) {
	  }
	  else {
		 document.getElementById('divtags').innerHTML = ""; 
		 var tg = document.getElementById('divtags');
		 var tgh = tg.offsetHeight;
		 tg.style.height = tgh  + 'px';
	  }
	}
	else {
		tg.style.height = calcheight + 'px';
		document.getElementById('divtags').innerHTML = "";
	}

//  do I need to check after I resize to be sure I didn't go too big?
//	var tg2 = document.getElementById('divtags');
//	var tgh2 = tg.offsetHeight;	
//	if (tgh2 > mh + 65) {
//	  var y = mh + 65;
//	  alert('expanded too much ' + tgh2 + ' should be at most ' + y);
//	  tg.style.height = y + 'px';
//	  document.getElementById('divtags').innerHTML = "";
//	}

  }
</script>

<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>



<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Fong, Terry; General Chair-Dautenhahn, Kerstin; Program Chair-Scheutz, Matthias; Program Chair-Demiris, Yiannis"> <meta name="citation_title" content="Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction"> <meta name="citation_date" content="03/12/2008"> <meta name="citation_isbn" content="978-1-60558-017-3"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1349822"> 



<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFFORM');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFDIV');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFTEXTAREA');
</script>

<script type="text/javascript">
	ColdFusion.Ajax.importTag('CFWINDOW');
</script>

<script type="text/javascript">
	var _cf_window_init_1338242121510=function()
	{
		_cf_bind_init_1338242121511=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide_body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242121511);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window1338242121509', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242121510);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242121513=function()
	{
		_cf_bind_init_1338242121514=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags_body','bindExpr':['showthetags.cfm?id=1349822']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242121514);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1349822',{ modal:false, closable:true, divid:'cf_window1338242121512', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242121513);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242121516=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window1338242121515', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242121516);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242121518=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window1338242121517', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242121518);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242121520=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window1338242121519', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242121520);
</script>

<script type="text/javascript">
	var _cf_window_init_1338242121522=function()
	{
		_cf_bind_init_1338242121523=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder_body','bindExpr':['savetobinder.cfm?id=1349822']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_1338242121523);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1349822',{ modal:false, closable:true, divid:'cf_window1338242121521', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_1338242121522);
</script>
</head>

<body style="text-align:center" onLoad="window.focus();">

<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'http://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
                        
<script src='AC_RunActiveContent.js' type="text/javascript"></script>




<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>



<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
	
    <tr style="vertical-align:top">
		
		<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text"  ><img src="http://dl.acm.org/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
		</td>
        
        <td style="padding-left: 5px; padding-right:10px; padding-bottom:0px;" class="small-link-text">
        	<table style="width:100%; border-collapse:collapse; padding:0px">
			<tr><td style="text-align:center">
				
                            <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
                    
					</td>		
			</tr>
			</table> 
        </td>
		<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text">
			 <p style="margin-top:0px; margin-bottom:10px;">
					
                            <a href="https://dl.acm.org/signin.cfm?cfid=105752528&amp;cftoken=27713757" class="small-link-text" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
                            &nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm?cfid=105752528&amp;cftoken=27713757"  class="small-link-text" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
						
			 </p>
            
			<table style="padding: 5px; border-collapse:collapse; float:right">
				
				
                            
                            <tr>
                            <td class="small-link-text" style="text-align:right">
                            <form name="qiksearch" action="results.cfm?h=1&amp;cfid=105752528&amp;cftoken=27713757" method="post">
                           
                           
                            
                                     
                                    <span style="margin-left:0px"><label><input type="text" name="query" size="34" value=" " /></label>&nbsp;
                                    <input style="vertical-align:top;" type="image" alt="Search" name="Go" src="http://dl.acm.org/images/search_small.jpg" />
                                    
                                    </span>
							  </form>
                                </td>
                            </tr>
                          
				  
			</table>

			
			
		</td>

	</tr>
    
    
    <tr><td colspan="3" class="small-link-text" style="padding-bottom:5px; padding-top:0px; text-align:center">
		<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
         
         </td>
    </tr>
    </table>
	
<map name="port" id="port" > 
  <area shape="rect" coords="1,1,60,50" href="http://www.acm.org/" alt="ACM Home Page" />
  <area shape="rect" coords="65,1,275,68" href="http://dl.acm.org/dl.cfm?CFID=105752528&CFTOKEN=27713757" alt="ACM Digital Library Home Page" />
</map>

<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
	<tr style="vertical-align:top">
		<td style="padding-right:10px; text-align:left" class="small-link-text">
        	<div id="divmain" style="border:1px solid #356b20;">
				 
				<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
					
                    	<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;"><strong>Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction</strong></h1>
                        
                </div>
                
                  

<table class="medium-text" style="border-collapse:collapse; padding:0px;">

<col style="width:540px" />

<tr style="vertical-align:top">
  <td>
    <table style="border-collapse:collapse; padding:2px;" class="medium-text">
      <col style="width:80px;" />
      <col style="width:auto" />
      <tr style="vertical-align:top">
        
      </tr>
    </table>

	
        <table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
            <col style="width:80px" />
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             General Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81455605555&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752528&amp;cftoken=27713757" title="Author Profile Page" target="_self">Terry Fong</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1015919&CFID=105752528&CFTOKEN=27713757" title="Institutional Profile Page"><small>NASA Ames Research Center, USA</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100447769&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752528&amp;cftoken=27713757" title="Author Profile Page" target="_self">Kerstin Dautenhahn</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1025943&CFID=105752528&CFTOKEN=27713757" title="Institutional Profile Page"><small>University of Hertfordshire, UK</small></a>
                      	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             Program Chairs:
                
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100029514&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752528&amp;cftoken=27713757" title="Author Profile Page" target="_self">Matthias Scheutz</a>
                
            </td>
            <td valign="bottom">
                
                        <small>Indiana University Bloomington, USA</small>
                    	
            </td>
            </tr>
            
            <tr>
            <td  valign="top" nowrap="nowrap">
             
            </td>
            <td valign="top" nowrap="nowrap">
                
                    <a  href="author_page.cfm?id=81100095668&amp;coll=DL&amp;dl=ACM&amp;trk=0&amp;cfid=105752528&amp;cftoken=27713757" title="Author Profile Page" target="_self">Yiannis Demiris</a>
                
            </td>
            <td valign="bottom">
                
                        <a href="inst_page.cfm?id=1010923&CFID=105752528&CFTOKEN=27713757" title="Institutional Profile Page"><small>Imperial College London, UK</small></a>
                      	
            </td>
            </tr>
            
        </table>
    

  </td>

  <td rowspan="20" nowrap="nowrap">
	<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
		<tr>
        	<td align="center" style="padding-bottom: 5px;">
			  
               <img src="http://portalparts.acm.org/1350000/1349822/thumb/cover_thumb.jpg" title="Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction" height="100"  width="78" ALT="Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction" /> 
              </td>
              <td valign="top" align="left" nowrap="nowrap">
	             <img src="images/acm_mini.jpg" title="Published by ACM" alt="Published by ACM" /> 2008 Proceeding<br />
                 
        	 </td>
        </tr>
        
        <tr>
        	<td colspan="2" valign="baseline" style="padding-bottom:5px;">
            <img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
            <a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
            </td>
         </tr>
         <tr>
            <td  class="small-text" colspan="2" valign="top" style="padding-left:30px;">
				
	                    	&middot;&nbsp;Downloads (6 Weeks): 1,097<br />
    	                    &middot;&nbsp;Downloads (12 Months): 8,456<br />
                          
                        &middot;&nbsp;Citation Count: 226 
			</td>
         </tr>

	</table>
  </td>
</tr>
</table>

<br clear="all" />

<table border="0" class="medium-text" cellpadding="2" cellspacing="0">

<tr valign="top">
    <td colspan="3"><table border="0" class="medium-text" cellpadding="1" cellspacing="0">

<tr valign="top">
    <td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>

	<tr valign="top">
    	<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
	</tr>
    <tr valign="top">
	    <td style="padding-left:10px;">
		   <a href="http://www.hri2008.org" title="Conference Website"  target="_self" class="link-text">HRI'08</a> International Conference on Human Robot Interaction 
        </td>
	</tr>
    
    <tr valign="top">
	    <td style="padding-left:10px; padding-bottom:10px"> Amsterdam, Netherlands &mdash; March 12 - 15, 2008
                    
                  <br />
                    
                  <a href="http://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text">  &copy;2008</span> 
                  <br />           
                  
      </td>
	</tr>
	 

</table></td>
</tr>

</table>

                  
                 <br clear="all" />
			</div>
			
		</td>
		<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
	
            <div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
              <div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;"><strong>Tools and Resources</strong></h1></div>


<ul title="Tools and Resources" style="list-style: none; list-style-position:inside;
margin-left: 0px;
padding-left: 0em;
text-indent: 5px;
margin-bottom: 0px;">


<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:6px;">
   <span class="small-link-text">TOC Service:</span>
   	  
	  <img src="http://dl.acm.org/images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
      <ul style="margin-left: 0; padding-left: 0; display:inline;">
      	
        <li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
        <li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');"  class="small-link-text">RSS</a></li>
		        
      </ul>
    </span>
</li>

        <li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:6px;">
        <a href="citation.cfm?id=1349822&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
         </span></li>
    




<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Export Formats:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1349822&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1349822&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
        <li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1349822&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
      </ul>
    </span>
</li>



 
   <li style="list-style-image:url(img/calbullet.jpg);margin-top:15px;"><span style="margin-left:6px; margin-bottom:0px">
   <span class="small-link-text">Upcoming Conference:</span>
      <ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
        <li style="list-style:disc; display:inline; margin-bottom:0px; margin-left:25px;"><a href="http://humanrobotinteraction.org/2013/" title="ACM/IEEE International Conference on Human-Robot Interaction" class="small-link-text">HRI'13</a></li>
      </ul>
    </span>
	</li>
    


</ul>           

  <!-- ADDTHIS BUTTON BEGIN -->
  
  <!-- ADDTHIS BUTTON END -->

<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>
  <!-- AddThis Button BEGIN -->



<!-- AddThis Button BEGIN -->
<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_slashdot"></a>
<a class="addthis_button_reddit"></a>


<span class="addthis_separator">|</span>
<a href="http://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>
<!-- AddThis Button END -->

  
 

  
  
            </div>
            
		</td>
	</tr>
    
</table>



</div>


<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">




<div id="fback" style="text-align:left; padding-top:10px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="feedback" border="0" /></a>
<a title="feedback" href="mailto:portal-feedback@hq.acm.org?subject=Comments_on_new_design"><strong>Feedback</strong></a>

<span style="padding:10px;">|</span>




<span>Switch to <a href="citation.cfm?id=1349822&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>


</span>

 
<div class="small-text" style="margin-top:10px; margin-bottom:5px;"> 
<br />

    <a href="#abstract"  title="Abstract" style="padding:5px"><span>Abstract</span></a> |
    
    <a href="#formats"  title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
    
    <a href="#authors"  title="Authors" style="padding:5px"><span>Authors</span></a> |
    <a href="#references"  title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
    <a href="#citedby"  title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
    <a href="#indexterms"  title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
    <a href="#source"  title="Publication" style="padding:5px"><span>Publication</span></a> |
    <a href="#revs"  title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |               
	<a href="#comments"  title="Comments" style="padding:5px"><span>Comments</span></a>
	
     |               
	<a href="#prox"  title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
    
</div>
    
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;"/>



</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
       	
			<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

			

		
			
           
			
				
				<p>
					<div style="display:inline"><p>It is our great pleasure to welcome you to the 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI 2008). HRI is a single-track, highly selective annual conference that seeks to showcase the very best research and thinking in human-robot interaction. Human-robot interaction is inherently inter-disciplinary and multi-disciplinary, and the conference sought papers from researchers in artificial intelligence, cognitive science, ergonomics, human-computer interaction, psychology, robotics, and other fields.</p> <p>The theme of HRI 2008, "Living With Robots", highlights the importance of building core science in HRI so that robots can be employed in everyday environments over the long-term. In particular, understanding and identifying the key social and technical issues for robots operating in settings such as home, office, shopping, and museum environments is crucial for developing effective systems. This year's conference places special emphasis on informing the development of companion and assistive robots.</p> <p>Of the 134 submissions, the program committee accepted 48 full papers and 24 short papers that cover a variety of topics, including field experiments and user studies, HRI foundations, multi-modal interaction, and ethics. A total of 10 videos (out of 27 submissions) are featured in a special session. Full papers are presented via either oral or poster presentation; all are archived in the ACM Digital Library. Two-page short papers showcase late-breaking results, but are not archived. Both full and short papers were selected via a highly rigorous review process based strictly on merit and impact to the field of HRI. We hope that these proceedings will serve as a valuable reference for researchers and students.</p></div>
				</p>
   				
           	</div>
			
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


  <div class="abstract">
        <SPAN><strong>FRONT MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1350000/1349822/fm/frontmatter.pdf?ip=188.194.239.219&CFID=105752528&CFTOKEN=27713757" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(cd label, copyright, welcome, contents, organization, sponsors) 
  </div>          
  
  <div style="margin-top: 10px;"  class="abstract">
        <SPAN><strong>BACK MATTER</strong></span>
  </div>
  <div style="margin-left:10px; line-height:180%;">
      
          <A NAME="FullText" HREF="http://portalparts.acm.org/1350000/1349822/bm/backmatter.pdf?ip=188.194.239.219&CFID=105752528&CFTOKEN=27713757" title="PDF" target="_blank">
          <img src="http://dl.acm.org/imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
          &nbsp;(author index) 
  </div>          
  
<div style="margin-top: 10px; height: auto; padding: 5px; ">
		
		
        


	</div>

<br clear="all" />
</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<dl title="Authors" style="margin-top:0px">




<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 General Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Terry Fong" href="author_page.cfm?id=81455605555&CFID=105752528&CFTOKEN=27713757">Terry Fong</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">2001-2010</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">14</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">102</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">5</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">56</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">554</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Terry Fong" href="author_page.cfm?id=81455605555&amp;dsp=coll&amp;trk=1&amp;CFID=105752528&CFTOKEN=27713757" target="_self">View colleagues</a> of Terry Fong
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Kerstin Dautenhahn" href="author_page.cfm?id=81100447769&CFID=105752528&CFTOKEN=27713757">Kerstin Dautenhahn</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1997-2011</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">58</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">202</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">18</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">111</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">815</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Kerstin Dautenhahn" href="author_page.cfm?id=81100447769&amp;dsp=coll&amp;trk=1&amp;CFID=105752528&CFTOKEN=27713757" target="_self">View colleagues</a> of Kerstin Dautenhahn
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              



<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
 <strong>
 Program Chairs 
  </strong>
 </dt> 
          
          <dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
			
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Matthias Scheutz" href="author_page.cfm?id=81100029514&CFID=105752528&CFTOKEN=27713757">Matthias Scheutz</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">1999-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">40</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">114</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">17</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">156</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">894</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Matthias Scheutz" href="author_page.cfm?id=81100029514&amp;dsp=coll&amp;trk=1&amp;CFID=105752528&CFTOKEN=27713757" target="_self">View colleagues</a> of Matthias Scheutz
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
								<span>
									
                                    <br><br />
                                    
                                        <table border="0" cellspacing="10">
                                        <tr><td><table border="0" width="300"  cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">

	<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
		
			<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
		
	</td>

<td  bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
	&nbsp;<span class="small-text"><strong><a title="author page of Yiannis Demiris" href="author_page.cfm?id=81100095668&CFID=105752528&CFTOKEN=27713757">Yiannis Demiris</a></strong><br /></span>
	
	
	
	<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
	
	
			
	
	</span>
	
	
	
</td>
</tr>


</table></td>
                                            <td><table border="0" width="300" cellpadding="0" cellspacing="0">

<tr>

	<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
		
        <table width="90%" style="margin-top: 1px; margin-bottom: 10px" border="0" align="left">
			<tr>
				<td class="smaller-text">Publication years</td><td class="small-text" align="right">2005-2012</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">22</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Citation Count</td><td class="small-text" align="right">21</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">5</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
			<tr>
				<td class="smaller-text">Downloads (6 Weeks)</td><td class="small-text" align="right">14</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
			<tr>
				<td class="smaller-text">Downloads (12 Months)</td><td class="small-text" align="right">155</td>
			</tr>
			<tr><td height="1" bgcolor="#808080" colspan="2"></td></tr>
 			
		</table>
	</td>
	
</tr>

</table></td>
                                        </tr>
                                        <tr><td style="padding:0px">
                                                    <a title="colleagues of Yiannis Demiris" href="author_page.cfm?id=81100095668&amp;dsp=coll&amp;trk=1&amp;CFID=105752528&CFTOKEN=27713757" target="_self">View colleagues</a> of Yiannis Demiris
                                            </td>
                                         </tr>
                                        </table>
                                     
								</span>
					
			</dd>
                              

</dl>
</div>

		  
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	References are not available

</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
		
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	Citings are not available
		
 </div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

Index Terms are not available


</div>


<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">



<table border="0" class="medium-text" cellpadding="0" cellspacing="5">



    <tr valign="top">
    	<td>Title</td> 
	    <td>
		   <a href="http://www.hri2008.org" title="Conference Website"  target="_self" class="link-text">HRI'08</a> International Conference on Human Robot Interaction 
        </td>
	</tr>
    <tr><td></td><td>Amsterdam, Netherlands &mdash; March 12 - 15, 2008</td></tr> <tr><td>Pages</td><td>394</td></tr> 
                 <tr>
                 
                     <td>Sponsors</td>
                    
                  <td>
                  <a href="sig.cfm?id=SP918&CFID=105752528&CFTOKEN=27713757"> SIGART</a> ACM Special Interest Group on Artificial Intelligence
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="sig.cfm?id=SP923&CFID=105752528&CFTOKEN=27713757"> SIGCHI</a> ACM Special Interest Group on Computer-Human Interaction
                  </td>
                  </tr>
              
                 <tr>
                 
                      <td></td>
                  
                  <td>
                  <a href="http://www.acm.org/"> ACM</a> Association for Computing Machinery
                  </td>
                  </tr>
              
                  <tr><td>Publisher</td><td><a href="http://www.acm.org/publications">ACM</a> New York, NY, USA</td>
				  </tr>
             <tr><td>ISBN</td><td> 978-1-60558-017-3</td></tr> <tr><td>Order Number</td><td>609084</td></tr> 
			<tr valign="top">
        	<td>Conference</td>
            <td valign="top" align="left"  style="padding-bottom: 25px;">
	            <strong style="padding-right:10px">HRI</strong><a href="event.cfm?id=RE285&CFID=105752528&CFTOKEN=27713757" title="ACM/IEEE International Conference on Human-Robot Interaction">ACM/IEEE International Conference on Human-Robot Interaction</a>
                
                       
                        <a href="event.cfm?id=RE285&CFID=105752528&CFTOKEN=27713757" title="ACM/IEEE International Conference on Human-Robot Interaction"><img border="0" src="http://portalparts.acm.org/event_logos/677/677.jpg" title="HRI logo" height="62"  width="100" ALT="HRI logo" style="vertical-align:top"></a>
						 

        	 </td>
            </tr>
		    <tr><td colspan="2">Paper Acceptance Rate 48 of 134 submissions, 36%</td></tr> <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 227 of 905 submissions, 25%</td></tr>
                       <tr valign="top">
                        <td colspan="2" style="padding-left:25px;">
                        	<table>
                            	<tr><td>
                                        <!-- WebCharts3D v5.1(2077) -->
<IMG SRC="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=Images/2455395680525696.JPG" id="Images_2455395680525696_JPG" name="Images_2455395680525696_JPG" usemap="#Images_2455395680525696_JPG_map" border="0"/>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAB' id='GP1338242122105AAAB'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>140</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAC' id='GP1338242122105AAAC'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '06</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>41</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAD' id='GP1338242122105AAAD'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>101</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAE' id='GP1338242122105AAAE'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '07</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>22</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAF' id='GP1338242122105AAAF'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>134</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAG' id='GP1338242122105AAAG'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '08</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>48</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAH' id='GP1338242122105AAAH'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>120</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAI' id='GP1338242122105AAAI'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '09</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>23</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAJ' id='GP1338242122105AAAJ'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>124</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAK' id='GP1338242122105AAAK'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '10</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>26</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAL' id='GP1338242122105AAAL'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>149</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAM' id='GP1338242122105AAAM'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '11</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>33</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAN' id='GP1338242122105AAAN'><tr><td width='8'>&nbsp;</td><td width='62'>Submitted</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='62'>137</td></tr></table>
<table cellpadding='0' cellspacing='1' style='visibility: hidden;display: none; position:absolute;font-family: Dialog;font-size: 11px;background:#FFFFFF;foreground:#333333;color:#333333;-moz-opacity:.78;-opacity:.78;filter:alpha(opacity=78);border:1px solid #333333;' name='GP1338242122105AAAO' id='GP1338242122105AAAO'><tr><td width='8'>&nbsp;</td><td width='58'>Accepted</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>HRI '12</td></tr><tr><td width='8'>&nbsp;</td><td width='58'>34</td></tr></table>
<MAP name='Images_2455395680525696_JPG_map'>
<AREA shape='rect' coords='0,0,1,1'/>
<AREA shape="rect" coords="293,179,309,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAO",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAO",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAO",event)'/>
<AREA shape="rect" coords="277,74,293,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAN",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAN",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAN",event)'/>
<AREA shape="rect" coords="253,180,269,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAM",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAM",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAM",event)'/>
<AREA shape="rect" coords="237,62,253,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAL",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAL",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAL",event)'/>
<AREA shape="rect" coords="213,187,229,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAK",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAK",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAK",event)'/>
<AREA shape="rect" coords="197,87,213,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAJ",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAJ",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAJ",event)'/>
<AREA shape="rect" coords="173,190,189,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAI",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAI",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAI",event)'/>
<AREA shape="rect" coords="157,91,173,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAH",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAH",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAH",event)'/>
<AREA shape="rect" coords="133,165,149,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAG",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAG",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAG",event)'/>
<AREA shape="rect" coords="117,77,133,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAF",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAF",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAF",event)'/>
<AREA shape="rect" coords="93,191,109,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAE",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAE",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAE",event)'/>
<AREA shape="rect" coords="77,111,93,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAD",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAD",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAD",event)'/>
<AREA shape="rect" coords="53,172,69,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAC",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAC",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAC",event)'/>
<AREA shape="rect" coords="37,71,53,213" onMouseover='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAB",event,true)' onMouseout='xx_set_visible("Images_2455395680525696_JPG","GP1338242122105AAAB",event,false)' onMousemove='xx_move_tag("Images_2455395680525696_JPG","GP1338242122105AAAB",event)'/>
<AREA shape="rect" coords="160,13,227,27"/>
<AREA shape="rect" coords="89,13,160,27"/>
</MAP>

<script language="javascript" src="/CFIDE/GraphData.cfm?graphCache=wc50&graphID=script.js"></script>

                                      </td>
                                      <td style="padding-left:20px;">
                                             <table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
                                                <tr bgcolor="#ffffff">
                                                  <th style="width:50%">Year</th>
                                                  <th  align="right" style="width:15%">Submitted</th>
                                                  <th  align="right" style="width:15%">Accepted</th>
                                                  <th  align="center">Rate</th>
                                                </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '06</td>
                                                            <td align="right">140</td>
                                                            <td align="right">41</td>
                                                            <td align="center">29%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '07</td>
                                                            <td align="right">101</td>
                                                            <td align="right">22</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '08</td>
                                                            <td align="right">134</td>
                                                            <td align="right">48</td>
                                                            <td align="center">36%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '09</td>
                                                            <td align="right">120</td>
                                                            <td align="right">23</td>
                                                            <td align="center">19%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '10</td>
                                                            <td align="right">124</td>
                                                            <td align="right">26</td>
                                                            <td align="center">21%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#ffffff">
                                                            <td>HRI '11</td>
                                                            <td align="right">149</td>
                                                            <td align="right">33</td>
                                                            <td align="center">22%</td>
                                                         </tr>
                                                
                                                        <tr bgcolor="#f0f0f0">
                                                            <td>HRI '12</td>
                                                            <td align="right">137</td>
                                                            <td align="right">34</td>
                                                            <td align="center">25%</td>
                                                         </tr>
                                                
                                                 <tr bgcolor="#ffffff">
                                                    <td><strong>Overall</strong></td>
                                                    <td align="right">905</td>
                                                    <td align="right">227</td>
                                                    <td align="center">25%</td>
                                                  </tr>
                                                </table>
                                       </td>
                                     </tr>
                               </table>
                        </td>
                    </tr>
                     
                     
            
</table>


</table>




</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
        
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">

    	<br />Reviews are not available for this item
        
        <div align="left" style="margin-top:30px">
					<a title="Computing Reviews" href="ocr_review_main.cfm?CFID=105752528&CFTOKEN=27713757">
                 <img src="http://dl.acm.org/images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
        
        
       		<ul style="list-style:disc; display:inline-block">
	            <li>Access <a href="ocr_review_main.cfm?CFID=105752528&CFTOKEN=27713757" target="_blank">critical reviews</a> of computing literature.</li>
            	<li><a href="http://www.computingreviews.com/Reviewer/"  target="_blank">Become a reviewer</a> for Computing Reviews</li>
            </ul>
        </div>
        
</div>



<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
         
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">


<div>
<div>
	
	<p style="margin-left:5px;">
    <strong>Be the first to comment</strong>
    	
          	To Post a comment please <a href="signin.cfm?CFID=105752528&CFTOKEN=27713757">sign in or create</a> a free Web account</a>
        
    
    
	 </p>
	   	
 
</div>


</div>

	
		<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="http://dl.acm.org/images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
        
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" >

<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction</h5>


<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>

<div style="clear:both">
    
        <div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1228716&picked=prox&CFID=105752528&CFTOKEN=27713757" title="previous: HRI '07"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1514095&picked=prox&CFID=105752528&CFTOKEN=27713757" title="Next: HRI '09">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
        
</div>


 
<table class="text12" border="0">

  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Technical papers</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349824&CFID=105752528&CFTOKEN=27713757">Achieving fluency through perceptual-symbol practice in human-robot collaboration</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100537802&CFID=105752528&CFTOKEN=27713757">Guy Hoffman</a>, 
                        <a href="author_page.cfm?id=81100258451&CFID=105752528&CFTOKEN=27713757">Cynthia Breazeal</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 1-8</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349824" title="DOI">10.1145/1349822.1349824</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349824&ftid=506287&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow2" style="display:inline;"><br /><div style="display:inline">We have developed a cognitive architecture for robotic teammates based on the neuro-psychological principles of perceptual symbols and simulation, with the aim of attaining increased fluency in human-robot teams. An instantiation of this architecture ...</div></span>
          <span id="toHide2" style="display:none;"><br /><div style="display:inline"><p>We have developed a cognitive architecture for robotic teammates based on the neuro-psychological principles of perceptual symbols and simulation, with the aim of attaining increased fluency in human-robot teams. An instantiation of this architecture was implemented on a robotic desk lamp, performing in a human-robot collaborative task. This paper describes initial results from a human-subject study measuring team efficiency and team fluency, in which the robot works on a joint task with untrained subjects. We find significant differences in a number of efficiency and fluency metrics, when comparing our architecture to a purely reactive robot with similar capabilities.</p></div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349825&CFID=105752528&CFTOKEN=27713757">Assessing cooperation in human control of heterogeneous robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81450595190&CFID=105752528&CFTOKEN=27713757">Jijun Wang</a>, 
                        <a href="author_page.cfm?id=81100349943&CFID=105752528&CFTOKEN=27713757">Michael Lewis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 9-16</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349825" title="DOI">10.1145/1349822.1349825</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349825&ftid=505303&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow3" style="display:inline;"><br /><div style="display:inline">Human control of multiple robots has been characterized by the average demand of single robots on human attention. While this matches situations in which independent robots are controlled sequentially it does not capture aspects of demand associated ...</div></span>
          <span id="toHide3" style="display:none;"><br /><div style="display:inline"><p>Human control of multiple robots has been characterized by the average demand of single robots on human attention. While this matches situations in which independent robots are controlled sequentially it does not capture aspects of demand associated with coordinating dependent actions among robots. This paper presents an extension of Crandall's neglect tolerance model intended to accommodate both coordination demands (CD) and heterogeneity among robots. The reported experiment attempts to manipulate coordination demand by varying the proximity needed to perform a joint task in two conditions and by automating coordination within subteams in a third. Team performance and the process measure CD were assessed for each condition. Automating cooperation reduced CD and improved performance. We discuss the utility of process measures such as CD to analyze and improve control performance.</p></div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349826&CFID=105752528&CFTOKEN=27713757">Behaviour delay and robot expressiveness in child-robot interactions: a user study on interaction kinesics</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100599834&CFID=105752528&CFTOKEN=27713757">Ben Robins</a>, 
                        <a href="author_page.cfm?id=81100447769&CFID=105752528&CFTOKEN=27713757">Kerstin Dautenhahn</a>, 
                        <a href="author_page.cfm?id=81333491529&CFID=105752528&CFTOKEN=27713757">Rene te Boekhorst</a>, 
                        <a href="author_page.cfm?id=81100341427&CFID=105752528&CFTOKEN=27713757">Chrystopher L. Nehaniv</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 17-24</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349826" title="DOI">10.1145/1349822.1349826</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349826&ftid=506283&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow4" style="display:inline;"><br /><div style="display:inline">This paper presents results of a novel study on interaction kinesics where 18 children interacted with a humanoid child-sized robot called KASPAR. Based on findings in psychology and social sciences we propose the temporal behaviour matching hypothesis ...</div></span>
          <span id="toHide4" style="display:none;"><br /><div style="display:inline"><p>This paper presents results of a novel study on interaction kinesics where 18 children interacted with a humanoid child-sized robot called KASPAR. Based on findings in psychology and social sciences we propose the temporal behaviour matching hypothesis which predicts that children will adapt to and match the robot's temporal behaviour. Each child took part in six experimental trials involving two games in which the dynamics of interactions played a key part: a body expression imitation game, where the robot imitated expressions demonstrated by the children, and a drumming game where the robot mirrored the children's drumming. In both games KASPAR responded either with or without a delay. Additionally, in the drumming game, KASPAR responded with or without exhibiting facial/gestural expressions. Individual case studies as well as statistical analysis of the complete sample are presented. Results show that a delay of the robot's drumming response lead to larger pauses (with and without robot nonverbal gestural expressions) and longer drumming durations (with nonverbal gestural expressions only). In the imitation game, the robot's delay lead to longer imitation eliciting behaviour with longer pauses for the children, but systematic individual differences are observed in regards to the effects on the children's pauses. Results are generally consistent with the temporal behaviour matching hypothesis, i.e. children adapted the timing of their behaviour, e.g. by mirroring to the robot's temporal behaviour.</p></div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349827&CFID=105752528&CFTOKEN=27713757">Beyond dirty, dangerous and dull: what everyday people think robots should do</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100499212&CFID=105752528&CFTOKEN=27713757">Leila Takayama</a>, 
                        <a href="author_page.cfm?id=81100098733&CFID=105752528&CFTOKEN=27713757">Wendy Ju</a>, 
                        <a href="author_page.cfm?id=81100153283&CFID=105752528&CFTOKEN=27713757">Clifford Nass</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 25-32</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349827" title="DOI">10.1145/1349822.1349827</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349827&ftid=506284&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow5" style="display:inline;"><br /><div style="display:inline">We present a study of people's attitudes toward robot workers, identifying the characteristics of occupations for which people believe robots are qualified and desired. We deployed a web-based public-opinion survey that asked respondents (n=250) about ...</div></span>
          <span id="toHide5" style="display:none;"><br /><div style="display:inline"><p>We present a study of people's attitudes toward robot workers, identifying the characteristics of occupations for which people believe robots are qualified and desired. We deployed a web-based public-opinion survey that asked respondents (n=250) about their attitudes regarding robots' suitability for a variety of jobs (n=812) from the U.S. Department of Labor's O*NET occupational information database. We found that public opinion favors robots for jobs that require memorization, keen perceptual abilities, and service-orientation. People are preferred for occupations that require artistry, evaluation, judgment and diplomacy. In addition, we found that people will feel more positively toward robots doing jobs with people rather than in place of people.</p></div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349828&CFID=105752528&CFTOKEN=27713757">Combining dynamical systems control and programmingby demonstration for teaching discrete bimanual coordination tasks to a humanoid robot</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350587405&CFID=105752528&CFTOKEN=27713757">Elena Gribovskaya</a>, 
                        <a href="author_page.cfm?id=81100342762&CFID=105752528&CFTOKEN=27713757">Aude Billard</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 33-40</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349828" title="DOI">10.1145/1349822.1349828</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349828&ftid=506285&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow6" style="display:inline;"><br /><div style="display:inline">We present a generic framework that combines Dynamical Systems movement control with Programming by Demonstration (PbD) to teach a robot bimanual coordination task. The model consists of two systems: a learning system that processes data collected during ...</div></span>
          <span id="toHide6" style="display:none;"><br /><div style="display:inline"><p>We present a generic framework that combines Dynamical Systems movement control with Programming by Demonstration (PbD) to teach a robot bimanual coordination task. The model consists of two systems: a learning system that processes data collected during the demonstration of the task to extract coordination constraints and a motor system that reproduces the movements dynamically, while satisfying the coordination constraints learned by the first system. We validate the model through a series of experiments in which a robot is taught bimanual manipulatory tasks with the help of a human.</p></div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349829&CFID=105752528&CFTOKEN=27713757">A comparative psychophysical and EEG study of different feedback modalities for HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350585358&CFID=105752528&CFTOKEN=27713757">Xavier Perrin</a>, 
                        <a href="author_page.cfm?id=81318493385&CFID=105752528&CFTOKEN=27713757">Ricardo Chavarriaga</a>, 
                        <a href="author_page.cfm?id=81350582126&CFID=105752528&CFTOKEN=27713757">C&#233;line Ray</a>, 
                        <a href="author_page.cfm?id=81325490184&CFID=105752528&CFTOKEN=27713757">Roland Siegwart</a>, 
                        <a href="author_page.cfm?id=81100190806&CFID=105752528&CFTOKEN=27713757">Jos&#233; del R. Mill&#225;n</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 41-48</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349829" title="DOI">10.1145/1349822.1349829</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349829&ftid=506286&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow7" style="display:inline;"><br /><div style="display:inline">This paper presents a comparison between six different ways to convey navigational information provided by a robot to a human. Visual, auditory, and tactile feedback modalities were selected and designed to suggest a direction of travel to a human user, ...</div></span>
          <span id="toHide7" style="display:none;"><br /><div style="display:inline"><p>This paper presents a comparison between six different ways to convey navigational information provided by a robot to a human. Visual, auditory, and tactile feedback modalities were selected and designed to suggest a direction of travel to a human user, who can then decide if he agrees or not with the robot's proposition. This work builds upon a previous research on a novel semi-autonomous navigation system in which the human supervises an autonomous system, providing corrective monitoring signals whenever necessary.</p> <p>We recorded both qualitative (user impressions based on selected criteria and ranking of their feelings) and quantitative (response time and accuracy) information regarding different types of feedback. In addition, a preliminary analysis of the influence of the different types of feedback on brain activity is also shown. The result of this study may provide guidelines for the design of such a human-robot interaction system, depending on both the task and the human user.</p></div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349830&CFID=105752528&CFTOKEN=27713757">Compass visualizations for human-robotic interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81325488567&CFID=105752528&CFTOKEN=27713757">Curtis M. Humphrey</a>, 
                        <a href="author_page.cfm?id=81100313646&CFID=105752528&CFTOKEN=27713757">Julie A. Adams</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 49-56</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349830" title="DOI">10.1145/1349822.1349830</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349830&ftid=506288&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow8" style="display:inline;"><br /><div style="display:inline">Compasses have been used for centuries to express directions and are commonplace in many user interfaces; however, there has not been work in human-robotic interaction (HRI) to ascertain how different compass visualizations affect the interaction. This ...</div></span>
          <span id="toHide8" style="display:none;"><br /><div style="display:inline"><p>Compasses have been used for centuries to express directions and are commonplace in many user interfaces; however, there has not been work in human-robotic interaction (HRI) to ascertain how different compass visualizations affect the interaction. This paper presents a HRI evaluation comparing two representative compass visualizations: top-down and in-world world-aligned. The compass visualizations were evaluated to ascertain which one provides better metric judgment accuracy, lowers workload, provides better situational awareness, is perceived as easier to use, and is preferred. Twenty-four participants completed a within-subject repeated measures experiment. The results agreed with the existing principles relating to 2D and 3D views, or projections of a three-dimensional scene, in that a top-down (2D view) compass visualization is easier to use for metric judgment tasks and a world-aligned (3D view) compass visualization yields faster performance for general navigation tasks. The implication for HRI is that the choice in compass visualization has a definite and non-trivial impact on operator performance (world-aligned was faster), situational awareness (top-down was better), and perceived ease of use (top-down was easier).</p></div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349831&CFID=105752528&CFTOKEN=27713757">Concepts about the capabilities of computers and robots: a test of the scope of adults' theory of mind</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350571966&CFID=105752528&CFTOKEN=27713757">Daniel T. Levin</a>, 
                        <a href="author_page.cfm?id=81350586647&CFID=105752528&CFTOKEN=27713757">Stephen S. Killingsworth</a>, 
                        <a href="author_page.cfm?id=81350578238&CFID=105752528&CFTOKEN=27713757">Megan M. Saylor</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 57-64</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349831" title="DOI">10.1145/1349822.1349831</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349831&ftid=506289&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow9" style="display:inline;"><br /><div style="display:inline">We have previously demonstrated that people apply fundamentally different concepts to mechanical agents and human agents, assuming that mechanical agents engage in more location-based, and feature-based behaviors whereas humans engage in more goal-based, ...</div></span>
          <span id="toHide9" style="display:none;"><br /><div style="display:inline"><p>We have previously demonstrated that people apply fundamentally different concepts to mechanical agents and human agents, assuming that mechanical agents engage in more location-based, and feature-based behaviors whereas humans engage in more goal-based, and category-based behavior. We also found that attributions about anthropomorphic agents such as robots are very similar to those about computers, unless subjects are asked to attend closely to specific intentional-appearing behaviors. In the present studies, we ask whether subjects initially do not attribute intentionality to robots because they believe that temporary limits in current technology preclude real intelligent behavior. In addition, we ask whether a basic categorization as an artifact affords lessened attributions of intentionality. We find that subjects assume that robots created with future technology may become more intentional, but will not be fully equivalent to humans, and that even a fully human-controlled robot will not be as intentional as a human. These results suggest that subjects strongly distinguish intelligent agents based on intentionality, and that the basic living/mechanical distinction is powerful enough, even in adults, to make it difficult for adults to assent to the possibility that mechanical things can be fully intentional.</p></div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349832&CFID=105752528&CFTOKEN=27713757">Construction and evaluation of a model of natural human motion based on motion diversity</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100438978&CFID=105752528&CFTOKEN=27713757">Takashi Minato</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752528&CFTOKEN=27713757">Hiroshi Ishiguro</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 65-72</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349832" title="DOI">10.1145/1349822.1349832</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349832&ftid=506290&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow10" style="display:inline;"><br /><div style="display:inline">A natural human-robot communication is supported by a person's interpersonal behavior for a robot. The condition to elicit interpersonal behavior is thought to be related to a mechanism to support natural communication. In the present study, we hypothesize ...</div></span>
          <span id="toHide10" style="display:none;"><br /><div style="display:inline"><p>A natural human-robot communication is supported by a person's interpersonal behavior for a robot. The condition to elicit interpersonal behavior is thought to be related to a mechanism to support natural communication. In the present study, we hypothesize that motion diversity produced independently of a subject's intention contributes to the human-like nature of the motions of an android that closely resembles a human being. In order to verify this hypothesis, we construct a model of motion diversity through the observation of human motion, specifically, a touching motion. Psychological experiments have shown that the presence of motion diversity in android motion influences the impression toward the android. </p></div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349833&CFID=105752528&CFTOKEN=27713757">Crew roles and operational protocols for rotary-wing micro-uavs in close urban environments</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100545387&CFID=105752528&CFTOKEN=27713757">Robin R. Murphy</a>, 
                        <a href="author_page.cfm?id=81350580063&CFID=105752528&CFTOKEN=27713757">Kevin S. Pratt</a>, 
                        <a href="author_page.cfm?id=81320488460&CFID=105752528&CFTOKEN=27713757">Jennifer L. Burke</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 73-80</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349833" title="DOI">10.1145/1349822.1349833</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349833&ftid=506291&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow11" style="display:inline;"><br /><div style="display:inline">A crew organization and four-step operational protocol is recommended based on a cumulative descriptive field study of teleoperated rotary-wing micro air vehicles (MAV) used for structural inspection during the response and recovery phases of Hurricanes ...</div></span>
          <span id="toHide11" style="display:none;"><br /><div style="display:inline"><p>A crew organization and four-step operational protocol is recommended based on a cumulative descriptive field study of teleoperated rotary-wing micro air vehicles (MAV) used for structural inspection during the response and recovery phases of Hurricanes Katrina and Wilma. The use of MAVs for real civilian missions in real operating environments provides a unique opportunity to consider human-robot interaction. The analysis of the human-robot interaction during 8 days, 14 missions, and 38 flights finds that a three person crew is currently needed to perform distinct roles: Pilot, Mission Specialist, and Flight Director. The general operations procedure is driven by the need for safety of bystanders, other aircraft, the tactical team, and the MAV itself, which leads to missions being executed as a series of short, line-of-sight flights rather than a single flight. Safety concerns may limit the utility of autonomy in reducing the crew size or enabling beyond line-of-sight-operations but autonomy could lead to an increase in flights per mission and reduced Pilot training demands. This paper is expected to contribute to set a foundation for future research in HRI and MAV autonomy and to help establish regulations and acquisition guidelines for civilian operations. Additional research in autonomy, interfaces, attention, and out-of-the-loop (OOTL) control is warranted.</p></div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349834&CFID=105752528&CFTOKEN=27713757">Crossmodal content binding in information-processing architectures</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350573611&CFID=105752528&CFTOKEN=27713757">Henrik Jacobsson</a>, 
                        <a href="author_page.cfm?id=81414617742&CFID=105752528&CFTOKEN=27713757">Nick Hawes</a>, 
                        <a href="author_page.cfm?id=81436593209&CFID=105752528&CFTOKEN=27713757">Geert-Jan Kruijff</a>, 
                        <a href="author_page.cfm?id=81388599481&CFID=105752528&CFTOKEN=27713757">Jeremy Wyatt</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 81-88</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349834" title="DOI">10.1145/1349822.1349834</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349834&ftid=506292&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow12" style="display:inline;"><br /><div style="display:inline">Operating in a physical context, an intelligent robot faces two fundamental problems. First, it needs to combine information from its different sensors to form a representation of the environment that is more complete than any representation a single ...</div></span>
          <span id="toHide12" style="display:none;"><br /><div style="display:inline"><p>Operating in a physical context, an intelligent robot faces two fundamental problems. First, it needs to combine information from its different sensors to form a representation of the environment that is more complete than any representation a single sensor could provide. Second, it needs to combine high-level representations (such as those for planning and dialogue) with sensory information, to ensure that the interpretations of these symbolic representations are grounded in the situated context. Previous approaches to this problem have used techniques such as (low-level) information fusion, ontological reasoning, and (high-level) concept learning. This paper presents a framework in which these, and related approaches, can be used to form a shared representation of the current state of the robot in relation to its environment and other agents. Preliminary results from an implemented system are presented to illustrate how the framework supports behaviours commonly required of an intelligent robot.</p></div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349835&CFID=105752528&CFTOKEN=27713757">Decision-theoretic human-robot communication</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350567477&CFID=105752528&CFTOKEN=27713757">Tobias Kaupp</a>, 
                        <a href="author_page.cfm?id=81375605858&CFID=105752528&CFTOKEN=27713757">Alexei Makarenko</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 89-96</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349835" title="DOI">10.1145/1349822.1349835</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349835&ftid=506293&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow13" style="display:inline;"><br /><div style="display:inline">Humans and robots need to exchange information if the objective is to achieve a task cooperatively. Two questions are considered in this paper: what type of information to communicate, and how to cope with the limited resources of human operators. Decision-theoretic ...</div></span>
          <span id="toHide13" style="display:none;"><br /><div style="display:inline"><p>Humans and robots need to exchange information if the objective is to achieve a task cooperatively. Two questions are considered in this paper: what type of information to communicate, and how to cope with the limited resources of human operators. Decision-theoretic human-robot communication can provide answers to both questions: the type of information is determined by the underlying probabilistic representation, and value-of-information theory helps decide when it is appropriate to query operators for information. A robot navigation task is used to evaluate the system by comparing it to conventional teleoperation. The results of a user study show that the developed system is superior with respect to performance, operator workload, and usability.</p></div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349836&CFID=105752528&CFTOKEN=27713757">Design patterns for sociality in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81408592441&CFID=105752528&CFTOKEN=27713757">Peter H. Kahn</a>, 
                        <a href="author_page.cfm?id=81100563869&CFID=105752528&CFTOKEN=27713757">Nathan G. Freier</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752528&CFTOKEN=27713757">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752528&CFTOKEN=27713757">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81350569693&CFID=105752528&CFTOKEN=27713757">Jolina H. Ruckert</a>, 
                        <a href="author_page.cfm?id=81350575852&CFID=105752528&CFTOKEN=27713757">Rachel L. Severson</a>, 
                        <a href="author_page.cfm?id=81329489903&CFID=105752528&CFTOKEN=27713757">Shaun K. Kane</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 97-104</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349836" title="DOI">10.1145/1349822.1349836</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349836&ftid=506294&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow14" style="display:inline;"><br /><div style="display:inline">We propose that Christopher Alexander's idea of design patterns can benefit the emerging field of HRI. We first discuss four features of design patterns that appear particularly useful. For example, a pattern should be specified abstractly enough such ...</div></span>
          <span id="toHide14" style="display:none;"><br /><div style="display:inline"><p>We propose that Christopher Alexander's idea of design patterns can benefit the emerging field of HRI. We first discuss four features of design patterns that appear particularly useful. For example, a pattern should be specified abstractly enough such that many different instantiations of the pattern can be uniquely realized in the solution to specific problems in context. Then, after describing our method for generating patterns, we offer and describe eight possible design patterns for sociality in human robot interaction: initial introduction, didactic communication, in motion together, personal interests and history, recovering from mistakes, reciprocal turn-taking in game context, physical intimacy, and claiming unfair treatment or wrongful harms. We also discuss the issue of validation of design patterns. If a design pattern program proves successful, it will provide HRI researchers with basic knowledge about human robot interaction, and save time through the reuse of patterns to achieve high levels of sociality.</p></div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349837&CFID=105752528&CFTOKEN=27713757">Development and evaluation of a flexible interface for a wheelchair mounted robotic arm</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414601137&CFID=105752528&CFTOKEN=27713757">Katherine Tsui</a>, 
                        <a href="author_page.cfm?id=81100443257&CFID=105752528&CFTOKEN=27713757">Holly Yanco</a>, 
                        <a href="author_page.cfm?id=81350585414&CFID=105752528&CFTOKEN=27713757">David Kontak</a>, 
                        <a href="author_page.cfm?id=81350583993&CFID=105752528&CFTOKEN=27713757">Linda Beliveau</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 105-112</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349837" title="DOI">10.1145/1349822.1349837</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349837&ftid=506295&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow15" style="display:inline;"><br /><div style="display:inline">Accessibility is a challenge for people with disabilities. Differences in cognitive ability, sensory impairments, motor dexterity, behavioral skills, and social skills must be taken into account when designing interfaces for assistive devices. Flexible ...</div></span>
          <span id="toHide15" style="display:none;"><br /><div style="display:inline"><p>Accessibility is a challenge for people with disabilities. Differences in cognitive ability, sensory impairments, motor dexterity, behavioral skills, and social skills must be taken into account when designing interfaces for assistive devices. Flexible interfaces tuned for individuals, instead of custom-built solutions, may benefit a larger number of people. The development and evaluation of a flexible interface for controlling a wheelchair mounted robotic arm is described in this paper. There are four versions of the interface based on input device (touch screen or joystick) and a moving or stationary shoulder camera. We describe results from an eight week experiment conducted with representative end users who range in physical and cognitive ability.</p></div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349838&CFID=105752528&CFTOKEN=27713757">Enjoyment intention to use and actual use of a conversational robot by elderly people</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350587500&CFID=105752528&CFTOKEN=27713757">Marcel Heerink</a>, 
                        <a href="author_page.cfm?id=81100527009&CFID=105752528&CFTOKEN=27713757">Ben Kr&#246;se</a>, 
                        <a href="author_page.cfm?id=81100478624&CFID=105752528&CFTOKEN=27713757">Bob Wielinga</a>, 
                        <a href="author_page.cfm?id=81100133286&CFID=105752528&CFTOKEN=27713757">Vanessa Evers</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 113-120</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349838" title="DOI">10.1145/1349822.1349838</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349838&ftid=506296&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow16" style="display:inline;"><br /><div style="display:inline">In this paper we explore the concept of enjoyment as a possible factor influencing acceptance of robotic technology by elderly people. We describe an experiment with a conversational robot and elderly users (n=30) that incorporates both a test session ...</div></span>
          <span id="toHide16" style="display:none;"><br /><div style="display:inline"><p>In this paper we explore the concept of enjoyment as a possible factor influencing acceptance of robotic technology by elderly people. We describe an experiment with a conversational robot and elderly users (n=30) that incorporates both a test session and a long term user observation. The experiment did confirm the hypothesis that perceived enjoyment has an effect on the intention to use a robotic system. Furthermore, findings show that the general assumption in technology acceptance models that intention to use predicts actual use is also applicable to this specific technology used by elderly people.</p></div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349839&CFID=105752528&CFTOKEN=27713757">Governing lethal behavior: embedding ethics in a hybrid deliberative/reactive robot architecture</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81409594784&CFID=105752528&CFTOKEN=27713757">Ronald C. Arkin</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 121-128</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349839" title="DOI">10.1145/1349822.1349839</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349839&ftid=506297&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow17" style="display:inline;"><br /><div style="display:inline">This paper provides the motivation and philosophy underlying the design of an ethical control and reasoning system potentially suitable for constraining lethal actions in an autonomous robotic system, so that its behavior will fall within the bounds ...</div></span>
          <span id="toHide17" style="display:none;"><br /><div style="display:inline"><p>This paper provides the motivation and philosophy underlying the design of an ethical control and reasoning system potentially suitable for constraining lethal actions in an autonomous robotic system, so that its behavior will fall within the bounds prescribed by the Laws of War and Rules of Engagement. This research, funded by the U.S. Army Research Office, is intended to ensure that robots do not behave illegally or unethically in the battlefield. Reasons are provided for the necessity of developing such a system at this time, as well as arguments for and against its creation.</p></div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349840&CFID=105752528&CFTOKEN=27713757">Housewives or technophiles?: understanding domestic robot owners</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81330498575&CFID=105752528&CFTOKEN=27713757">Ja-Young Sung</a>, 
                        <a href="author_page.cfm?id=81328488487&CFID=105752528&CFTOKEN=27713757">Rebecca E. Grinter</a>, 
                        <a href="author_page.cfm?id=81100285247&CFID=105752528&CFTOKEN=27713757">Henrik I. Christensen</a>, 
                        <a href="author_page.cfm?id=81350572821&CFID=105752528&CFTOKEN=27713757">Lan Guo</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 129-136</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349840" title="DOI">10.1145/1349822.1349840</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349840&ftid=506298&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow18" style="display:inline;"><br /><div style="display:inline">Despite the growing body of Human-Robot Interaction (HRI) research focused on domestic robots, surprisingly little is known about the demographic profile of robot owners and their influence on usage patterns. In this paper, we present the results of ...</div></span>
          <span id="toHide18" style="display:none;"><br /><div style="display:inline"><p>Despite the growing body of Human-Robot Interaction (HRI) research focused on domestic robots, surprisingly little is known about the demographic profile of robot owners and their influence on usage patterns. In this paper, we present the results of a survey of 379 iRobot's Roombao wners, that identified their demographic and usage trends. The outcome of the survey suggests that Roomba users are equally likely to be men or women, and they tend to be younger with high levels of education and technical backgrounds. Their adoption and use patterns illustrate the important role that gift exchange plays in adoption, and how the robot changes cleaning routines and creates non-cleaning activities. More generally, we argue that domestic robot adoption is growing, and suggest some of the factors that lead to a positive experience.</p></div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349841&CFID=105752528&CFTOKEN=27713757">How close?: model of proximity control for information-presenting robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310501032&CFID=105752528&CFTOKEN=27713757">Fumitaka Yamaoka</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752528&CFTOKEN=27713757">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752528&CFTOKEN=27713757">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752528&CFTOKEN=27713757">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 137-144</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349841" title="DOI">10.1145/1349822.1349841</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349841&ftid=506299&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow19" style="display:inline;"><br /><div style="display:inline">This paper describes a model for a robot to appropriately control its position when it presents information to a user. This capability is indispensable since in the future many robots will be functioning in daily situations as shopkeepers presenting ...</div></span>
          <span id="toHide19" style="display:none;"><br /><div style="display:inline"><p>This paper describes a model for a robot to appropriately control its position when it presents information to a user. This capability is indispensable since in the future many robots will be functioning in daily situations as shopkeepers presenting products to customers or museum guides presenting information to visitors. Psychology research suggests that people adjust their positions to establish a joint view toward a target object. Similarly, when a robot presents an object, it should stand at an appropriate position that considers the positions of both the listener and the object to optimize the listener's field of view and to establish a joint view. We observed human-human interaction situations where people presented objects and developed a model for an information-presenting robot to appropriately adjust its position. Our model consists of four constraints for establishing O-space: (1) proximity to listener, (2) proximity to object, (3) listener's field of view, and (4) presenter's field of view. We also present an experimental evaluation of the effectiveness of our model.</p></div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349842&CFID=105752528&CFTOKEN=27713757">How people anthropomorphize robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100583820&CFID=105752528&CFTOKEN=27713757">Susan R. Fussell</a>, 
                        <a href="author_page.cfm?id=81100476487&CFID=105752528&CFTOKEN=27713757">Sara Kiesler</a>, 
                        <a href="author_page.cfm?id=81100057970&CFID=105752528&CFTOKEN=27713757">Leslie D. Setlock</a>, 
                        <a href="author_page.cfm?id=81350580792&CFID=105752528&CFTOKEN=27713757">Victoria Yew</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 145-152</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349842" title="DOI">10.1145/1349822.1349842</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349842&ftid=506300&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow20" style="display:inline;"><br /><div style="display:inline">We explored anthropomorphism in people's reactions to a robot in social context vs. their more considered judgments of robots in the abstract. Participants saw a photo and read transcripts from a health interview by a robot or human interviewer. For ...</div></span>
          <span id="toHide20" style="display:none;"><br /><div style="display:inline"><p>We explored anthropomorphism in people's reactions to a robot in social context vs. their more considered judgments of robots in the abstract. Participants saw a photo and read transcripts from a health interview by a robot or human interviewer. For half of the participants, the interviewer was polite and for the other half, the interviewer was impolite. Participants then summarized the interactions in their own words and responded true or false to adjectives describing the interviewer. They later completed a post-task survey about whether a robot interviewer would possess moods, attitudes, and feelings. The results showed substantial anthropomorphism in participants' interview summaries and true-false responses, but minimal anthropomorphism in the abstract robot survey. Those who interacted with the robot interviewer tended to anthropomorphize more in the post-task survey, suggesting that as people interact more with robots, their abstract conceptions of them will become more anthropomorphic.</p></div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349843&CFID=105752528&CFTOKEN=27713757">How quickly should communication robots respond?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414602907&CFID=105752528&CFTOKEN=27713757">Toshiyuki Shiwa</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752528&CFTOKEN=27713757">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100282909&CFID=105752528&CFTOKEN=27713757">Michita Imai</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752528&CFTOKEN=27713757">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752528&CFTOKEN=27713757">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 153-160</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349843" title="DOI">10.1145/1349822.1349843</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349843&ftid=506301&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow21" style="display:inline;"><br /><div style="display:inline">This paper reports a study about system response time (SRT) in communication robots that utilize human-like social features, such as anthropomorphic appearance and conversation in natural language. Our research purpose established a design guideline ...</div></span>
          <span id="toHide21" style="display:none;"><br /><div style="display:inline"><p>This paper reports a study about system response time (SRT) in communication robots that utilize human-like social features, such as anthropomorphic appearance and conversation in natural language. Our research purpose established a design guideline for SRT in communication robots. The first experiment observed user preferences toward different SRTs in interaction with a robot. In other existing user interfaces, faster response is usually preferred. In contrast, our experimental result indicated that user preference for SRT in a communication robot is highest at one second, and user preference ratings level off at two seconds.</p> <p>However, a robot cannot always respond in such a short time as one or two seconds. Thus, the important question is "What should a robot do if it cannot respond quickly enough?" The second experiment tested the effectiveness of a conversational filler: behavior to notify listeners that the robot is going to respond. In Japanese "etto" is used to buy time to think and resembles "well..." and "uh..." In English. We used the same strategy in a communication robot to shadow system response time. Our results indicated that using a conversational filler by the robot moderated the user's impression toward a long SRT. </p></div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349844&CFID=105752528&CFTOKEN=27713757">How training and experience affect the benefits of autonomy in a dirty-bomb experiment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100579981&CFID=105752528&CFTOKEN=27713757">David J. Bruemmer</a>, 
                        <a href="author_page.cfm?id=81337492155&CFID=105752528&CFTOKEN=27713757">Curtis W. Nielsen</a>, 
                        <a href="author_page.cfm?id=81100618483&CFID=105752528&CFTOKEN=27713757">David I. Gertman</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 161-168</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349844" title="DOI">10.1145/1349822.1349844</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349844&ftid=506302&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow22" style="display:inline;"><br /><div style="display:inline">A dirty-bomb experiment conducted at the INL is used to evaluate the effectiveness and suitability of three different modes of robot control. The experiment uses three distinct user groups to understand how participants' background and training affect ...</div></span>
          <span id="toHide22" style="display:none;"><br /><div style="display:inline"><p>A dirty-bomb experiment conducted at the INL is used to evaluate the effectiveness and suitability of three different modes of robot control. The experiment uses three distinct user groups to understand how participants' background and training affect the way in which they use and benefit from autonomy. The results show that the target mode, which involves automated mapping and plume tracing together with a point and click tasking tool, provides the best performance for each group. This is true for objective performance such as source detection and localization accuracy as well as subjective measures such as perceived workload, frustration and preference. The best overall performance is achieved by the Explosive Ordinance Disposal group which has experience in both robot teleoperation and dirty bomb response. The user group that benefits least from autonomy is the Nuclear Engineers that have no experience with either robot operation or dirty bomb response. The group that benefits most from autonomy is the Weapons of Mass Destruction Civil Support Team that has extensive experience related to the task, but no robot training.</p></div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349845&CFID=105752528&CFTOKEN=27713757">Human emotion and the uncanny valley: a GLM, MDS, and Isomap analysis of robot video ratings</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81375608981&CFID=105752528&CFTOKEN=27713757">Chin-Chang Ho</a>, 
                        <a href="author_page.cfm?id=81100632490&CFID=105752528&CFTOKEN=27713757">Karl F. MacDorman</a>, 
                        <a href="author_page.cfm?id=81350584590&CFID=105752528&CFTOKEN=27713757">Z. A. D. Dwi Pramono</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 169-176</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349845" title="DOI">10.1145/1349822.1349845</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349845&ftid=506303&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow23" style="display:inline;"><br /><div style="display:inline">The eerie feeling attributed to human-looking robots and animated characters may be a key factor in our perceptual and cognitive discrimination of the human and humanlike. This study applies regression, the generalized linear model (GLM), factor analysis, ...</div></span>
          <span id="toHide23" style="display:none;"><br /><div style="display:inline"><p>The eerie feeling attributed to human-looking robots and animated characters may be a key factor in our perceptual and cognitive discrimination of the human and humanlike. This study applies regression, the generalized linear model (GLM), factor analysis, multidimensional scaling (MDS), and kernel isometric mapping (Isomap) to analyze ratings of 27 emotions of 18 moving figures whose appearance varies along a human likeness continuum. The results indicate (1) Attributions of eerie and creepy better capture our visceral reaction to an uncanny robot than strange. (2) Eerie and creepy are mainly associated with fear but also shocked, disgusted, and nervous. Strange is less strongly associated with emotion. (3) Thus, strange may be more cognitive, while eerie and creepy are more perceptual/emotional. (4) Human features increase ratings of human likeness. (5) Women are slightly more sensitive to eerie and creepy than men; and older people may be more willing to attribute human likeness to a robot despite its eeriness.</p></div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349846&CFID=105752528&CFTOKEN=27713757">Human to robot demonstrations of routine home tasks: exploring the role of the robot's feedback</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81331500793&CFID=105752528&CFTOKEN=27713757">Nuno Otero</a>, 
                        <a href="author_page.cfm?id=81310502395&CFID=105752528&CFTOKEN=27713757">Aris Alissandrakis</a>, 
                        <a href="author_page.cfm?id=81100447769&CFID=105752528&CFTOKEN=27713757">Kerstin Dautenhahn</a>, 
                        <a href="author_page.cfm?id=81100341427&CFID=105752528&CFTOKEN=27713757">Chrystopher Nehaniv</a>, 
                        <a href="author_page.cfm?id=81350567691&CFID=105752528&CFTOKEN=27713757">Dag Sverre Syrdal</a>, 
                        <a href="author_page.cfm?id=81350568813&CFID=105752528&CFTOKEN=27713757">Kheng Lee Koay</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 177-184</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349846" title="DOI">10.1145/1349822.1349846</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349846&ftid=506304&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow24" style="display:inline;"><br /><div style="display:inline">In this paper, we explore some conceptual issues, relevant for the design of robotic systems aimed at interacting with humans in domestic environments. More specifically, we study the role of the robot's feedback (positive or negative acknowledgment ...</div></span>
          <span id="toHide24" style="display:none;"><br /><div style="display:inline"><p>In this paper, we explore some conceptual issues, relevant for the design of robotic systems aimed at interacting with humans in domestic environments. More specifically, we study the role of the robot's feedback (positive or negative acknowledgment of understanding) on a human teacher's demonstration of a routine home task (laying a table). Both the human and the system's perspectives are considered in the analysis and discussion of results from a human-robot user study, highlighting some important conceptual and practical issues. These include the lack of explicitness and consistency on people's demonstration strategies. Furthermore, we discuss the need to investigate design strategies to elicit people's knowledge about the task and also successfully advertize the robot's abilities in order to promote people's ability to provide appropriate demonstrations.</p></div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349847&CFID=105752528&CFTOKEN=27713757">A hybrid algorithm for tracking and following people using a robotic dog</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350580541&CFID=105752528&CFTOKEN=27713757">Martijn Liem</a>, 
                        <a href="author_page.cfm?id=81328490909&CFID=105752528&CFTOKEN=27713757">Arnoud Visser</a>, 
                        <a href="author_page.cfm?id=81100129884&CFID=105752528&CFTOKEN=27713757">Frans Groen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 185-192</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349847" title="DOI">10.1145/1349822.1349847</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349847&ftid=506305&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow25" style="display:inline;"><br /><div style="display:inline">The capability to follow a person in a domestic environment is an important prerequisite for a robot companion. In this paper, a tracking algorithm is presented that makes it possible to follow a person using a small robot. This algorithm can track a ...</div></span>
          <span id="toHide25" style="display:none;"><br /><div style="display:inline"><p>The capability to follow a person in a domestic environment is an important prerequisite for a robot companion. In this paper, a tracking algorithm is presented that makes it possible to follow a person using a small robot. This algorithm can track a person while moving around, regardless of the sometimes erratic movements of the legged robot. Robust performance is obtained by fusion of two algorithms, one based on salient features and one on color histograms. Re-initializing object histograms enables the system to track a person even when the illumination in the environment changes. By being able to re-initialize the system on run time using background subtraction, the system gains an extra level of robustness.</p></div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349848&CFID=105752528&CFTOKEN=27713757">Hybrid tracking of human operators using IMU/UWB data fusion by a Kalman filter</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100467727&CFID=105752528&CFTOKEN=27713757">J. A. Corrales</a>, 
                        <a href="author_page.cfm?id=81100419857&CFID=105752528&CFTOKEN=27713757">F. A. Candelas</a>, 
                        <a href="author_page.cfm?id=81100217013&CFID=105752528&CFTOKEN=27713757">F. Torres</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 193-200</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349848" title="DOI">10.1145/1349822.1349848</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349848&ftid=506306&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow26" style="display:inline;"><br /><div style="display:inline">The precise localization of human operators in robotic workplaces is an important requirement to be satisfied in order to develop human-robot interaction tasks. Human tracking provides not only safety for human operators, but also context information ...</div></span>
          <span id="toHide26" style="display:none;"><br /><div style="display:inline"><p>The precise localization of human operators in robotic workplaces is an important requirement to be satisfied in order to develop human-robot interaction tasks. Human tracking provides not only safety for human operators, but also context information for intelligent human-robot collaboration. This paper evaluates an inertial motion capture system which registers full-body movements of an user in a robotic manipulator workplace. However, the presence of errors in the global translational measurements returned by this system has led to the need of using another localization system, based on Ultra-WideBand (UWB) technology. A Kalman filter fusion algorithm which combines the measurements of these systems is developed. This algorithm unifies the advantages of both technologies: high data rates from the motion capture system and global translational precision from the UWB localization system. The developed hybrid system not only tracks the movements of all limbs of the user as previous motion capture systems, but is also able to position precisely the user in the environment.</p></div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349849&CFID=105752528&CFTOKEN=27713757">Integrating vision and audition within a cognitive architecture to track conversations</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100061774&CFID=105752528&CFTOKEN=27713757">J. Gregory Trafton</a>, 
                        <a href="author_page.cfm?id=81350575902&CFID=105752528&CFTOKEN=27713757">Magda D. Bugajska</a>, 
                        <a href="author_page.cfm?id=81325488400&CFID=105752528&CFTOKEN=27713757">Benjamin R. Fransen</a>, 
                        <a href="author_page.cfm?id=81350583314&CFID=105752528&CFTOKEN=27713757">Raj M. Ratwani</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 201-208</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349849" title="DOI">10.1145/1349822.1349849</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349849&ftid=506307&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow27" style="display:inline;"><br /><div style="display:inline">We describe a computational cognitive architecture for robots which we call ACT-R/E (ACT-R/Embodied). ACT-R/E is based on ACT-R [1, 2] but uses different visual, auditory, and movement modules. We describe a model that uses ACT-R/E to integrate visual ...</div></span>
          <span id="toHide27" style="display:none;"><br /><div style="display:inline"><p>We describe a computational cognitive architecture for robots which we call ACT-R/E (ACT-R/Embodied). ACT-R/E is based on ACT-R [1, 2] but uses different visual, auditory, and movement modules. We describe a model that uses ACT-R/E to integrate visual and auditory information to perform conversation tracking in a dynamic environment. We also performed an empirical evaluation study which shows that people see our conversational tracking system as extremely natural.</p></div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349850&CFID=105752528&CFTOKEN=27713757">Learning polite behavior with situation models</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350585319&CFID=105752528&CFTOKEN=27713757">R&#233;mi Barraquand</a>, 
                        <a href="author_page.cfm?id=81100490913&CFID=105752528&CFTOKEN=27713757">James L. Crowley</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 209-216</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349850" title="DOI">10.1145/1349822.1349850</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349850&ftid=506308&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow28" style="display:inline;"><br /><div style="display:inline">In this paper, we describe experiments with methods for learning the appropriateness of behaviors based on a model of the current social situation. We first review different approaches for social robotics, and present a new approach based on situation ...</div></span>
          <span id="toHide28" style="display:none;"><br /><div style="display:inline"><p>In this paper, we describe experiments with methods for learning the appropriateness of behaviors based on a model of the current social situation. We first review different approaches for social robotics, and present a new approach based on situation modeling. We then review algorithms for social learning and propose three modifications to the classical Q-Learning algorithm. We describe five experiments with progressively complex algorithms for learning the appropriateness of behaviors. The first three experiments illustrate how social factors can be used to improve learning by controlling learning rate. In the fourth experiment we demonstrate that proper credit assignment improves the effectiveness of reinforcement learning for social interaction. In our fifth experiment we show that analogy can be used to accelerate learning rates in contexts composed of many situations.</p></div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349851&CFID=105752528&CFTOKEN=27713757">Loudness measurement of human utterance to a robot in noisy environment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100481894&CFID=105752528&CFTOKEN=27713757">Satoshi Kagami</a>, 
                        <a href="author_page.cfm?id=81325490024&CFID=105752528&CFTOKEN=27713757">Yoko Sasaki</a>, 
                        <a href="author_page.cfm?id=81314485902&CFID=105752528&CFTOKEN=27713757">Simon Thompson</a>, 
                        <a href="author_page.cfm?id=81350584177&CFID=105752528&CFTOKEN=27713757">Tomoaki Fujihara</a>, 
                        <a href="author_page.cfm?id=81325487980&CFID=105752528&CFTOKEN=27713757">Tadashi Enomoto</a>, 
                        <a href="author_page.cfm?id=81100110304&CFID=105752528&CFTOKEN=27713757">Hiroshi Mizoguchi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 217-224</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349851" title="DOI">10.1145/1349822.1349851</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349851&ftid=506309&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow29" style="display:inline;"><br /><div style="display:inline">In order to understand utterance based human-robot interation, and to develop such a system, this paper initially analyzes how loud humans speak in a noisy environment. Experiments were conducted to measure how loud humans speak with 1) different noise ...</div></span>
          <span id="toHide29" style="display:none;"><br /><div style="display:inline"><p>In order to understand utterance based human-robot interation, and to develop such a system, this paper initially analyzes how loud humans speak in a noisy environment. Experiments were conducted to measure</p> <p>how loud humans speak with 1) different noise levels, 2) different number of sound sources, 3) different sound sources, and 4) different distances to a robot. Synchronized sound sources add noise to the auditory scene, and resultant utterances are recorded and compared to a previously recorded noiseless utterance. From experiments, we understand that humans generate basically the same level of sound pressure level at his/her location irrespective of distance and background noise. More precisely, there is a band according to a distance, and also according to sound sources that is including</p> <p>language pronounce.</p> <p>According to this understanding, we developed an online spoken command recognition system for a mobile robot. System consists of two key componenets: 1) Low side-lobe microphone array that works as omini-directional telescopic microphone, and 2) DSBF combined with FBS</p> <p>method for sound source localization and segmentation. Caller location and segmented sound stream are calculated, and then the segmented sound stream is sent to voice recognition system. The system works with at most five sound sources at the same time with about at most</p> <p>18[dB] sound pressure differences. Experimental results with the modile robot are also shown.</p></div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349852&CFID=105752528&CFTOKEN=27713757">Multi-thresholded approach to demonstration selection for interactive robot learning</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81333488007&CFID=105752528&CFTOKEN=27713757">Sonia Chernova</a>, 
                        <a href="author_page.cfm?id=81100032034&CFID=105752528&CFTOKEN=27713757">Manuela Veloso</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 225-232</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349852" title="DOI">10.1145/1349822.1349852</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349852&ftid=506310&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow30" style="display:inline;"><br /><div style="display:inline">Effective learning from demonstration techniques enable complex robot behaviors to be taught from a small number of demonstrations. A number of recent works have explored interactive approaches to demonstration, in which both the robot and the teacher ...</div></span>
          <span id="toHide30" style="display:none;"><br /><div style="display:inline"><p>Effective learning from demonstration techniques enable complex robot behaviors to be taught from a small number of demonstrations. A number of recent works have explored interactive approaches to demonstration, in which both the robot and the teacher are able to select training examples. In this paper, we focus on a demonstration selection algorithm used by the robot to identify informative states for demonstration. Existing automated approaches for demonstration selection typically rely on a single threshold value, which is applied to a measure of action confidence. We highlight the limitations of using a single fixed threshold for a specific subset of algorithms, and contribute a method for automatically setting multiple confidence thresholds designed to target domain states with the greatest uncertainty. We present a comparison of our multi-threshold selection method to confidence-based selection using a single fixed threshold, and to manual data selection by a human teacher. Our results indicate that the automated multi-threshold approach significantly reduces the number of demonstrations required to learn the task.</p></div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349853&CFID=105752528&CFTOKEN=27713757">Object schemas for responsive robotic language use</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310503073&CFID=105752528&CFTOKEN=27713757">Kai-yuh Hsiao</a>, 
                        <a href="author_page.cfm?id=81413591729&CFID=105752528&CFTOKEN=27713757">Soroush Vosoughi</a>, 
                        <a href="author_page.cfm?id=81100089105&CFID=105752528&CFTOKEN=27713757">Stefanie Tellex</a>, 
                        <a href="author_page.cfm?id=81350570131&CFID=105752528&CFTOKEN=27713757">Rony Kubat</a>, 
                        <a href="author_page.cfm?id=81100653347&CFID=105752528&CFTOKEN=27713757">Deb Roy</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 233-240</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349853" title="DOI">10.1145/1349822.1349853</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349853&ftid=506311&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow31" style="display:inline;"><br /><div style="display:inline">The use of natural language should be added to a robot system without sacrificing responsiveness to the environment. In this paper, we present a robot that manipulates objects on a tabletop in response to verbal interaction. Reactivity is maintained ...</div></span>
          <span id="toHide31" style="display:none;"><br /><div style="display:inline"><p>The use of natural language should be added to a robot system without sacrificing responsiveness to the environment. In this paper, we present a robot that manipulates objects on a tabletop in response to verbal interaction. Reactivity is maintained by using concurrent interaction processes, such as visual trackers and collision detection processes. The interaction processes and their associated data are organized into object schemas, each representing a physical object in the environment, based on the target of each process. The object schemas then serve as discrete structures of coordination between reactivity, planning, and language use, permitting rapid integration of information from multiple sources.</p></div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349854&CFID=105752528&CFTOKEN=27713757">A point-and-click interface for the real world: laser designation of objects for mobile manipulation</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100204143&CFID=105752528&CFTOKEN=27713757">Charles C. Kemp</a>, 
                        <a href="author_page.cfm?id=81350576495&CFID=105752528&CFTOKEN=27713757">Cressel D. Anderson</a>, 
                        <a href="author_page.cfm?id=81350575528&CFID=105752528&CFTOKEN=27713757">Hai Nguyen</a>, 
                        <a href="author_page.cfm?id=81350584333&CFID=105752528&CFTOKEN=27713757">Alexander J. Trevor</a>, 
                        <a href="author_page.cfm?id=81350568156&CFID=105752528&CFTOKEN=27713757">Zhe Xu</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 241-248</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349854" title="DOI">10.1145/1349822.1349854</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349854&ftid=506312&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow32" style="display:inline;"><br /><div style="display:inline">We present a novel interface for human-robot interaction that enables a human to intuitively and unambiguously select a 3D location in the world and communicate it to a mobile robot. The human points at a location of interest and illuminates it (``clicks ...</div></span>
          <span id="toHide32" style="display:none;"><br /><div style="display:inline"><p>We present a novel interface for human-robot interaction that enables a human to intuitively and unambiguously select a 3D location in the world and communicate it to a mobile robot. The human points at a location of interest and illuminates it (``clicks it'') with an unaltered, off-the-shelf, green laser pointer. The robot detects the resulting laser spot with an omnidirectional, catadioptric camera with a narrow-band green filter. After detection, the robot moves its stereo pan/tilt camera to look at this location and estimates the location's 3D position with respect to the robot's frame of reference.</p> <p>Unlike previous approaches, this interface for gesture-based pointing requires no instrumentation of the environment, makes use of a non-instrumented everyday pointing device, has low spatial error out to 3 meters, is fully mobile, and is robust enough for use in real-world applications.</p> <p>We demonstrate that this human-robot interface enables a person to designate a wide variety of everyday objects placed throughout a room. In 99.4% of these tests, the robot successfully looked at the designated object and estimated its 3D position with low average error. We also show that this interface can support object acquisition by a mobile manipulator. For this application, the user selects an object to be picked up from the floor by ``clicking'' on it with the laser pointer interface. In 90% of these trials, the robot successfully moved to the designated object and picked it up off of the floor.</p></div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349855&CFID=105752528&CFTOKEN=27713757">Reasoning for a multi-modal service robot considering uncertainty in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350583857&CFID=105752528&CFTOKEN=27713757">Sven R. Schmidt-Rohr</a>, 
                        <a href="author_page.cfm?id=81350583820&CFID=105752528&CFTOKEN=27713757">Steffen Knoop</a>, 
                        <a href="author_page.cfm?id=81350580406&CFID=105752528&CFTOKEN=27713757">Martin L&#246;sch</a>, 
                        <a href="author_page.cfm?id=81100225179&CFID=105752528&CFTOKEN=27713757">R&#252;diger Dillmann</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 249-254</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349855" title="DOI">10.1145/1349822.1349855</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349855&ftid=506313&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow33" style="display:inline;"><br /><div style="display:inline">This paper presents a reasoning system for a multi-modal service robot with human-robot interaction. The reasoning system uses partially observable Markov decision processes (POMDPs) for decision making and an intermediate level for bridging the gap ...</div></span>
          <span id="toHide33" style="display:none;"><br /><div style="display:inline"><p>This paper presents a reasoning system for a multi-modal service robot with human-robot interaction. The reasoning system uses partially observable Markov decision processes (POMDPs) for decision making and an intermediate level for bridging the gap of abstraction between multi-modal real world sensors and actuators on the one hand and POMDP reasoning on the other. A filter system handles the abstraction of multi-modal perception while preserving uncertainty and model-soundness. A command sequencer is utilized to control the execution of symbolic POMDP decisions on multiple actuator components. By using POMDP reasoning, the robot is able to deal with uncertainty in both observation and prediction of human behavior and can balance risk and opportunity. The system has been implemented on a multi-modal service robot and is able to let the robot act autonomously in modeled human-robot interaction scenarios. Experiments evaluate the characteristics of the proposed algorithms and architecture.</p></div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349856&CFID=105752528&CFTOKEN=27713757">Relational vs. group self-construal: untangling the role of national culture in HRI</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100133286&CFID=105752528&CFTOKEN=27713757">Vanessa Evers</a>, 
                        <a href="author_page.cfm?id=81100169027&CFID=105752528&CFTOKEN=27713757">Heidy C. Maldonado</a>, 
                        <a href="author_page.cfm?id=81350584523&CFID=105752528&CFTOKEN=27713757">Talia L. Brodecki</a>, 
                        <a href="author_page.cfm?id=81100572021&CFID=105752528&CFTOKEN=27713757">Pamela J. Hinds</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 255-262</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349856" title="DOI">10.1145/1349822.1349856</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349856&ftid=506314&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow34" style="display:inline;"><br /><div style="display:inline">As robots (and other technologies) increasingly make decisions on behalf of people, it is important to understand how people from diverse cultures respond to this capability. Thus far, much design of autonomous systems takes a Western view valuing individual ...</div></span>
          <span id="toHide34" style="display:none;"><br /><div style="display:inline"><p>As robots (and other technologies) increasingly make decisions on behalf of people, it is important to understand how people from diverse cultures respond to this capability. Thus far, much design of autonomous systems takes a Western view valuing individual preferences and choice. We challenge the assumption that Western values are universally optimal for robots. In this study, we sought to clarify the effects of users' cultural background on human-robot collaboration by investigating their attitudes toward and the extent to which people accepted choices made by a robot or human assistant. A 2x2x2 experiment was conducted with nationality (US vs. Chinese), in group strength (weak vs. strong) and human vs. robot assistant as dimensions. US participants reported higher trust of and compliance with the assistants (human and robot) although when the assistant was characterized as a strong ingroup member, Chinese as compared with the US subjects were more comfortable. Chinese also reported a stronger sense of control with both assistants and were more likely to anthropomorphize the robot than were US subjects. This pattern of findings confirms that people from different national cultures may respond differently to robots, but also suggests that predictions from human-human interaction do not hold universally.</p></div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349857&CFID=105752528&CFTOKEN=27713757">Robot social presence and gender: do females view robots differently than males?</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100573922&CFID=105752528&CFTOKEN=27713757">Paul Schermerhorn</a>, 
                        <a href="author_page.cfm?id=81100029514&CFID=105752528&CFTOKEN=27713757">Matthias Scheutz</a>, 
                        <a href="author_page.cfm?id=81100495266&CFID=105752528&CFTOKEN=27713757">Charles R. Crowell</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 263-270</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349857" title="DOI">10.1145/1349822.1349857</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349857&ftid=506315&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow35" style="display:inline;"><br /><div style="display:inline">Social-psychological processes in humans will play an important role in long-term human-robot interactions. This study investigates people's perceptions of social presence in robots during (relatively) short interactions. Findings indicate that males ...</div></span>
          <span id="toHide35" style="display:none;"><br /><div style="display:inline"><p>Social-psychological processes in humans will play an important role in long-term human-robot interactions. This study investigates people's perceptions of social presence in robots during (relatively) short interactions. Findings indicate that males tend to think of the robot as more human-like and accordingly show some evidence of "social facilitation" on an arithmetic task as well as more socially desirable responding on a survey administered by a robot. In contrast, females saw the robot as more machine-like, exhibited less socially desirable responding to the robot's survey, and were not socially facilitated by the robot while engaged in the arithmetic tasks. Various alternative accounts of these findings are explored and the implications of these results for future work are discussed.</p></div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349858&CFID=105752528&CFTOKEN=27713757">Robotic animals might aid in the social development of children with autism</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81408599514&CFID=105752528&CFTOKEN=27713757">Cady M. Stanton</a>, 
                        <a href="author_page.cfm?id=81408592441&CFID=105752528&CFTOKEN=27713757">Peter H. Kahn Jr.</a>, 
                        <a href="author_page.cfm?id=81350575852&CFID=105752528&CFTOKEN=27713757">Rachel L. Severson</a>, 
                        <a href="author_page.cfm?id=81350569693&CFID=105752528&CFTOKEN=27713757">Jolina H. Ruckert</a>, 
                        <a href="author_page.cfm?id=81100631042&CFID=105752528&CFTOKEN=27713757">Brian T. Gill</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 271-278</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349858" title="DOI">10.1145/1349822.1349858</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349858&ftid=506316&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow36" style="display:inline;"><br /><div style="display:inline">This study investigated whether a robotic dog might aid in the social development of children with autism. Eleven children diagnosed with autism (ages 5-8) interacted with the robotic dog AIBO and, during a different period within the same experimental ...</div></span>
          <span id="toHide36" style="display:none;"><br /><div style="display:inline"><p>This study investigated whether a robotic dog might aid in the social development of children with autism. Eleven children diagnosed with autism (ages 5-8) interacted with the robotic dog AIBO and, during a different period within the same experimental session, a simple mechanical toy dog (Kasha), which had no ability to detect or respond to its physical or social environment. Results showed that, in comparison to Kasha, the children spoke more words to AIBO, and more often engaged in three types of behavior with AIBO typical of children without autism: verbal engagement, reciprocal interaction, and authentic interaction. In addition, we found suggestive evidence (with p values ranging from .07 to .09) that the children interacted more with AIBO, and, while in the AIBO session, engaged in fewer autistic behaviors. Discussion focuses on why robotic animals might benefit children with autism.</p></div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349859&CFID=105752528&CFTOKEN=27713757">Robotics operator performance in a military multi-tasking environment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414613395&CFID=105752528&CFTOKEN=27713757">Jessie Y.C. Chen</a>, 
                        <a href="author_page.cfm?id=81100476496&CFID=105752528&CFTOKEN=27713757">Michael J Barnes</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 279-286</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349859" title="DOI">10.1145/1349822.1349859</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349859&ftid=506317&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow37" style="display:inline;"><br /><div style="display:inline">We simulated a military mounted environment and examined the performance of the combined position of gunner and robotics operator and how aided target recognition (AiTR) capabilities (delivered either through tactile or tactile + visual cueing) for the ...</div></span>
          <span id="toHide37" style="display:none;"><br /><div style="display:inline"><p>We simulated a military mounted environment and examined the performance of the combined position of gunner and robotics operator and how aided target recognition (AiTR) capabilities (delivered either through tactile or tactile + visual cueing) for the gunnery task might benefit the concurrent robotics and communication tasks. Results showed that participants' teleoperation task improved significantly when the AiTR was available to assist them with their gunnery task. However, the same improvement was not found for semi-autonomous robotics task performance. Additionally, when teleoperating, those participants with higher spatial ability outperformed those with lower spatial ability. However, performance gap between those with higher and lower spatial ability appeared to be narrower when the AiTR was available to assist the gunnery task. Participants' communication task performance also improved significantly when the gunnery task was aided by AiTR. Finally, participant's perceived workload was significantly higher when they teleoperated a robotic asset and when their gunnery task was unassisted.</p></div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349860&CFID=105752528&CFTOKEN=27713757">Robots in organizations: the role of workflow, social, and environmental factors in human-robot interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310500023&CFID=105752528&CFTOKEN=27713757">Bilge Mutlu</a>, 
                        <a href="author_page.cfm?id=81100492013&CFID=105752528&CFTOKEN=27713757">Jodi Forlizzi</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 287-294</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349860" title="DOI">10.1145/1349822.1349860</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349860&ftid=506318&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow38" style="display:inline;"><br /><div style="display:inline">Robots are becoming increasingly integrated into the workplace, impacting organizational structures and processes, and affecting products and services created by these organizations. While robots promise significant benefits to organizations, their introduction ...</div></span>
          <span id="toHide38" style="display:none;"><br /><div style="display:inline"><p>Robots are becoming increasingly integrated into the workplace, impacting organizational structures and processes, and affecting products and services created by these organizations. While robots promise significant benefits to organizations, their introduction poses a variety of design challenges. In this paper, we use ethnographic data collected at a hospital using an autonomous delivery robot to examine how organizational factors affect the way its members respond to robots and the changes engendered by their use. Our analysis uncovered dramatic differences between the medical and post-partum units in how people integrated the robot into their workflow and their perceptions of and interactions with it. Different patient profiles in these units led to differences in workflow, goals, social dynamics, and the use of the physical environment. In medical units, low tolerance for interruptions, a discrepancy between the perceived cost and benefits of using the robot, and breakdowns due to high traffic and clutter in the robot's path caused the robot to have a negative impact on the workflow and staff resistance. On the contrary, post-partum units integrated the robot into their workflow and social context. Based on our findings, we provide design guidelines for the development of robots for organizations.</p></div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349861&CFID=105752528&CFTOKEN=27713757">The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81320489825&CFID=105752528&CFTOKEN=27713757">Mary Ellen Foster</a>, 
                        <a href="author_page.cfm?id=81100118435&CFID=105752528&CFTOKEN=27713757">Ellen Gurman Bard</a>, 
                        <a href="author_page.cfm?id=81310484827&CFID=105752528&CFTOKEN=27713757">Markus Guhe</a>, 
                        <a href="author_page.cfm?id=81328488415&CFID=105752528&CFTOKEN=27713757">Robin L. Hill</a>, 
                        <a href="author_page.cfm?id=81100267045&CFID=105752528&CFTOKEN=27713757">Jon Oberlander</a>, 
                        <a href="author_page.cfm?id=81100609341&CFID=105752528&CFTOKEN=27713757">Alois Knoll</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 295-302</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349861" title="DOI">10.1145/1349822.1349861</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349861&ftid=506319&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow39" style="display:inline;"><br /><div style="display:inline">Generating referring expressions is a task that has received a great deal of attention in the natural-language generation community, with an increasing amount of recent effort targeted at the generation of multimodal referring expressions. However, most ...</div></span>
          <span id="toHide39" style="display:none;"><br /><div style="display:inline"><p>Generating referring expressions is a task that has received a great deal of attention in the natural-language generation community, with an increasing amount of recent effort targeted at the generation of multimodal referring expressions. However, most implemented systems tend to assume very little shared knowledge between the speaker and the hearer, and therefore must generate fully-elaborated linguistic references. Some systems do include a representation of the physical context or the dialogue context; however, other sources of contextual information are not normally used. Also, the generated references normally consist only of language and, possibly, deictic pointing gestures.</p> <p>When referring to objects in the context of a task-based interaction involving jointly manipulating objects, a much richer notion of context is available, which permits a wider range of referring options. In particular, when conversational partners cooperate on a mutual task in a shared environment, objects can be made accessible simply by manipulating them as part of the task. We demonstrate that such expressions are common in a corpus of human-human dialogues based on constructing virtual objects, and then describe how this type of reference can be incorporated into the output of a humanoid robot that engages in similar joint construction dialogues with a human partner.</p></div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349862&CFID=105752528&CFTOKEN=27713757">A semi-autonomous communication robot: a field trial at a train station</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81310499910&CFID=105752528&CFTOKEN=27713757">Masahiro Shiomi</a>, 
                        <a href="author_page.cfm?id=81329491765&CFID=105752528&CFTOKEN=27713757">Daisuke Sakamoto</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752528&CFTOKEN=27713757">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81350582479&CFID=105752528&CFTOKEN=27713757">Carlos Toshinori Ishi</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752528&CFTOKEN=27713757">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752528&CFTOKEN=27713757">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 303-310</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349862" title="DOI">10.1145/1349822.1349862</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349862&ftid=506320&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow40" style="display:inline;"><br /><div style="display:inline">This paper reports an initial field trial with a prototype of a semiautonomous communication robot at a train station. We developed an operator-requesting mechanism to achieve semiautonomous operation for a communication robot functioning in real environments. ...</div></span>
          <span id="toHide40" style="display:none;"><br /><div style="display:inline"><p>This paper reports an initial field trial with a prototype of a semiautonomous communication robot at a train station. We developed an operator-requesting mechanism to achieve semiautonomous operation for a communication robot functioning in real environments. The operator-requesting mechanism autonomously detects situations that the robot cannot handle by itself; a human operator helps by assuming control of the robot.</p> <p>This approach gives semi-autonomous robots the ability to function naturally with minimum human effort. Our system consists of a humanoid robot and ubiquitous sensors. The robot has such basic communicative behaviors as greeting and route guidance. The experimental results revealed that the operator-requesting mechanism correctly requested operator's help in 85% of the necessary situations; the operator only had to control 25% of the experiment time in the semi-autonomous mode with a robot system that successfully guided 68% of the passengers. At the same time, this trial provided the opportunity to gather user data for the further development of natural behaviors for such robots operating in real environments.</p></div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349863&CFID=105752528&CFTOKEN=27713757">Simultaneous teleoperation of multiple social robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350582585&CFID=105752528&CFTOKEN=27713757">Dylan F. Glas</a>, 
                        <a href="author_page.cfm?id=81311482839&CFID=105752528&CFTOKEN=27713757">Takayuki Kanda</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752528&CFTOKEN=27713757">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100399958&CFID=105752528&CFTOKEN=27713757">Norihiro Hagita</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 311-318</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349863" title="DOI">10.1145/1349822.1349863</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349863&ftid=506321&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow41" style="display:inline;"><br /><div style="display:inline">Teleoperation of multiple robots has been studied extensively for applications such as robot navigation; however, this concept has never been applied to the field of social robots. To explore the unique challenges posed by the remote operation of multiple ...</div></span>
          <span id="toHide41" style="display:none;"><br /><div style="display:inline"><p>Teleoperation of multiple robots has been studied extensively for applications such as robot navigation; however, this concept has never been applied to the field of social robots. To explore the unique challenges posed by the remote operation of multiple social robots, we have implemented a system in which a single operator simultaneously controls up to four robots, all engaging in communication interactions with users. We present a user inter-face designed for operating a single robot while monitoring several others in the background, then we propose methods for characterizing task difficulty and introduce a technique for improving multiple-robot performance by reducing the number of conflicts between robots demanding the operator's attention. Finally, we demonstrate the success of our system in laboratory trials based on real-world interactions.</p></div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349864&CFID=105752528&CFTOKEN=27713757">Spiral response-cascade hypothesis: intrapersonal responding-cascade in gaze interaction</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350568010&CFID=105752528&CFTOKEN=27713757">Yuichiro Yoshikawa</a>, 
                        <a href="author_page.cfm?id=81350581400&CFID=105752528&CFTOKEN=27713757">Shunsuke Yamamoto</a>, 
                        <a href="author_page.cfm?id=81350585463&CFID=105752528&CFTOKEN=27713757">Hidenobu Sumioka</a>, 
                        <a href="author_page.cfm?id=81100572813&CFID=105752528&CFTOKEN=27713757">Hiroshi Ishiguro</a>, 
                        <a href="author_page.cfm?id=81100515894&CFID=105752528&CFTOKEN=27713757">Minoru Asada</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 319-326</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349864" title="DOI">10.1145/1349822.1349864</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349864&ftid=506322&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow42" style="display:inline;"><br /><div style="display:inline">A spiral response-cascade hypothesis is proposed to model the mechanism that enables human communication to emerge or be maintained among agents. In this hypothesis, we propose the existence of three cascades each of which indicates intrapersonal or ...</div></span>
          <span id="toHide42" style="display:none;"><br /><div style="display:inline"><p>A spiral response-cascade hypothesis is proposed to model the mechanism that enables human communication to emerge or be maintained among agents. In this hypothesis, we propose the existence of three cascades each of which indicates intrapersonal or interpersonal mutual facilitation in the formation of someone's feelings about one's communication partners and the exhibition of behaviors in communicating with them, i.e., responding. In this paper, we discuss our examination of an important part of the hypothesis, i.e., what we call an intrapersonal responding cascade, through an experiment where the gaze interactions between a participant and a communication robot were controlled not only by controlling the robot's gaze but also by signaling participants when to shift their gaze. We report that the participants' experiences in responding to the robot enable them to regard the robot as a communicative being, which partially supports the hypothesis of the intrapersonal responding cascade.</p></div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349865&CFID=105752528&CFTOKEN=27713757">Supervision and motion planning for a mobile manipulator interacting with humans</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81309507877&CFID=105752528&CFTOKEN=27713757">Emrah Akin Sisbot</a>, 
                        <a href="author_page.cfm?id=81309481490&CFID=105752528&CFTOKEN=27713757">Aurelie Clodic</a>, 
                        <a href="author_page.cfm?id=81100454878&CFID=105752528&CFTOKEN=27713757">Rachid Alami</a>, 
                        <a href="author_page.cfm?id=81350584924&CFID=105752528&CFTOKEN=27713757">Maxime Ransan</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 327-334</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349865" title="DOI">10.1145/1349822.1349865</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349865&ftid=506323&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow43" style="display:inline;"><br /><div style="display:inline">Human Robot collaborative task achievement requires adapted tools and algorithms for both decision making and motion computation. The human presence as well as its behavior must be considered and actively monitored at the decisional level for the robot ...</div></span>
          <span id="toHide43" style="display:none;"><br /><div style="display:inline"><p>Human Robot collaborative task achievement requires adapted tools and algorithms for both decision making and motion computation. The human presence as well as its behavior must be considered and actively monitored at the decisional level for the robot to produce synchronized and adapted behavior. Additionally, having a human within the robot range of action introduces security constraints as well as comfort considerations which must be taken into account at the motion planning and control level. This paper presents a robotic architecture adapted to human robot interaction and focuses on two tools: a human aware manipulation planner and a supervision system dedicated to collaborative task achievement.</p></div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349866&CFID=105752528&CFTOKEN=27713757">Theory of mind (ToM) on robots: a functional neuroimaging study</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81414614279&CFID=105752528&CFTOKEN=27713757">Frank Hegel</a>, 
                        <a href="author_page.cfm?id=81350585160&CFID=105752528&CFTOKEN=27713757">Soeren Krach</a>, 
                        <a href="author_page.cfm?id=81350586081&CFID=105752528&CFTOKEN=27713757">Tilo Kircher</a>, 
                        <a href="author_page.cfm?id=81100624850&CFID=105752528&CFTOKEN=27713757">Britta Wrede</a>, 
                        <a href="author_page.cfm?id=81100458351&CFID=105752528&CFTOKEN=27713757">Gerhard Sagerer</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 335-342</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349866" title="DOI">10.1145/1349822.1349866</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349866&ftid=506324&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow44" style="display:inline;"><br /><div style="display:inline">Theory of Mind (ToM) is not only a key capability for cognitive development but also for successful social interaction. In order for a robot to interact successfully with a human both interaction partners need to have an adequate representation of the ...</div></span>
          <span id="toHide44" style="display:none;"><br /><div style="display:inline"><p>Theory of Mind (ToM) is not only a key capability for cognitive development but also for successful social interaction. In order for a robot to interact successfully with a human both interaction partners need to have an adequate representation of the other's actions. In this paper we address the question of how a robot's actions are perceived and represented in a human subject interacting with the robot and how this perception is influenced by the appearance of the robot. We present the preliminary results of an fMRI-study in which participants had to play a version of the classical Prisoners' Dilemma Game (PDG) against four opponents: a human partner (HP), an anthropomorphic robot (AR), a functional robot (FR), and a computer (CP). The PDG scenario enables to implicitly measure mentalizing or Theory of Mind (ToM) abilities, a technique commonly applied in functional imaging. As the responses of each game partner were randomized unknowingly to the participants, the attribution of intention or will to an opponent (i.e. HP, AR, FR or CP) was based purely on differences in the perception of shape and embodiment. </p> <p>The present study is the first to apply functional neuroimaging methods to study human-robot interaction on a higher cognitive level such as ToM. We hypothesize that the degree of anthropomorphism and embodiment of the game partner will modulate cortical activity in previously detected ToM networks as the medial prefrontal lobe and anterior cingulate cortex.</p></div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349867&CFID=105752528&CFTOKEN=27713757">Three dimensional tangible user interface for controlling a robotic team</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350578065&CFID=105752528&CFTOKEN=27713757">Paul Lapides</a>, 
                        <a href="author_page.cfm?id=81339528031&CFID=105752528&CFTOKEN=27713757">Ehud Sharlin</a>, 
                        <a href="author_page.cfm?id=81350586077&CFID=105752528&CFTOKEN=27713757">Mario Costa Sousa</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 343-350</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349867" title="DOI">10.1145/1349822.1349867</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349867&ftid=506325&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow45" style="display:inline;"><br /><div style="display:inline">We describe a new method for controlling a group of robots in three-dimensional (3D) space using a tangible user interface called the 3D Tractus. Our interface maps the task space into an interactive 3D space, allowing a single user to intuitively monitor ...</div></span>
          <span id="toHide45" style="display:none;"><br /><div style="display:inline"><p>We describe a new method for controlling a group of robots in three-dimensional (3D) space using a tangible user interface called the 3D Tractus. Our interface maps the task space into an interactive 3D space, allowing a single user to intuitively monitor and control a group of robots. We present the use of the interface in controlling a group of virtual software bots and a physical Sony AIBO robot dog in a simulated Explosive Ordnance Disposal (EOD) environment involving a bomb hidden inside of a building. We also describe a comparative user study we performed where participants were asked to use both the 3D physical interface and a traditional 2D graphical user interface in order to try and demonstrate the benefits and drawbacks of each approach for HRI tasks.</p></div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349868&CFID=105752528&CFTOKEN=27713757">Towards combining UAV and sensor operator roles in UAV-enabled visual search</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350576063&CFID=105752528&CFTOKEN=27713757">Joseph Cooper</a>, 
                        <a href="author_page.cfm?id=81350575550&CFID=105752528&CFTOKEN=27713757">Michael A. Goodrich</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 351-358</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349868" title="DOI">10.1145/1349822.1349868</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349868&ftid=506326&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow46" style="display:inline;"><br /><div style="display:inline">Wilderness search and rescue (WiSAR) is a challenging problem because of the large areas and often rough terrain that must be searched. Using mini-UAVs to deliver aerial video to searchers has potential to support WiSAR efforts, but a number of technology ...</div></span>
          <span id="toHide46" style="display:none;"><br /><div style="display:inline"><p>Wilderness search and rescue (WiSAR) is a challenging problem because of the large areas and often rough terrain that must be searched. Using mini-UAVs to deliver aerial video to searchers has potential to support WiSAR efforts, but a number of technology and human factors problems must be overcome to make this practical. At the source of many of these problems is a desire to manage the UAV using as few people as possible, so that more people can be used in ground-based search efforts. This paper uses observations from two informal studies and one formal experiment to identify what human operators may be unaware of as a function of autonomy and information display. Results suggest that progress is being made on designing autonomy and information displays that may make it possible for a single human to simultaneously manage the UAV and its camera in WiSAR, but that adaptable displays that support systematic navigation are probably needed.</p></div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349869&CFID=105752528&CFTOKEN=27713757">Towards multimodal human-robot interaction in large scale virtual environment</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81387601368&CFID=105752528&CFTOKEN=27713757">Pierre Boudoin</a>, 
                        <a href="author_page.cfm?id=81350572313&CFID=105752528&CFTOKEN=27713757">Christophe Domingues</a>, 
                        <a href="author_page.cfm?id=81314491211&CFID=105752528&CFTOKEN=27713757">Samir Otmane</a>, 
                        <a href="author_page.cfm?id=81314491252&CFID=105752528&CFTOKEN=27713757">Nassima Ouramdane</a>, 
                        <a href="author_page.cfm?id=81100153761&CFID=105752528&CFTOKEN=27713757">Malik Mallem</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 359-366</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349869" title="DOI">10.1145/1349822.1349869</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349869&ftid=506677&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow47" style="display:inline;"><br /><div style="display:inline">Human Operators (HO) of telerobotics systems may be able to achieve complex operations with robots. Designing usable and effective Human-Robot Interaction (HRI) is very challenging for system developers and human factors specialists. The search for new ...</div></span>
          <span id="toHide47" style="display:none;"><br /><div style="display:inline"><p>Human Operators (HO) of telerobotics systems may be able to achieve complex operations with robots. Designing usable and effective Human-Robot Interaction (HRI) is very challenging for system developers and human factors specialists. The search for new metaphors and techniques for HRI adapted to telerobotics systems emerge on the conception of Multimodal HRI (MHRI). MHRI allows to interact naturally and easily with robots due to combination of many devices and an efficient Multimodal Management System (MMS). A system like this should bring a new user's experience in terms of natural interaction, usability, efficiency and flexibility to HRI system. So, a good management of multimodality is very. Moreover, the MMS must be transparent to user in order to be efficient and natural.</p> <p>Empirical evaluation is necessary to have an idea about the goodness of our MMS. We will use an Empirical Evaluation Assistant (EEA) designed in the IBISC laboratory. EEA permits to rapidly gather significant feedbacks about the usability of interaction during the development lifecycle. However the HRI would be classically evaluated by ergonomics experts at the end of its development lifecycle.</p> <p>Results from a preliminary evaluation on a robot teleoperation tasks using the ARITI software framework for assisting the user in piloting the robot, and the IBISC semi-immersive VR/AR platform EVR@, are given. They compare the use of a Flystick and Data Gloves for the 3D interaction with the robot. They show that our MMS is functional although multimodality used in our experiments is not sufficient to provide an efficient Human-Robot Interaction. The EVR@ SPIDAR force feedback will be integrated in our MMS to improve the user's efficiency.</p></div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349870&CFID=105752528&CFTOKEN=27713757">Understanding human intentions via hidden markov models in autonomous mobile robots</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81388598666&CFID=105752528&CFTOKEN=27713757">Richard Kelley</a>, 
                        <a href="author_page.cfm?id=81322507569&CFID=105752528&CFTOKEN=27713757">Alireza Tavakkoli</a>, 
                        <a href="author_page.cfm?id=81406599982&CFID=105752528&CFTOKEN=27713757">Christopher King</a>, 
                        <a href="author_page.cfm?id=81100536266&CFID=105752528&CFTOKEN=27713757">Monica Nicolescu</a>, 
                        <a href="author_page.cfm?id=81100536268&CFID=105752528&CFTOKEN=27713757">Mircea Nicolescu</a>, 
                        <a href="author_page.cfm?id=81100022965&CFID=105752528&CFTOKEN=27713757">George Bebis</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 367-374</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349870" title="DOI">10.1145/1349822.1349870</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349870&ftid=506678&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow48" style="display:inline;"><br /><div style="display:inline">Understanding intent is an important aspect of communication among people and is an essential component of the human cognitive system. This capability is particularly relevant for situations that involve collaboration among agents or detection of situations ...</div></span>
          <span id="toHide48" style="display:none;"><br /><div style="display:inline"><p>Understanding intent is an important aspect of communication among people and is an essential component of the human cognitive system. This capability is particularly relevant for situations that involve collaboration among agents or detection of situations that can pose a threat. In this paper, we propose an approach that allows a robot to detect intentions of others based on experience acquired through its own sensory-motor capabilities, then using this experience while taking the perspective of the agent whose intent should be recognized. Our method uses a novel formulation of Hidden Markov Models designed to model a robot's experience and interaction with the world. The robot's capability to observe and analyze the current scene employs a novel vision-based technique for target detection and tracking, using a non-parametric recursive modeling approach. We validate this architecture with a physically embedded robot, detecting the intent of several people performing various activities.</p></div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349871&CFID=105752528&CFTOKEN=27713757">Using a robot proxy to create common ground in exploration tasks</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350575708&CFID=105752528&CFTOKEN=27713757">Kristen Stubbs</a>, 
                        <a href="author_page.cfm?id=81100412265&CFID=105752528&CFTOKEN=27713757">David Wettergreen</a>, 
                        <a href="author_page.cfm?id=81100538966&CFID=105752528&CFTOKEN=27713757">Illah Nourbakhsh</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 375-382</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349871" title="DOI">10.1145/1349822.1349871</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349871&ftid=506679&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow49" style="display:inline;"><br /><div style="display:inline">In this paper, we present a user study of a new collaborative communication method between a user and remotely-located robot performing an exploration task. In the studied scenario, our user possesses scientific expertise but not necessarily detailed ...</div></span>
          <span id="toHide49" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present a user study of a new collaborative communication method between a user and remotely-located robot performing an exploration task. In the studied scenario, our user possesses scientific expertise but not necessarily detailed knowledge of the robot's capabilities, resulting in very little common ground</p> <p>between the user and robot. Because the robot is not available during mission planning, we introduce a robot proxy to build common ground with the user. Our robot proxy has the ability to provide feedback to the user about the user's plans before the plans are executed. Our study demonstrated that the use of the robot proxy resulted in</p> <p>improved performance and efficiency on an exploration task, more accurate mental models of the robot's capabilities, a stronger perception of effectiveness at the task, and stronger feelings of collaboration with the robotic system.</p></div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Videos</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349873&CFID=105752528&CFTOKEN=27713757">HRI caught on film 2</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100461702&CFID=105752528&CFTOKEN=27713757">Christoph Bartneck</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 383-388</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349873" title="DOI">10.1145/1349822.1349873</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349873&ftid=506680&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow51" style="display:inline;"><br /><div style="display:inline">Following the great success of the first video session at the HRI2007 conference (Bartneck & Kanda, 2007), the Human Robot Interaction 2008 conference hosted the second video session, in which movies of interesting, important, illustrative, or humorous ...</div></span>
          <span id="toHide51" style="display:none;"><br /><div style="display:inline"><p>Following the great success of the first video session at the HRI2007 conference (Bartneck & Kanda, 2007), the Human Robot Interaction 2008 conference hosted the second video session, in which movies of interesting, important, illustrative, or humorous HRI research moments were shown. Robots and humans do not always behave as expected and the results can be entertaining and even enlightening -- therefore instances of failures have also been considered in the video session. Besides the importance of the lessons learned and the novelty of the situation, the videos have also an entertaining value. The video session had the character of a design competition.</p></div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="2">SESSION: <strong>Invited-keynote talks</strong></td>
  </tr>
  
          <tr>
          <td></td>
          <td style="padding-bottom:5px;">
          
          </td>
          </tr>

          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349875&CFID=105752528&CFTOKEN=27713757">Joint action in man and autonomous systems</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81350568329&CFID=105752528&CFTOKEN=27713757">Harold Bekkering</a>, 
                        <a href="author_page.cfm?id=81350587362&CFID=105752528&CFTOKEN=27713757">Estela Bicho</a>, 
                        <a href="author_page.cfm?id=81343500750&CFID=105752528&CFTOKEN=27713757">Ruud G.J. Meulenbroek</a>, 
                        <a href="author_page.cfm?id=81350570163&CFID=105752528&CFTOKEN=27713757">Wolfram Erlhagen</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 389-390</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349875" title="DOI">10.1145/1349822.1349875</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349875&ftid=506681&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow53" style="display:inline;"><br /><div style="display:inline">This talk presents recent functional insights derived from behavioural and neuroimaging studies into the cognitive mechanisms underlying human joint action. The main question is how the cognitive system of one actor can organize the perceptual consequences ...</div></span>
          <span id="toHide53" style="display:none;"><br /><div style="display:inline"><p>This talk presents recent functional insights derived from behavioural and neuroimaging studies into the cognitive mechanisms underlying human joint action. The main question is how the cognitive system of one actor can organize the perceptual consequences of the movements of another actor such that effective joint action in the two actors can take place. Particularly, the issue of complementing each other's action in contrast to merely imitating the actions that one observes will be discussed. Other issues that have been investigated are motivational states (cooperative or competitive), error-monitoring and the interaction between actors at the level of motor control. The talk is completed with presenting recent attempts to implement the functional insights from these experiments into autonomous systems being capable of collaborating intelligently on shared motor tasks.</p></div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349876&CFID=105752528&CFTOKEN=27713757">Toward cognitive robot companions</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100543671&CFID=105752528&CFTOKEN=27713757">Raja Chatila</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 391-392</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349876" title="DOI">10.1145/1349822.1349876</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349876&ftid=506682&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow54" style="display:inline;"><br /><div style="display:inline">If robots have to be, one day, part of our environment and assist humans in their daily life, they will have to be endowed not only with the necessary functions for sensing, moving and acting, but also and inevitably, with more advanced cognitive capacities. ...</div></span>
          <span id="toHide54" style="display:none;"><br /><div style="display:inline"><p>If robots have to be, one day, part of our environment and assist humans in their daily life, they will have to be endowed not only with the necessary functions for sensing, moving and acting, but also and inevitably, with more advanced cognitive capacities. Indeed a robot that will interact with people will need to be able to understand the spatial and dynamic structure of its environment, to exhibit a social behavior, to communicate with humans at the appropriate level of abstraction, to focus its attention and to take decisions for achieving tasks, to learn new knowledge and to evolve new capacities in an open-ended fashion.</p> <p>The COGNIRON (The Cognitive Robot Companion) project studies the development of robots whose ultimate task would be serve and assist humans. The talk will overview the achievements of the project and its results that are demonstrated in three main experimental settings, enabling to exhibit cognitive capacities : the robot home tour, the curious and proactive robot and the robot learner.</p></div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
          </div>
		  </td>
          </tr>          
          
  <tr>
  <td></td>
  <td colspan="1"><span style="padding-left:20"><a href="citation.cfm?id=1349877&CFID=105752528&CFTOKEN=27713757">Talking as if</a></span></td>
  </tr>
  
          <tr>
          <td></td>
          <td>
          <span style="padding-left:20">
          
                        <a href="author_page.cfm?id=81100621388&CFID=105752528&CFTOKEN=27713757">Herbert H. Clark</a> 
          </span>    
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">Pages: 393-394</span></td>
          </tr>
          
          <tr>
          <td></td>
          <td> <span style="padding-left:20">doi&gt;<a href="http://dx.doi.org/10.1145/1349822.1349877" title="DOI">10.1145/1349822.1349877</a></span></td>
          </tr>
          
          <tr>
          <td></td>
          <td>
           <span style="padding-left:20">
          Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1349877&ftid=506683&dwn=1&CFID=105752528&CFTOKEN=27713757" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
           </span>
          </td>
          </tr>
          
          <tr>
          <td></td>
          <td style="padding-bottom:15px;">
          <div style="padding-left:20">
          <span id="toShow55" style="display:inline;"><br /><div style="display:inline">If ordinary people are to work with humanoid robots, they will need to communicate with them. But how will they do that? For many in the field, the goal is to design robots that people can talk to just as they talk to actual people. But a close look ...</div></span>
          <span id="toHide55" style="display:none;"><br /><div style="display:inline"><p>If ordinary people are to work with humanoid robots, they will need to communicate with them. But how will they do that? For many in the field, the goal is to design robots that people can talk to just as they talk to actual people. But a close look at actual communication suggests that this goal isn't realist. It may even be untenable in principle.</p> <p>An alternative goal is to design robots that people can talk to just as they talk to dynamic depictions of other people-what I will call characters. As it happens, ordinary people have a great deal of experience in interpreting the speech, gestures, and other actions of characters, and even in interacting with them. But talking to characters is different from talking to actual people. So once we view robots as characters, we will need a model of communication based on different principles. That, in turn, may change our ideas of what people can and cannot do with robots.</p></div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
          </div>
		  </td>
          </tr>          
          
</table>


</div> 
</div>


 <p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>



 <br />
<div class="footerbody" align="center" >
	

	The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2012 ACM, Inc.<br />
	<a href="http://www.acm.org/publications/policies/usage">Terms of Usage</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
	<a href="http://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;	  
	<a href="http://www.acm.org/about/contact-us">Contact Us</a>

<br /><br />
Useful downloads: 
<a href="http://www.adobe.com/products/acrobat/readstep2.html"><img src="http://dl.acm.org/images/pdf_logo.gif" width="16" height="16" alt="" border="0" /> Adobe Acrobat</a>
&nbsp;&nbsp;
<a href="http://www.apple.com/quicktime/download/" target="_blank"><img src="http://dl.acm.org/images/qtlogo.gif" width="16" height="16" alt="" border="0" /> QuickTime</a>
&nbsp;&nbsp;
<a href="http://www.microsoft.com/windows/windowsmedia/download/default.asp" target="_blank"><img src="http://dl.acm.org/images/wmv.gif" width="16" height="15" alt="" border="0" /> Windows Media Player</a>
&nbsp;&nbsp;
<a href="http://www.real.com/" target="_blank"><img src="http://dl.acm.org/images/realplayer.gif" width="20" height="18" alt="" border="0" /> Real Player</a>

</div> 



<div  id="cf_window1338242121509" class="yuiextdlg">
	
	<div  id="theguide_title" class="x-dlg-hd">
		The ACM Guide to Computing Literature
	 </div>
	<div  id="theguide_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242121512" class="yuiextdlg">
	
	<div  id="thetags_title" class="x-dlg-hd">
		All Tags
	 </div>
	<div  id="thetags_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242121515" class="yuiextdlg">
	
	<div  id="theformats_title" class="x-dlg-hd">
		Export Formats
	 </div>
	<div  id="theformats_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242121517" class="yuiextdlg">
	
	<div  id="theexplaination_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theexplaination_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242121519" class="yuiextdlg">
	
	<div  id="theservices_title" class="x-dlg-hd">
		&nbsp;
	 </div>
	<div  id="theservices_body" class="x-dlg-bd">
		
		
	 </div>
 </div> <div  id="cf_window1338242121521" class="yuiextdlg">
	
	<div  id="savetobinder_title" class="x-dlg-hd">
		Save to Binder
	 </div>
	<div  id="savetobinder_body" class="x-dlg-bd">
		
		
	 </div>
 </div> 

</body>
</html>